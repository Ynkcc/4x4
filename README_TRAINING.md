# 暗棋强化学习训练指南

本项目基于Numpy向量实现的4x4暗棋环境，使用MaskablePPO进行强化学习训练。

## 文件说明

- `Game.py`: 基于Numpy向量的暗棋游戏环境实现
- `train.py`: MaskablePPO训练脚本 
- `test_model.py`: 模型测试脚本
- `game_gui.py`: 图形化界面（如果存在）

## 快速开始

### 1. 开始训练

```bash
python train.py
```

训练参数说明:
- 默认训练步数: 3,000,000
- 并行环境数: 6
- 网络结构: [512, 512, 256]
- 学习率: 3e-4

### 2. 测试模型

```bash
# 测试模型性能（5个回合）
python test_model.py

# 人机对战演示
python test_model.py play
```

### 3. 监控训练进度

使用TensorBoard监控训练过程:

```bash
tensorboard --logdir ./banqi_numpy_ppo_logs/
```

然后在浏览器访问 http://localhost:6006

## 训练特性

### 优化特性
- **并行训练**: 使用6个子进程环境加速数据收集
- **动作掩码**: 只允许合法动作，提高训练效率
- **进度回调**: 实时显示训练统计信息
- **模型检查点**: 定期保存模型，防止训练中断
- **最佳模型保存**: 自动保存表现最好的模型

### 网络结构
- 策略网络: [512, 512, 256] 全连接层
- 激活函数: Tanh
- 优化器: Adam (学习率 3e-4)

### 奖励设计
- 时间惩罚: -0.0005 (鼓励快速决策)
- 翻棋奖励: +0.0005
- 吃子奖励: 根据棋子价值给予奖励
- 误杀惩罚: 炮误杀己方棋子时扣除分数

## 游戏规则

### 棋子类型与价值
- 兵/卒: 4分
- 炮: 10分  
- 马: 10分
- 车: 10分
- 象/相: 10分
- 士/仕: 20分
- 将/帅: 30分

### 特殊规则
- 兵可以吃将，将不能吃兵
- 炮需要有炮架才能攻击
- 炮可以误杀己方棋子（会被扣分）
- 先达到60分或对方无子可走则获胜
- 连续40步无吃子/翻子则平局

## 动作空间

总共112个动作:
- 翻棋动作: 0-15 (16个位置)
- 普通移动: 16-63 (48个移动方向)
- 炮攻击: 64-111 (48个炮击路径)

## 状态空间

状态向量包含:
- 我方棋子位置 (7 × 16 = 112维)
- 对方棋子位置 (7 × 16 = 112维) 
- 暗棋位置 (16维)
- 空格位置 (16维)
- 标量特征 (3维): 我方得分、对方得分、连续移动数

总计: 259维状态向量

## 性能优化

### 已实现的优化
1. **查找表预计算**: 所有动作映射和炮击路径预计算
2. **向量化操作**: 使用Numpy布尔向量替代逐位操作
3. **稀疏更新**: 只更新变化的状态位
4. **并行环境**: 多进程数据收集

### 训练建议
- CPU训练: 适合中小规模实验
- GPU训练: 可设置 `device='cuda'` （需要PyTorch GPU支持）
- 内存优化: 如遇内存不足，可减少并行环境数

## 故障排除

### 常见问题

1. **内存不足**
   ```bash
   # 减少并行环境数
   n_envs = 4  # 在train.py中修改
   ```

2. **训练速度慢**  
   ```bash
   # 检查CPU使用情况
   htop
   # 考虑使用更少的环境但更大的batch_size
   ```

3. **模型不收敛**
   ```bash
   # 调整学习率
   learning_rate = 1e-4  # 降低学习率
   # 或调整网络结构
   ```

## 扩展建议

- 实现更完善的人机对战界面
- 添加自对弈训练模式
- 实现多种算法对比（A3C, SAC等）
- 加入棋力评估系统
- 支持不同棋盘尺寸

## 许可证

本项目仅供学习研究使用。
