# train_simple.py
import os
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
from sb3_contrib import MaskablePPO

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from game.environment import GameEnvironment
from game.policy import CustomActorCriticPolicy
from training.simple_agent import SimpleAgent
from utils.scheduler import linear_schedule
from utils.constants import N_ENVS, INITIAL_LR

def train_with_simple_opponent():
    """
    ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ã€åŸºäºè§„åˆ™çš„å¯¹æ‰‹æ¥è®­ç»ƒä¸»æ¨¡å‹ã€‚
    è¿™ç”¨äºéªŒè¯å­¦ä¹ æµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
    """
    print("=================================================")
    print("      å¼€å§‹ä½¿ç”¨ç®€å•Agentè¿›è¡ŒéªŒè¯æ€§è®­ç»ƒ       ")
    print("=================================================")
    
    # --- é…ç½® ---
    TOTAL_STEPS = 200_000 # æ€»è®­ç»ƒæ­¥æ•°
    LOG_DIR = "./tensorboard_logs/simple_agent_test/"
    MODEL_SAVE_PATH = "./models/simple_agent_test/final_model.zip"
    
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
    
    # 1. åˆ›å»ºç®€å•å¯¹æ‰‹çš„å®ä¾‹
    simple_opponent = SimpleAgent()
    
    # 2. åˆ›å»ºç¯å¢ƒï¼Œå¹¶é€šè¿‡ `env_kwargs` å°†å¯¹æ‰‹å®ä¾‹æ³¨å…¥
    env_kwargs = {
        'curriculum_stage': 4,       # ä½¿ç”¨å®Œæ•´æ¸¸æˆæ¨¡å¼
        'opponent_agent': simple_opponent
    }
    
    # ä½¿ç”¨ SubprocVecEnv ä»¥å®ç°çœŸæ­£çš„å¹¶è¡ŒåŒ–
    env = make_vec_env(
        GameEnvironment,
        n_envs=N_ENVS,
        vec_env_cls=SubprocVecEnv,
        env_kwargs=env_kwargs
    )
    
    # 3. åˆ›å»ºæˆ–åŠ è½½å­¦ä¹ è€…æ¨¡å‹
    # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ€»æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°çš„æ¨¡å‹æ¥è¿›è¡ŒéªŒè¯
    model = MaskablePPO(
        CustomActorCriticPolicy,
        env,
        learning_rate=linear_schedule(INITIAL_LR),
        verbose=1,
        tensorboard_log=LOG_DIR,
        gamma=0.99,
        n_steps=2048
    )
    
    print("\n--- [è®­ç»ƒå¼€å§‹] ---")
    print(f"å­¦ä¹ è€…æ¨¡å‹: MaskablePPO (CustomPolicy)")
    print(f"å¯¹æ‰‹æ¨¡å‹: SimpleAgent (éšæœºç­–ç•¥)")
    print(f"å¹¶è¡Œç¯å¢ƒæ•°: {N_ENVS}")
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {TOTAL_STEPS:,}")
    print(f"æ—¥å¿—è·¯å¾„: {LOG_DIR}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {MODEL_SAVE_PATH}")
    print("---------------------------------\n")

    # 4. å¯åŠ¨è®­ç»ƒ
    try:
        model.learn(
            total_timesteps=TOTAL_STEPS,
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        
    # 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(MODEL_SAVE_PATH)
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_SAVE_PATH}")
    
    env.close()

if __name__ == "__main__":
    train_with_simple_opponent()