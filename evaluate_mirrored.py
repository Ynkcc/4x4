# evaluate_mirrored.py
"""
é•œåƒå¯¹å±€è¯„ä¼°è„šæœ¬ã€‚

ã€ä¼˜åŒ–ã€‘:
- ä¸å†ç»´æŠ¤ç‹¬ç«‹çš„ play_one_game å‡½æ•°ï¼Œè€Œæ˜¯ç›´æ¥ä» training.evaluator æ¨¡å—å¯¼å…¥ï¼Œç¡®ä¿è¯„ä¼°é€»è¾‘ç»Ÿä¸€ã€‚
- å¯¹å±€æ•°ç°åœ¨ä¹Ÿç”± constants.py æ§åˆ¶ã€‚
"""
import os
import warnings

# ç¦ç”¨TensorFlowè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore', category=UserWarning, module='google.protobuf')

import numpy as np
import time
from tqdm import tqdm

# ã€æ›´æ–°ã€‘å¯¼å…¥æœ¬åœ°æ¨¡å—
from game.environment import GameEnvironment
# ã€æ›´æ–°ã€‘ç›´æ¥å¤ç”¨è®­ç»ƒå™¨ä¸­çš„è¯„ä¼°é€»è¾‘å’ŒAgent
from training.evaluator import EvaluationAgent, _play_one_game
# ã€æ›´æ–°ã€‘ä»å¸¸é‡æ–‡ä»¶ä¸­è¯»å–é…ç½®
from utils.constants import EVALUATION_GAMES


def evaluate_mirrored_matches(model1_path: str, model2_path: str):
    """
    æ‰§è¡Œé•œåƒå¯¹å±€è¯„ä¼°ã€‚

    :param model1_path: æ¨¡å‹1çš„è·¯å¾„ã€‚
    :param model2_path: æ¨¡å‹2çš„è·¯å¾„ã€‚
    """
    # ã€ä¼˜åŒ–ã€‘æ ¡éªŒè¯„ä¼°å±€æ•°
    if EVALUATION_GAMES % 2 != 0:
        raise ValueError(f"EVALUATION_GAMES ({EVALUATION_GAMES}) å¿…é¡»æ˜¯å¶æ•°ï¼Œæ‰èƒ½è¿›è¡Œå®Œç¾çš„é•œåƒå¯¹å±€ã€‚")
    num_groups = EVALUATION_GAMES // 2

    print("=" * 70)
    print("           âš”ï¸  é•œåƒå¯¹å±€è¯„ä¼°ç³»ç»Ÿ âš”ï¸")
    print("=" * 70)

    # 1. åŠ è½½æ¨¡å‹
    try:
        model1 = EvaluationAgent(model1_path)
        model2 = EvaluationAgent(model2_path)
        print(f"è¯„æµ‹æ¨¡å‹ A: {model1.name}")
        print(f"è¯„æµ‹æ¨¡å‹ B: {model2.name}")
        print(f"å¯¹å±€ç»„æ•°: {num_groups} (æ€»è®¡ {EVALUATION_GAMES} å±€æ¸¸æˆ)")
        print("-" * 70)
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return

    # 2. åˆå§‹åŒ–ç¯å¢ƒå’Œè®¡åˆ†æ¿
    eval_env = GameEnvironment()
    scores = {
        'model1_wins': 0,
        'model2_wins': 0,
        'draws': 0,
        'model1_as_red_wins': 0,
        'model2_as_red_wins': 0,
    }

    # 3. æ‰§è¡Œè¯„ä¼°å¾ªç¯
    start_time = time.time()
    # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨å’Œè®­ç»ƒå™¨å®Œå…¨ä¸€è‡´çš„ _play_one_game å‡½æ•°
    for i in tqdm(range(num_groups), desc="æ­£åœ¨è¿›è¡Œé•œåƒå¯¹å±€è¯„ä¼°"):
        game_seed = int(time.time_ns() + i) % (2**32 - 1)

        # ç¬¬ä¸€å±€: æ¨¡å‹Aæ‰§çº¢ vs æ¨¡å‹Bæ‰§é»‘
        winner_1 = _play_one_game(eval_env, red_player=model1, black_player=model2, seed=game_seed)
        if winner_1 == 1:
            scores['model1_wins'] += 1
            scores['model1_as_red_wins'] += 1
        elif winner_1 == -1:
            scores['model2_wins'] += 1
        else:
            scores['draws'] += 1

        # ç¬¬äºŒå±€: æ¨¡å‹Bæ‰§çº¢ vs æ¨¡å‹Aæ‰§é»‘ (é•œåƒ)
        winner_2 = _play_one_game(eval_env, red_player=model2, black_player=model1, seed=game_seed)
        if winner_2 == 1:
            scores['model2_wins'] += 1
            scores['model2_as_red_wins'] += 1
        elif winner_2 == -1:
            scores['model1_wins'] += 1
        else:
            scores['draws'] += 1
            
    eval_env.close()
    end_time = time.time()

    # 4. è®¡ç®—å¹¶æ‰“å°ç»“æœ
    total_games = num_groups * 2
    total_decisive_games = scores['model1_wins'] + scores['model2_wins']
    win_rate_model1 = scores['model1_wins'] / total_decisive_games if total_decisive_games > 0 else 0.0

    print("\n" + "=" * 70)
    print("           ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ ğŸ“Š")
    print("=" * 70)
    print(f"æ€»è®¡ç”¨æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"å¹³å‡æ¯å±€ç”¨æ—¶: {(end_time - start_time) / total_games:.2f} ç§’\n")
    print(f"--- æ€»ä½“æˆ˜ç»© (å…± {total_games} å±€) ---")
    print(f"  - {model1.name} èƒœ: {scores['model1_wins']} å±€")
    print(f"  - {model2.name} èƒœ: {scores['model2_wins']} å±€")
    print(f"  - å¹³å±€: {scores['draws']} å±€\n")
    print(f"--- èƒœç‡è®¡ç®— (åŸºäºéå¹³å±€) ---")
    print(f"  - {model1.name} èƒœç‡: {win_rate_model1:.2%}")
    print(f"  - {model2.name} èƒœç‡: {1.0 - win_rate_model1:.2%}\n")
    print(f"--- å…ˆæ‰‹ï¼ˆçº¢æ–¹ï¼‰è¡¨ç°åˆ†æ ---")
    print(f"  - {model1.name} æ‰§çº¢èƒœå±€: {scores['model1_as_red_wins']} / {num_groups} ({scores['model1_as_red_wins']/num_groups:.2%})")
    print(f"  - {model2.name} æ‰§çº¢èƒœå±€: {scores['model2_as_red_wins']} / {num_groups} ({scores['model2_as_red_wins']/num_groups:.2%})")
    print("=" * 70)


if __name__ == '__main__':
    # --- åœ¨æ­¤é…ç½®æ‚¨è¦è¯„ä¼°çš„æ¨¡å‹è·¯å¾„ ---
    MODEL_A_PATH = "./models/self_play_final/main_opponent.zip"
    MODEL_B_PATH = "./models/self_play_final/challenger.zip"
    
    # ã€ä¼˜åŒ–ã€‘å¯¹å±€ç»„æ•°ç°åœ¨ç”± constants.py ç»Ÿä¸€ç®¡ç†ï¼Œæ­¤å¤„æ— éœ€é…ç½®
    # NUM_EVALUATION_GROUPS = 100

    evaluate_mirrored_matches(MODEL_A_PATH, MODEL_B_PATH)