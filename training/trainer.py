# src_code/training/trainer.py

import os
import shutil
import time
import re
import json
import numpy as np
import sys
import multiprocessing as mp
from queue import Empty
from functools import partial

from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecEnv
from sb3_contrib import MaskablePPO
from typing import Dict, Any, List, Optional

from utils.constants import *
from game.environment import GameEnvironment
from game.policy import CustomActorCriticPolicy
from training.evaluator import evaluate_models

# --- ã€æ–°å¢ã€‘é¢„æµ‹å™¨å·¥ä½œè¿›ç¨‹å‡½æ•° ---
def _predictor_worker(
    prediction_q: mp.Queue,
    result_q: mp.Queue,
    initial_opponent_data: List[Dict[str, Any]]
):
    """
    ä¸€ä¸ªåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œçš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºåŠ è½½æ¨¡å‹å¹¶å¤„ç†é¢„æµ‹è¯·æ±‚ã€‚
    """
    print(f"[Predictor Worker, PID: {os.getpid()}] å¯åŠ¨...")

    # åœ¨å·¥ä½œè¿›ç¨‹å†…éƒ¨åŠ è½½æ¨¡å‹
    loaded_models: Dict[str, MaskablePPO] = {}

    def load_models(opponent_data: List[Dict[str, Any]]):
        """åŠ è½½æˆ–é‡æ–°åŠ è½½æ¨¡å‹"""
        loaded_models.clear()
        for item in opponent_data:
            path = item['path']
            if path not in loaded_models:
                try:
                    loaded_models[path] = MaskablePPO.load(path, device="auto")
                except Exception as e:
                    print(f"[Predictor Worker] é”™è¯¯ï¼šåŠ è½½æ¨¡å‹ {path} å¤±è´¥: {e}")
        print(f"[Predictor Worker] å·²åŠ è½½/æ›´æ–° {len(loaded_models)} ä¸ªæ¨¡å‹ã€‚")

    # åˆå§‹åŠ è½½
    load_models(initial_opponent_data)

    while True:
        try:
            # ä½¿ç”¨å¸¦è¶…æ—¶çš„getï¼Œå…è®¸å®šæœŸæ£€æŸ¥ç‰¹æ®ŠæŒ‡ä»¤
            request = prediction_q.get(timeout=0.1)

            # --- å¤„ç†ç‰¹æ®ŠæŒ‡ä»¤ ---
            if isinstance(request, str) and request == "STOP":
                print(f"[Predictor Worker, PID: {os.getpid()}] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡ºã€‚")
                break

            if isinstance(request, dict) and request.get("command") == "RELOAD_MODELS":
                print("[Predictor Worker] æ”¶åˆ°é‡æ–°åŠ è½½æ¨¡å‹çš„æŒ‡ä»¤ã€‚")
                load_models(request["data"])
                continue

            # --- å¤„ç†é¢„æµ‹è¯·æ±‚ ---
            worker_id, model_path, observation, action_masks = request

            model = loaded_models.get(model_path)
            if model:
                action, _ = model.predict(observation, action_masks=action_masks, deterministic=True)
                result_q.put((worker_id, int(action)))
            else:
                print(f"[Predictor Worker] è­¦å‘Šï¼šæ‰¾ä¸åˆ°è¯·æ±‚çš„æ¨¡å‹ {model_path}ï¼Œå°†è¿”å›éšæœºåŠ¨ä½œã€‚")
                valid_actions = np.where(action_masks)[0]
                action = np.random.choice(valid_actions) if len(valid_actions) > 0 else 0
                result_q.put((worker_id, action))

        except Empty:
            # é˜Ÿåˆ—ä¸ºç©ºæ—¶ç»§ç»­å¾ªç¯ï¼Œè¿™å¾ˆæ­£å¸¸
            continue
        except (KeyboardInterrupt, EOFError):
            print(f"[Predictor Worker, PID: {os.getpid()}] è¿›ç¨‹è¢«ä¸­æ–­ã€‚")
            break
        except Exception as e:
            print(f"[Predictor Worker] å¤„ç†è¯·æ±‚æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

def create_new_ppo_model(env, tensorboard_log=None):
    """
    åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„éšæœºåˆå§‹åŒ–çš„PPOæ¨¡å‹ã€‚
    """
    # ã€V8 ä¿®æ”¹ã€‘è¾“å…¥ç»´åº¦ç°åœ¨æ˜¯CNNå’ŒMLPè¾“å‡ºä¹‹å’Œ
    input_dim = NETWORK_NUM_HIDDEN_CHANNELS + SCALAR_ENCODER_OUTPUT_DIM

    # æ¨èæ–¹æ¡ˆ: å¼ºåŒ–ä»·å€¼ç½‘ç»œ (å·²åŠ æ·±åŠ å®½)
    policy_net_arch = dict(
        pi=[input_dim, input_dim * 2, input_dim],
        vf=[input_dim, input_dim * 2, input_dim * 2, input_dim]
    )

    model = MaskablePPO(
        policy=CustomActorCriticPolicy,
        env=env,
        learning_rate=INITIAL_LR,
        clip_range=PPO_CLIP_RANGE,
        n_steps=PPO_N_STEPS,
        batch_size=PPO_BATCH_SIZE,
        n_epochs=PPO_N_EPOCHS,
        gae_lambda=PPO_GAE_LAMBDA,
        vf_coef=PPO_VF_COEF,
        ent_coef=PPO_ENT_COEF,
        max_grad_norm=PPO_MAX_GRAD_NORM,
        tensorboard_log=tensorboard_log,
        device=PPO_DEVICE,
        verbose=PPO_VERBOSE,
        policy_kwargs={
            "net_arch": policy_net_arch,
        }
    )
    return model

def load_ppo_model_with_hyperparams(model_path: str, env, tensorboard_log=None):
    """
    åŠ è½½PPOæ¨¡å‹å¹¶åº”ç”¨è‡ªå®šä¹‰è¶…å‚æ•°ã€‚
    """
    # ã€V8 ä¿®æ”¹ã€‘è¾“å…¥ç»´åº¦ç°åœ¨æ˜¯CNNå’ŒMLPè¾“å‡ºä¹‹å’Œ
    input_dim = NETWORK_NUM_HIDDEN_CHANNELS + SCALAR_ENCODER_OUTPUT_DIM

    # æ¨èæ–¹æ¡ˆ: å¼ºåŒ–ä»·å€¼ç½‘ç»œ (å·²åŠ æ·±åŠ å®½)
    policy_net_arch = dict(
        pi=[input_dim, input_dim * 2, input_dim],
        vf=[input_dim, input_dim * 2, input_dim * 2, input_dim]
    )

    model = MaskablePPO.load(
        model_path,
        env=env,
        learning_rate=INITIAL_LR,
        clip_range=PPO_CLIP_RANGE,
        tensorboard_log=tensorboard_log,
        n_steps=PPO_N_STEPS,
        device=PPO_DEVICE,
        custom_objects={
            "policy_class": CustomActorCriticPolicy
        },
        policy_kwargs={
            "net_arch": policy_net_arch,
        }
    )
    # é‡æ–°åº”ç”¨è¶…å‚æ•°
    model.batch_size = PPO_BATCH_SIZE
    model.n_epochs = PPO_N_EPOCHS
    model.gae_lambda = PPO_GAE_LAMBDA
    model.vf_coef = PPO_VF_COEF
    model.ent_coef = PPO_ENT_COEF
    model.max_grad_norm = PPO_MAX_GRAD_NORM
    return model

def _init_env(env_class, **kwargs):
    """ä¸€ä¸ªç®€å•çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå®ä¾‹åŒ–å¸¦å‚æ•°çš„ç¯å¢ƒã€‚"""
    return env_class(**kwargs)

class SelfPlayTrainer:
    """
    ã€V7 æ–°è§„åˆ™ç‰ˆ + è¿›ç¨‹é—´é¢„æµ‹ä¼˜åŒ–ã€‘
    - ä»¥ "æŒ‘æˆ˜è€…" ä¸ºæ ¸å¿ƒè¿›è¡ŒæŒç»­è®­ç»ƒã€‚
    - å¯¹æ‰‹æ± åˆ†ä¸º "é•¿æœŸ" å’Œ "çŸ­æœŸ" æ± ï¼Œé‡‡ç”¨æ–°çš„åŠ¨æ€å·®å€¼è§„åˆ™ã€‚
    - å®ç°äº†æ›´ç§‘å­¦çš„å†å²æ¨¡å‹ä¿ç•™å’Œé‡‡æ ·æœºåˆ¶ã€‚
    - ä½¿ç”¨ç‹¬ç«‹çš„é¢„æµ‹å™¨è¿›ç¨‹æ¥å¤„ç†æ‰€æœ‰å¯¹æ‰‹çš„åŠ¨ä½œé¢„æµ‹ï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨ã€‚
    """
    def __init__(self):
        self.model: Optional[MaskablePPO] = None
        self.env: Optional[VecEnv] = None
        self.tensorboard_log_run_path = None

        # --- å¯¹æ‰‹æ± æ ¸å¿ƒå±æ€§ (æ–°è§„åˆ™) ---
        self.long_term_pool_paths = []
        self.short_term_pool_paths = []
        self.long_term_power_of_2 = 1 # è®°å½•é•¿æœŸæ¨¡å‹ä¸­2çš„æŒ‡æ•°ï¼Œåˆå§‹ä¸º1
        self.combined_opponent_data: List[Dict[str, Any]] = []

        # --- Eloä¸æ¨¡å‹ç®¡ç† ---
        self.elo_ratings = {}
        self.model_generations = {} # æ–°å¢: ç”¨äºè¿½è¸ªæ¨¡å‹ä»£æ•°
        self.latest_generation = 0
        self.default_elo = ELO_DEFAULT
        self.elo_k_factor = ELO_K_FACTOR

        # --- ã€æ–°å¢ã€‘è¿›ç¨‹é—´é€šä¿¡ ---
        self.prediction_q: Optional[mp.Queue] = None
        self.result_q: Optional[mp.Queue] = None
        self.predictor_process: Optional[mp.Process] = None

        self._setup()

    def _setup(self):
        """
        ã€é‡æ„ã€‘æ‰§è¡Œæ‰€æœ‰å¯åŠ¨å‰çš„å‡†å¤‡å·¥ä½œï¼Œç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸã€‚
        """
        print("--- [æ­¥éª¤ 1/5] åˆå§‹åŒ–è®¾ç½® ---")
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        os.makedirs(TENSORBOARD_LOG_PATH, exist_ok=True)

        # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³• (å¯¹Windows/macOSå¾ˆé‡è¦)
        if sys.platform.startswith('win') or sys.platform == 'darwin':
            mp.set_start_method('spawn', force=True)

        self._load_elo_and_generations()

        # æ ¸å¿ƒæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
        if not os.path.exists(CHALLENGER_PATH):
            print(">>> æŒ‘æˆ˜è€…æ¨¡å‹ä¸å­˜åœ¨ï¼Œè§†ä¸ºä»é›¶å¼€å§‹è®­ç»ƒã€‚")
            self._create_initial_models()

        if not os.path.exists(MAIN_OPPONENT_PATH):
            print(">>> ä¸»å®°è€…æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†ä»ç°æœ‰æŒ‘æˆ˜è€…æ¨¡å‹å¤åˆ¶ã€‚")
            shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
            main_opp_name = os.path.basename(MAIN_OPPONENT_PATH)
            challenger_name = os.path.basename(CHALLENGER_PATH)
            if main_opp_name not in self.elo_ratings:
                self.elo_ratings[main_opp_name] = self.elo_ratings.get(challenger_name, self.default_elo)
            if main_opp_name not in self.model_generations:
                 self.model_generations[main_opp_name] = self.model_generations.get(challenger_name, 0)
            self._save_elo_and_generations()

        # å¯åŠ¨æ—¶è¿›è¡Œä¸€æ¬¡æ± ç®¡ç†ï¼ˆä¸»è¦ç”¨äºæ¸…ç†æ— æ•ˆæ–‡ä»¶ï¼‰
        self._manage_opponent_pool()

    def _create_initial_models(self):
        """åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„éšæœºåˆå§‹åŒ–æ¨¡å‹ä½œä¸ºè®­ç»ƒèµ·ç‚¹ã€‚"""
        print("æ­£åœ¨åˆ›å»ºä¸´æ—¶ç¯å¢ƒä»¥åˆå§‹åŒ–æ¨¡å‹...")
        temp_env = GameEnvironment()

        print("æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        new_model = create_new_ppo_model(env=temp_env)

        # ä¿å­˜ä¸ºæŒ‘æˆ˜è€…å’Œä¸»å®°è€…
        new_model.save(CHALLENGER_PATH)
        shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
        print(f"âœ… åˆå§‹æ¨¡å‹å·²åˆ›å»ºå¹¶ä¿å­˜ä¸º 'challenger.zip' å’Œ 'main_opponent.zip'")

        # åˆå§‹åŒ–Eloå’Œä»£æ•°
        challenger_name = os.path.basename(CHALLENGER_PATH)
        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)
        self.elo_ratings[challenger_name] = self.default_elo
        self.elo_ratings[main_opponent_name] = self.default_elo
        self.model_generations[challenger_name] = 0
        self.model_generations[main_opponent_name] = 0
        self.latest_generation = 0
        self._save_elo_and_generations()

        temp_env.close()
        print("âœ… ä¸´æ—¶ç¯å¢ƒå·²æ¸…ç†")

    def _load_elo_and_generations(self):
        """ä»JSONæ–‡ä»¶åŠ è½½Eloè¯„åˆ†ã€æ¨¡å‹ä»£æ•°å’Œæ–°çš„æ¨¡å‹æ± çŠ¶æ€ã€‚"""
        elo_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        if os.path.exists(elo_file):
            try:
                with open(elo_file, 'r') as f:
                    data = json.load(f)
                    self.elo_ratings = data.get("elo", {})
                    self.model_generations = data.get("generations", {})
                    self.latest_generation = data.get("latest_generation", 0)
                    self.long_term_pool_paths = data.get("long_term_pool_paths", [])
                    self.short_term_pool_paths = data.get("short_term_pool_paths", [])
                    self.long_term_power_of_2 = data.get("long_term_power_of_2", 1)
            except (json.JSONDecodeError, IOError, KeyError) as e:
                print(f"è­¦å‘Šï¼šè¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥æˆ–æ ¼å¼ä¸å®Œæ•´: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
                self.elo_ratings = {}
                self.model_generations = {}
                self.latest_generation = 0
                self.long_term_pool_paths = []
                self.short_term_pool_paths = []
                self.long_term_power_of_2 = 1

    def _save_elo_and_generations(self):
        """å°†Eloã€æ¨¡å‹ä»£æ•°å’Œæ¨¡å‹æ± çŠ¶æ€ä¿å­˜åˆ°åŒä¸€ä¸ªJSONæ–‡ä»¶ã€‚"""
        elo_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        data = {
            "elo": self.elo_ratings,
            "generations": self.model_generations,
            "latest_generation": self.latest_generation,
            "long_term_pool_paths": self.long_term_pool_paths,
            "short_term_pool_paths": self.short_term_pool_paths,
            "long_term_power_of_2": self.long_term_power_of_2,
        }
        try:
            with open(elo_file, 'w') as f:
                json.dump(data, f, indent=4)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜çŠ¶æ€æ–‡ä»¶: {e}")

    def _manage_opponent_pool(self, new_opponent_path=None):
        """
        ã€V7 æ–°è§„åˆ™ã€‘ç®¡ç†é•¿æœŸå’ŒçŸ­æœŸå¯¹æ‰‹æ± ã€‚
        """
        if new_opponent_path:
            self.latest_generation += 1
            new_opponent_name = os.path.basename(new_opponent_path)
            self.model_generations[new_opponent_name] = self.latest_generation

            added_to_long_term = False

            long_term_pool_with_gens = sorted(
                [(p, self.model_generations.get(p, 0)) for p in self.long_term_pool_paths],
                key=lambda x: x[1]
            )
            self.long_term_pool_paths = [p for p, _ in long_term_pool_with_gens]
            long_term_gens = [g for _, g in long_term_pool_with_gens]

            if not self.long_term_pool_paths:
                print(f"é•¿æœŸæ± ä¸ºç©ºï¼Œæ–°æ¨¡å‹ {new_opponent_name} ç›´æ¥åŠ å…¥ã€‚")
                self.long_term_pool_paths.append(new_opponent_name)
                added_to_long_term = True
            else:
                required_gap = 2 ** self.long_term_power_of_2
                actual_gap = self.latest_generation - long_term_gens[-1]

                if actual_gap == required_gap:
                    if len(self.long_term_pool_paths) >= LONG_TERM_POOL_SIZE:
                        print(f"é•¿æœŸæ± å·²æ»¡ä¸”æ»¡è¶³å·®å€¼ {required_gap}ï¼Œè§¦å‘æŒ‡æ•°æ›´æ–°ã€‚")
                        self.long_term_power_of_2 += 1
                        new_required_gap = 2 ** self.long_term_power_of_2
                        print(f"2çš„æŒ‡æ•°æå‡è‡³ {self.long_term_power_of_2} (æ–°å·®å€¼ä¸º {new_required_gap})ã€‚")

                        retained_pool = [self.long_term_pool_paths[0]]
                        last_kept_gen = long_term_gens[0]

                        for i in range(1, len(long_term_gens)):
                            if (long_term_gens[i] - last_kept_gen) == new_required_gap:
                                retained_pool.append(self.long_term_pool_paths[i])
                                last_kept_gen = long_term_gens[i]

                        self.long_term_pool_paths = retained_pool
                        print(f"é•¿æœŸæ± æ›´æ–°åä¿ç•™ {len(self.long_term_pool_paths)} ä¸ªæ¨¡å‹ã€‚")

                        new_last_gen = self.model_generations.get(self.long_term_pool_paths[-1], 0)
                        if len(self.long_term_pool_paths) < LONG_TERM_POOL_SIZE and (self.latest_generation - new_last_gen) == new_required_gap:
                            self.long_term_pool_paths.append(new_opponent_name)
                            added_to_long_term = True
                            print(f"æ–°æ¨¡å‹ {new_opponent_name} åœ¨æ›´æ–°åæˆåŠŸåŠ å…¥é•¿æœŸæ± ã€‚")
                    else:
                        self.long_term_pool_paths.append(new_opponent_name)
                        added_to_long_term = True
                        print(f"é•¿æœŸæ± æœªæ»¡ï¼Œæ–°æ¨¡å‹ {new_opponent_name} æˆåŠŸåŠ å…¥ã€‚")

            if not added_to_long_term:
                self.short_term_pool_paths.append(new_opponent_name)
                self.short_term_pool_paths.sort(
                    key=lambda p: self.model_generations.get(p, 0),
                    reverse=True
                )
                if len(self.short_term_pool_paths) > SHORT_TERM_POOL_SIZE:
                    self.short_term_pool_paths = self.short_term_pool_paths[:SHORT_TERM_POOL_SIZE]

        current_pool_names = set(self.short_term_pool_paths + self.long_term_pool_paths)

        for filename in os.listdir(OPPONENT_POOL_DIR):
            if filename.endswith('.zip') and filename not in current_pool_names:
                print(f"âœ‚ï¸ æ¸…ç†è¿‡æ—¶å¯¹æ‰‹: {filename}")
                os.remove(os.path.join(OPPONENT_POOL_DIR, filename))
                self.elo_ratings.pop(filename, None)
                self.model_generations.pop(filename, None)

        self._save_elo_and_generations()
        self._update_opponent_data_structure()

    def _update_opponent_data_structure(self):
        """
        ã€IPCç‰ˆã€‘ä»…åˆ›å»ºåŒ…å«è·¯å¾„å’Œæƒé‡çš„å­—å…¸åˆ—è¡¨ï¼Œä¸å†é¢„åŠ è½½æ¨¡å‹ã€‚
        å®é™…çš„æ¨¡å‹åŠ è½½ç”±é¢„æµ‹å™¨è¿›ç¨‹å®Œæˆã€‚
        """
        self.combined_opponent_data.clear()

        pool_candidates = [
            {'filename': f, 'pool_type': 'short_term', 'sampling_weight': 1.0}
            for f in self.short_term_pool_paths
        ] + [
            {'filename': f, 'pool_type': 'long_term', 'sampling_weight': LONG_TERM_POOL_WEIGHT_MULTIPLIER}
            for f in self.long_term_pool_paths
        ]

        total_available = len(pool_candidates)
        if total_available < TRAINING_POOL_SAMPLE_SIZE:
            print(f"âš ï¸  è­¦å‘Šï¼šå¯ç”¨æ¨¡å‹æ•°é‡ ({total_available}) å°‘äºæ‰€éœ€é‡‡æ ·æ•°é‡ ({TRAINING_POOL_SAMPLE_SIZE})")

        selected_pool_models = []
        if pool_candidates:
            sample_size = min(TRAINING_POOL_SAMPLE_SIZE, total_available)
            weights = np.array([c['sampling_weight'] for c in pool_candidates])
            probs = weights / weights.sum()
            selected_indices = np.random.choice(len(pool_candidates), size=sample_size, replace=False, p=probs)
            selected_pool_models = [pool_candidates[i]['filename'] for i in selected_indices]

        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)
        if main_opponent_name not in self.elo_ratings:
            self.elo_ratings[main_opponent_name] = self.default_elo
        main_elo = self.elo_ratings[main_opponent_name]

        # ã€ä¿®æ”¹ã€‘è¿™é‡Œåªæ”¶é›†æ¨¡å‹è·¯å¾„
        all_model_names = selected_pool_models + [main_opponent_name]

        calculate_elo_weight = lambda name: np.exp(-abs(main_elo - self.elo_ratings.get(name, self.default_elo)) / ELO_WEIGHT_TEMPERATURE)
        pool_weights = [{'name': name, 'weight': calculate_elo_weight(name)} for name in all_model_names]

        pool_total_weight = sum(w['weight'] for w in pool_weights if w['name'] != main_opponent_name)
        main_current_weight = next((w['weight'] for w in pool_weights if w['name'] == main_opponent_name), 0)

        min_main_weight = pool_total_weight * MAIN_OPPONENT_MIN_WEIGHT_RATIO / (1 - MAIN_OPPONENT_MIN_WEIGHT_RATIO) if (1 - MAIN_OPPONENT_MIN_WEIGHT_RATIO) > 0 else float('inf')

        if main_current_weight < min_main_weight:
            pool_weights = [
                {**w, 'weight': min_main_weight} if w['name'] == main_opponent_name else w
                for w in pool_weights
            ]

        total_weight = sum(w['weight'] for w in pool_weights)

        if total_weight > 0:
            # ã€ä¿®æ”¹ã€‘æœ€ç»ˆæ•°æ®ç»“æ„åªåŒ…å«è·¯å¾„å’Œæƒé‡ï¼Œä¸å†åŒ…å«æ¨¡å‹å®ä¾‹
            self.combined_opponent_data = [
                {
                    'path': os.path.join(OPPONENT_POOL_DIR, w['name']) if w['name'] != main_opponent_name else MAIN_OPPONENT_PATH,
                    'weight': w['weight'] / total_weight
                }
                for w in pool_weights
            ]
        else:
            uniform_weight = 1.0 / len(all_model_names) if all_model_names else 0.0
            self.combined_opponent_data = [
                {
                    'path': os.path.join(OPPONENT_POOL_DIR, name) if name != main_opponent_name else MAIN_OPPONENT_PATH,
                    'weight': uniform_weight
                }
                for name in all_model_names
            ]

        print("\n--- å¯¹æ‰‹æ± çŠ¶æ€ (IPCæ¨¡å¼) ---")
        print(f"çŸ­æœŸæ±  ({len(self.short_term_pool_paths)}/{SHORT_TERM_POOL_SIZE}): {self.short_term_pool_paths}")
        print(f"é•¿æœŸæ±  ({len(self.long_term_pool_paths)}/{LONG_TERM_POOL_SIZE}): {self.long_term_pool_paths}")
        print(f"é•¿æœŸæ± ä»£æ•°å·®å€¼æŒ‡æ•°: {self.long_term_power_of_2} (å½“å‰è¦æ±‚å·®å€¼: {2**self.long_term_power_of_2})")

        print(f"\nå¯¹æ‰‹æ± æœ€ç»ˆæƒé‡åˆ†å¸ƒ (æ€»æ¨¡å‹: {len(self.combined_opponent_data)}):")
        sorted_data = sorted(self.combined_opponent_data, key=lambda x: os.path.basename(x['path']))
        for item in sorted_data:
            name = os.path.basename(item['path'])
            elo = self.elo_ratings.get(name, self.default_elo)
            is_main = "â˜…ä¸»å®°è€…" if item['path'] == MAIN_OPPONENT_PATH else ""
            print(f"  - {name:<25} (Elo: {elo:.0f}, æƒé‡: {item['weight']:.2%}) {is_main}")

    def _prepare_environment_and_models(self):
        """ã€IPCç‰ˆã€‘å‡†å¤‡æ¨¡å‹ã€ç¯å¢ƒï¼Œå¹¶å¯åŠ¨é¢„æµ‹å™¨è¿›ç¨‹ã€‚"""
        print("\n--- [æ­¥éª¤ 2/5] å‡†å¤‡ç¯å¢ƒå’Œæ¨¡å‹ (IPCæ¨¡å¼) ---")

        # --- 1. å¯åŠ¨é¢„æµ‹å™¨è¿›ç¨‹ ---
        print(">>> æ­£åœ¨å¯åŠ¨ä¸­å¤®é¢„æµ‹å™¨è¿›ç¨‹...")
        self.prediction_q = mp.Queue()
        self.result_q = mp.Queue()

        # åˆå§‹åŒ–çš„å¯¹æ‰‹æ•°æ®ï¼ˆä»…è·¯å¾„å’Œæƒé‡ï¼‰
        initial_opponent_data_for_worker = self.combined_opponent_data.copy()

        self.predictor_process = mp.Process(
            target=_predictor_worker,
            args=(self.prediction_q, self.result_q, initial_opponent_data_for_worker),
            daemon=True # è®¾ç½®ä¸ºå®ˆæŠ¤è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶å®ƒä¹Ÿä¼šé€€å‡º
        )
        self.predictor_process.start()
        print(f"âœ… ä¸­å¤®é¢„æµ‹å™¨è¿›ç¨‹å·²å¯åŠ¨ (PID: {self.predictor_process.pid})")

        # --- 2. å‡†å¤‡è®­ç»ƒç¯å¢ƒ ---
        run_name = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
        self.tensorboard_log_run_path = os.path.join(TENSORBOARD_LOG_PATH, run_name)
        print(f"TensorBoard æ—¥å¿—å°†ä¿å­˜åˆ°: {self.tensorboard_log_run_path}")

        print(f"åˆ›å»º {N_ENVS} ä¸ªå¹¶è¡Œçš„è®­ç»ƒç¯å¢ƒ...")
        vec_env_cls = SubprocVecEnv if N_ENVS > 1 else DummyVecEnv

        # ã€ä¿®å¤ã€‘ä¸ºæ¯ä¸ªå¹¶è¡Œç¯å¢ƒæä¾›ä¸åŒçš„å‚æ•°
        env_kwargs_list = [{
            'opponent_data': self.combined_opponent_data,
            'shaping_coef': SHAPING_COEF_INITIAL,
            'prediction_q': self.prediction_q,
            'result_q': self.result_q,
            'worker_id': i
        } for i in range(N_ENVS)]
        
        if vec_env_cls == SubprocVecEnv:
            # æ‰‹åŠ¨åˆ›å»º env_fns åˆ—è¡¨
            env_fns = [partial(_init_env, GameEnvironment, **kwargs) for kwargs in env_kwargs_list]
            self.env = SubprocVecEnv(env_fns)
        else: # DummyVecEnv
            # make_vec_env å¯¹ DummyVecEnv çš„å¤„ç†æ˜¯æ­£ç¡®çš„
            self.env = make_vec_env(
                GameEnvironment, n_envs=N_ENVS, vec_env_cls=DummyVecEnv,
                env_kwargs=env_kwargs_list[0]
            )

        # --- 3. åŠ è½½å­¦ä¹ è€…æ¨¡å‹ ---
        print(f"åŠ è½½å­¦ä¹ è€…æ¨¡å‹: {os.path.basename(CHALLENGER_PATH)}")
        self.model = load_ppo_model_with_hyperparams(
            CHALLENGER_PATH,
            env=self.env,
            tensorboard_log=self.tensorboard_log_run_path
        )
        print("âœ… ç¯å¢ƒå’Œæ¨¡å‹å‡†å¤‡å®Œæˆï¼")


    def _train_learner(self):
        """è®­ç»ƒå­¦ä¹ è€…æ¨¡å‹ï¼ˆå³æŒ‘æˆ˜è€…ï¼‰ã€‚"""
        assert self.model is not None, "Model not initialized"
        assert self.env is not None, "Environment not initialized"
        print(f"ğŸ‹ï¸  é˜¶æ®µä¸€: æŒ‘æˆ˜è€…è¿›è¡Œ {STEPS_PER_LOOP:,} æ­¥è®­ç»ƒ...")
        start_time = time.time()
        self.model.learn(total_timesteps=STEPS_PER_LOOP, reset_num_timesteps=False, progress_bar=PPO_SHOW_PROGRESS)
        self.model.save(CHALLENGER_PATH)
        elapsed_time = time.time() - start_time
        print(f"âœ… è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {elapsed_time:.1f}ç§’, æ€»æ­¥æ•°: {self.model.num_timesteps:,}")
        print(f"âœ… æŒ‘æˆ˜è€…è®­ç»ƒå®Œæˆï¼Œæ–°å‚æ•°å·²ä¿å­˜è‡³ {os.path.basename(CHALLENGER_PATH)}")

    def _update_elo(self, player_a_name, player_b_name, player_a_win_rate):
        """æ ¹æ®èƒœç‡æ›´æ–°Eloã€‚"""
        player_a_elo = self.elo_ratings.get(player_a_name, self.default_elo)
        player_b_elo = self.elo_ratings.get(player_b_name, self.default_elo)

        expected_win_a = 1 / (1 + 10 ** ((player_b_elo - player_a_elo) / 400))

        new_player_a_elo = player_a_elo + self.elo_k_factor * (player_a_win_rate - expected_win_a)
        new_player_b_elo = player_b_elo - self.elo_k_factor * (player_a_win_rate - expected_win_a)

        self.elo_ratings[player_a_name] = new_player_a_elo
        self.elo_ratings[player_b_name] = new_player_b_elo

        print(f"Elo æ›´æ–° ({player_a_name} vs {player_b_name}, åŸºäºèƒœç‡ {player_a_win_rate:.2%}):")
        print(f"  - {player_a_name}: {player_a_elo:.0f} -> {new_player_a_elo:.0f} (Î” {new_player_a_elo - player_a_elo:+.1f})")
        print(f"  - {player_b_name}: {player_b_elo:.0f} -> {new_player_b_elo:.0f} (Î” {new_player_b_elo - player_b_elo:+.1f})")

    def _evaluate_and_update(self) -> bool:
        """è¯„ä¼°ã€å†³ç­–ã€æ›´æ–°Eloã€è½®æ¢å¯¹æ‰‹ã€åŒæ­¥ç¯å¢ƒçš„å®Œæ•´æµç¨‹ã€‚"""
        assert self.model is not None, "Model not initialized"
        assert self.env is not None, "Environment not initialized"
        print(f"\nğŸ’¾ é˜¶æ®µäºŒ: {os.path.basename(CHALLENGER_PATH)} å‘ {os.path.basename(MAIN_OPPONENT_PATH)} å‘èµ·æŒ‘æˆ˜")

        print(f"\nâš”ï¸  é˜¶æ®µä¸‰: å¯åŠ¨é•œåƒå¯¹å±€è¯„ä¼°...")
        win_rate = evaluate_models(CHALLENGER_PATH, MAIN_OPPONENT_PATH, show_progress=True)

        print(f"\nğŸ‘‘ é˜¶æ®µå››: å†³ç­–...")
        challenger_name = os.path.basename(CHALLENGER_PATH)
        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)

        self._update_elo(challenger_name, main_opponent_name, win_rate)

        if win_rate > EVALUATION_THRESHOLD:
            print(f"ğŸ† æŒ‘æˆ˜æˆåŠŸ (èƒœç‡ {win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")

            old_main_gen = self.latest_generation + 1
            new_opponent_name = f"opponent_{old_main_gen}.zip"
            new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)

            shutil.copy(MAIN_OPPONENT_PATH, new_opponent_path)
            self.elo_ratings[new_opponent_name] = self.elo_ratings[main_opponent_name]
            print(f"æ—§ä¸»å®°è€… {main_opponent_name} å·²å­˜å…¥å¯¹æ‰‹æ± ï¼Œåä¸º {new_opponent_name}")

            shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
            self.elo_ratings[main_opponent_name] = self.elo_ratings[challenger_name]
            print(f"æŒ‘æˆ˜è€…å·²æˆä¸ºæ–°ä¸»å®°è€…ï¼")

            self._manage_opponent_pool(new_opponent_path=new_opponent_path)

            # ã€ä¿®æ”¹ã€‘é€šè¿‡é˜Ÿåˆ—å‘é¢„æµ‹å™¨å’Œç¯å¢ƒå¹¿æ’­æ›´æ–°
            print(f"ğŸ”¥ å‘é€æŒ‡ä»¤ï¼Œæ›´æ–°ä¸­å¤®é¢„æµ‹å™¨å’Œæ‰€æœ‰ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ...")
            if self.prediction_q:
                self.prediction_q.put({
                    "command": "RELOAD_MODELS",
                    "data": self.combined_opponent_data
                })
            self.env.env_method("reload_opponent_pool", new_opponent_data=self.combined_opponent_data)
            print("âœ… æ‰€æœ‰ç¯å¢ƒä¸­çš„å¯¹æ‰‹æ± å‡å·²æˆåŠŸæ›´æ–°ï¼")

            return True
        else:
            print(f"ğŸ›¡ï¸  æŒ‘æˆ˜å¤±è´¥ (èƒœç‡ {win_rate:.2%} <= {EVALUATION_THRESHOLD:.2%})ã€‚ä¸»å®°è€…ä¿æŒä¸å˜ã€‚")
            print("...æŒ‘æˆ˜è€…å°†ç»§ç»­è®­ç»ƒä»¥å‘èµ·ä¸‹ä¸€æ¬¡æŒ‘æˆ˜ã€‚")
            self._save_elo_and_generations()
            return False

    def run(self):
        """å¯åŠ¨å¹¶æ‰§è¡Œå®Œæ•´çš„è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒæµç¨‹ã€‚"""
        try:
            self._prepare_environment_and_models()
            assert self.model is not None, "Model not initialized"
            assert self.env is not None, "Environment not initialized"
            print("\n--- [æ­¥éª¤ 3/5] å¼€å§‹Eloè‡ªæˆ‘å¯¹å¼ˆä¸»å¾ªç¯ ---")
            successful_challenges = 0

            total_decay_loops = min(TOTAL_TRAINING_LOOPS, SHAPING_DECAY_END_LOOP)
            if total_decay_loops > 0:
                decay_per_loop = (SHAPING_COEF_INITIAL - SHAPING_COEF_FINAL) / total_decay_loops
            else:
                decay_per_loop = 0

            for i in range(1, TOTAL_TRAINING_LOOPS + 1):
                print(f"\n{'='*70}\nğŸ”„ è®­ç»ƒå¾ªç¯ {i}/{TOTAL_TRAINING_LOOPS} | æˆåŠŸæŒ‘æˆ˜æ¬¡æ•°: {successful_challenges}\n{'='*70}")
                try:
                    if SHAPING_COEF_INITIAL > SHAPING_COEF_FINAL:
                        if i <= total_decay_loops:
                            current_coef = SHAPING_COEF_INITIAL - (i * decay_per_loop)
                        else:
                            current_coef = SHAPING_COEF_FINAL

                        self.env.set_attr("shaping_coef", current_coef)

                        if PPO_VERBOSE > 0 and (i < total_decay_loops + 1):
                            actual_coef = self.env.get_attr("shaping_coef")[0]
                            print(f"      [INFO] å¥–åŠ±å¡‘å½¢ç³»æ•° (shaping_coef) å·²æ›´æ–°ä¸º: {actual_coef:.4f}")

                    self._train_learner()
                    if self._evaluate_and_update():
                        successful_challenges += 1
                except Exception as e:
                    print(f"âš ï¸ è®­ç»ƒå¾ªç¯ {i} å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    print("...ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯...")
                    continue

            self.model.save(FINAL_MODEL_PATH)
            print(f"\n--- [æ­¥éª¤ 4/5] è®­ç»ƒå®Œæˆï¼ ---")

        finally:
            print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆçš„çŠ¶æ€æ–‡ä»¶...")
            self._save_elo_and_generations()

            # --- ã€æ–°å¢ã€‘æ¸…ç†IPCèµ„æº ---
            if self.predictor_process and self.predictor_process.is_alive():
                 print("\n--- [æ­¥éª¤ 5/5] æ¸…ç†èµ„æº ---")
                 print(">>> æ­£åœ¨å‘é¢„æµ‹å™¨è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·...")
                 if self.prediction_q: self.prediction_q.put("STOP")
                 self.predictor_process.join(timeout=5) # ç­‰å¾…æœ€å¤š5ç§’
                 if self.predictor_process.is_alive():
                     print(">>> é¢„æµ‹å™¨è¿›ç¨‹æœªèƒ½æ­£å¸¸å…³é—­ï¼Œå¼ºåˆ¶ç»ˆæ­¢ã€‚")
                     self.predictor_process.terminate()
                 else:
                     print(">>> é¢„æµ‹å™¨è¿›ç¨‹å·²æˆåŠŸå…³é—­ã€‚")

            if self.env:
                print(">>> æ­£åœ¨å…³é—­ç¯å¢ƒ...")
                self.env.close()
            print("âœ… èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == '__main__':
    trainer = SelfPlayTrainer()
    trainer.run()