# training/trainer.py

import os
import shutil
import time
import re
import json
import numpy as np
import sys

from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from sb3_contrib import MaskablePPO

from utils.constants import *
from game.environment import GameEnvironment
from game.policy import CustomActorCriticPolicy
from training.evaluator import evaluate_models

def create_new_ppo_model(env=None, tensorboard_log=None):
    """
    åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„éšæœºåˆå§‹åŒ–çš„PPOæ¨¡å‹ã€‚
    """
    model = MaskablePPO(
        policy=CustomActorCriticPolicy,
        env=env,
        learning_rate=INITIAL_LR,
        clip_range=PPO_CLIP_RANGE,
        n_steps=PPO_N_STEPS,
        batch_size=PPO_BATCH_SIZE,
        n_epochs=PPO_N_EPOCHS,
        gae_lambda=PPO_GAE_LAMBDA,
        vf_coef=PPO_VF_COEF,
        ent_coef=PPO_ENT_COEF,
        max_grad_norm=PPO_MAX_GRAD_NORM,
        tensorboard_log=tensorboard_log,
        device=PPO_DEVICE,
        verbose=PPO_VERBOSE,
        policy_kwargs={
            'features_extractor_kwargs': {
                'features_dim': NETWORK_FEATURES_DIM,
                'num_res_blocks': NETWORK_NUM_RES_BLOCKS,
                'num_hidden_channels': NETWORK_NUM_HIDDEN_CHANNELS
            }
        }
    )
    return model

def load_ppo_model_with_hyperparams(model_path: str, env=None, tensorboard_log=None):
    """
    åŠ è½½PPOæ¨¡å‹å¹¶åº”ç”¨è‡ªå®šä¹‰è¶…å‚æ•°ã€‚
    """
    model = MaskablePPO.load(
        model_path,
        env=env,
        learning_rate=INITIAL_LR,
        clip_range=PPO_CLIP_RANGE,
        tensorboard_log=tensorboard_log,
        n_steps=PPO_N_STEPS,
        device=PPO_DEVICE
    )
    model.batch_size = PPO_BATCH_SIZE
    model.n_epochs = PPO_N_EPOCHS
    model.gae_lambda = PPO_GAE_LAMBDA
    model.vf_coef = PPO_VF_COEF
    model.ent_coef = PPO_ENT_COEF
    model.max_grad_norm = PPO_MAX_GRAD_NORM
    return model

class SelfPlayTrainer:
    """
    ã€V6 é‡æ„ç‰ˆã€‘
    - ä»¥ "æŒ‘æˆ˜è€…" ä¸ºæ ¸å¿ƒè¿›è¡ŒæŒç»­è®­ç»ƒã€‚
    - å¯¹æ‰‹æ± åˆ†ä¸º "é•¿æœŸ" å’Œ "çŸ­æœŸ" æ± ã€‚
    - å®ç°äº†æ›´ç§‘å­¦çš„å†å²æ¨¡å‹ä¿ç•™å’Œé‡‡æ ·æœºåˆ¶ã€‚
    """
    def __init__(self):
        self.model = None
        self.env = None
        self.tensorboard_log_run_path = None
        
        # --- å¯¹æ‰‹æ± æ ¸å¿ƒå±æ€§ (é‡æ„) ---
        self.long_term_pool_paths = []
        self.short_term_pool_paths = []
        self.opponent_pool_for_env = []
        self.opponent_pool_weights = []

        # --- Eloä¸æ¨¡å‹ç®¡ç† ---
        self.elo_ratings = {}
        self.model_generations = {} # æ–°å¢: ç”¨äºè¿½è¸ªæ¨¡å‹ä»£æ•°
        self.latest_generation = 0
        self.default_elo = ELO_DEFAULT
        self.elo_k_factor = ELO_K_FACTOR
        
        self._setup()

    def _setup(self):
        """
        ã€é‡æ„ã€‘æ‰§è¡Œæ‰€æœ‰å¯åŠ¨å‰çš„å‡†å¤‡å·¥ä½œï¼Œç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸã€‚
        """
        print("--- [æ­¥éª¤ 1/5] åˆå§‹åŒ–è®¾ç½® ---")
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        os.makedirs(TENSORBOARD_LOG_PATH, exist_ok=True)

        self._load_elo_and_generations()

        # æ ¸å¿ƒæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
        if not os.path.exists(CHALLENGER_PATH):
            print(">>> æŒ‘æˆ˜è€…æ¨¡å‹ä¸å­˜åœ¨ï¼Œè§†ä¸ºä»é›¶å¼€å§‹è®­ç»ƒã€‚")
            self._create_initial_models()
        
        if not os.path.exists(MAIN_OPPONENT_PATH):
            print(">>> ä¸»å®°è€…æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†ä»ç°æœ‰æŒ‘æˆ˜è€…æ¨¡å‹å¤åˆ¶ã€‚")
            shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
            main_opp_name = os.path.basename(MAIN_OPPONENT_PATH)
            challenger_name = os.path.basename(CHALLENGER_PATH)
            if main_opp_name not in self.elo_ratings:
                self.elo_ratings[main_opp_name] = self.elo_ratings.get(challenger_name, self.default_elo)
            if main_opp_name not in self.model_generations:
                 self.model_generations[main_opp_name] = self.model_generations.get(challenger_name, 0)
            self._save_elo_and_generations()

        self._manage_opponent_pool()

    def _create_initial_models(self):
        """åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„éšæœºåˆå§‹åŒ–æ¨¡å‹ä½œä¸ºè®­ç»ƒèµ·ç‚¹ã€‚"""
        print("æ­£åœ¨åˆ›å»ºä¸´æ—¶ç¯å¢ƒä»¥åˆå§‹åŒ–æ¨¡å‹...")
        temp_env = GameEnvironment()
        
        print("æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        new_model = create_new_ppo_model(env=temp_env)
        
        # ä¿å­˜ä¸ºæŒ‘æˆ˜è€…å’Œä¸»å®°è€…
        new_model.save(CHALLENGER_PATH)
        shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
        print(f"âœ… åˆå§‹æ¨¡å‹å·²åˆ›å»ºå¹¶ä¿å­˜ä¸º 'challenger.zip' å’Œ 'main_opponent.zip'")
        
        # åˆå§‹åŒ–Eloå’Œä»£æ•°
        challenger_name = os.path.basename(CHALLENGER_PATH)
        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)
        self.elo_ratings[challenger_name] = self.default_elo
        self.elo_ratings[main_opponent_name] = self.default_elo
        self.model_generations[challenger_name] = 0
        self.model_generations[main_opponent_name] = 0
        self.latest_generation = 0
        self._save_elo_and_generations()
        
        temp_env.close()
        print("âœ… ä¸´æ—¶ç¯å¢ƒå·²æ¸…ç†")

    def _load_elo_and_generations(self):
        """ä»JSONæ–‡ä»¶åŠ è½½Eloè¯„åˆ†å’Œæ¨¡å‹ä»£æ•°ã€‚"""
        elo_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        if os.path.exists(elo_file):
            try:
                with open(elo_file, 'r') as f:
                    data = json.load(f)
                    self.elo_ratings = data.get("elo", {})
                    self.model_generations = data.get("generations", {})
                    self.latest_generation = data.get("latest_generation", 0)
            except (json.JSONDecodeError, IOError, KeyError) as e:
                print(f"è­¦å‘Šï¼šè¯»å–Eloæ–‡ä»¶å¤±è´¥æˆ–æ ¼å¼ä¸å®Œæ•´: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
                self.elo_ratings = {}
                self.model_generations = {}
                self.latest_generation = 0
    
    def _save_elo_and_generations(self):
        """å°†Eloå’Œæ¨¡å‹ä»£æ•°ä¿å­˜åˆ°åŒä¸€ä¸ªJSONæ–‡ä»¶ã€‚"""
        elo_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        data = {
            "elo": self.elo_ratings,
            "generations": self.model_generations,
            "latest_generation": self.latest_generation
        }
        try:
            with open(elo_file, 'w') as f:
                json.dump(data, f, indent=4)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜Eloè¯„åˆ†æ–‡ä»¶: {e}")

    def _manage_opponent_pool(self, new_opponent_path=None):
        """
        ã€æ ¸å¿ƒé‡æ„ã€‘ç®¡ç†é•¿æœŸå’ŒçŸ­æœŸå¯¹æ‰‹æ± ã€‚
        - æ·»åŠ æ–°å¯¹æ‰‹ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚
        - æ ¹æ®è§„åˆ™å¯¹æ± è¿›è¡Œä¿®å‰ªã€‚
        """
        # å¦‚æœæœ‰æ–°å¯¹æ‰‹åŠ å…¥ï¼ˆå³è¢«å‡»è´¥çš„ä¸»å®°è€…ï¼‰ï¼Œåˆ™å¢åŠ ä»£æ•°å¹¶è®°å½•
        if new_opponent_path:
            self.latest_generation += 1
            new_opponent_name = os.path.basename(new_opponent_path)
            self.model_generations[new_opponent_name] = self.latest_generation
            self._save_elo_and_generations()

        # 1. ä»ç£ç›˜åŠ è½½æ‰€æœ‰å¯¹æ‰‹ï¼Œå¹¶æŒ‰ä»£æ•°æ’åº
        all_opponents = []
        for filename in os.listdir(OPPONENT_POOL_DIR):
            if filename.endswith('.zip'):
                gen = self.model_generations.get(filename, 0)
                all_opponents.append((filename, gen))
        
        all_opponents.sort(key=lambda x: x[1], reverse=True) # æŒ‰ä»£æ•°é™åºæ’ï¼Œæœ€æ–°çš„åœ¨å‰

        # 2. æ¸…ç©ºç°æœ‰æ± 
        self.short_term_pool_paths = []
        self.long_term_pool_paths = []

        # 3. å¡«å……çŸ­æœŸæ± 
        self.short_term_pool_paths = [os.path.join(OPPONENT_POOL_DIR, name) for name, gen in all_opponents[:SHORT_TERM_POOL_SIZE]]

        # 4. å¡«å……é•¿æœŸæ± 
        # ä»çŸ­æœŸæ± ä¹‹åå¼€å§‹è€ƒè™‘
        candidates_for_long_term = all_opponents[SHORT_TERM_POOL_SIZE:]
        
        # é•¿æœŸæ± çš„ä¿ç•™è§„åˆ™ï¼šä¸æœ€æ–°æ¨¡å‹çš„ä»£æ•°å·®ä¸º2çš„æŒ‡æ•°
        for opp_name, opp_gen in candidates_for_long_term:
            if len(self.long_term_pool_paths) >= LONG_TERM_POOL_SIZE:
                break
            
            age = self.latest_generation - opp_gen
            if age > 0 and (age & (age - 1) == 0): # æ£€æŸ¥ageæ˜¯å¦æ˜¯2çš„å¹‚
                self.long_term_pool_paths.append(os.path.join(OPPONENT_POOL_DIR, opp_name))
        
        # 5. æ¸…ç†ç£ç›˜ä¸Šä¸å†éœ€è¦çš„æ¨¡å‹
        current_pool_names = {os.path.basename(p) for p in self.short_term_pool_paths + self.long_term_pool_paths}
        for filename, _ in all_opponents:
            if filename not in current_pool_names:
                print(f"âœ‚ï¸ æ¸…ç†è¿‡æ—¶å¯¹æ‰‹: {filename}")
                os.remove(os.path.join(OPPONENT_POOL_DIR, filename))
                self.elo_ratings.pop(filename, None)
                self.model_generations.pop(filename, None)
        
        self._save_elo_and_generations()
        self._update_opponent_weights()

    def _update_opponent_weights(self):
        """
        ã€é‡æ„ã€‘æ ¹æ®Eloè¯„åˆ†è®¡ç®—é‡‡æ ·æƒé‡ï¼Œåˆå¹¶é•¿çŸ­æœŸæ± ã€‚
        """
        self.opponent_pool_for_env = self.short_term_pool_paths + self.long_term_pool_paths
        
        # ç¡®ä¿ä¸»å®°è€…æœ‰Elo
        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)
        if main_opponent_name not in self.elo_ratings:
            self.elo_ratings[main_opponent_name] = self.default_elo
        main_elo = self.elo_ratings[main_opponent_name]
        
        weights = []
        # è®¡ç®—æ± ä¸­æ¯ä¸ªå¯¹æ‰‹çš„æƒé‡
        for path in self.opponent_pool_for_env:
            opp_name = os.path.basename(path)
            opp_elo = self.elo_ratings.get(opp_name, self.default_elo)
            elo_diff = abs(main_elo - opp_elo)
            weight = np.exp(-elo_diff / ELO_WEIGHT_TEMPERATURE)
            weights.append(weight)
        
        # ä¸»å®°è€…ä¹Ÿæœ‰ä¸€å®šæ¦‚ç‡æˆä¸ºå¯¹æ‰‹
        main_opponent_weight = sum(weights) * 0.3 if weights else 1.0

        final_pool_for_env = self.opponent_pool_for_env + [MAIN_OPPONENT_PATH]
        all_weights = weights + [main_opponent_weight]

        total_weight = sum(all_weights)
        if total_weight > 0:
            self.opponent_pool_weights = [w / total_weight for w in all_weights]
        else:
            num_opps = len(final_pool_for_env)
            self.opponent_pool_weights = [1.0 / num_opps] * num_opps if num_opps > 0 else []

        print("\n--- å¯¹æ‰‹æ± çŠ¶æ€ ---")
        print(f"çŸ­æœŸæ±  ({len(self.short_term_pool_paths)}/{SHORT_TERM_POOL_SIZE}): {[os.path.basename(p) for p in self.short_term_pool_paths]}")
        print(f"é•¿æœŸæ±  ({len(self.long_term_pool_paths)}/{LONG_TERM_POOL_SIZE}): {[os.path.basename(p) for p in self.long_term_pool_paths]}")
        print("\nå¯¹æ‰‹æ± é‡‡æ ·æƒé‡å·²æ›´æ–°:")
        for path, weight in zip(final_pool_for_env, self.opponent_pool_weights):
            elo = self.elo_ratings.get(os.path.basename(path), self.default_elo)
            print(f"  - {os.path.basename(path)} (Elo: {elo:.0f}, æƒé‡: {weight:.2%})")

    def _prepare_environment_and_models(self):
        """å‡†å¤‡ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œç¯å¢ƒã€‚"""
        print("\n--- [æ­¥éª¤ 2/5] å‡†å¤‡ç¯å¢ƒå’Œæ¨¡å‹ ---")
        run_name = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
        self.tensorboard_log_run_path = os.path.join(TENSORBOARD_LOG_PATH, run_name)
        print(f"TensorBoard æ—¥å¿—å°†ä¿å­˜åˆ°: {self.tensorboard_log_run_path}")

        print(f"åˆ›å»º {N_ENVS} ä¸ªå¹¶è¡Œçš„è®­ç»ƒç¯å¢ƒ...")
        vec_env_cls = SubprocVecEnv if N_ENVS > 1 else DummyVecEnv
        self.env = make_vec_env(
            GameEnvironment, n_envs=N_ENVS, vec_env_cls=vec_env_cls,
            env_kwargs={
                'opponent_pool': self.opponent_pool_for_env,
                'opponent_weights': self.opponent_pool_weights,
                # ä½¿ç”¨å¸¸é‡ä¸­çš„åˆå§‹å€¼
                'shaping_coef': SHAPING_COEF_INITIAL 
            }
        )
        
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘è®­ç»ƒå™¨æ°¸è¿œåªåŠ è½½å’Œè®­ç»ƒæŒ‘æˆ˜è€…æ¨¡å‹
        print(f"åŠ è½½å­¦ä¹ è€…æ¨¡å‹: {os.path.basename(CHALLENGER_PATH)}")
        self.model = load_ppo_model_with_hyperparams(
            CHALLENGER_PATH,
            env=self.env,
            tensorboard_log=self.tensorboard_log_run_path
        )
        print("âœ… ç¯å¢ƒå’Œæ¨¡å‹å‡†å¤‡å®Œæˆï¼")

    def _train_learner(self):
        """è®­ç»ƒå­¦ä¹ è€…æ¨¡å‹ï¼ˆå³æŒ‘æˆ˜è€…ï¼‰ã€‚"""
        print(f"ğŸ‹ï¸  é˜¶æ®µä¸€: æŒ‘æˆ˜è€…è¿›è¡Œ {STEPS_PER_LOOP:,} æ­¥è®­ç»ƒ...")
        start_time = time.time()
        self.model.learn(total_timesteps=STEPS_PER_LOOP, reset_num_timesteps=False, progress_bar=PPO_SHOW_PROGRESS)
        # è®­ç»ƒåç«‹å³ä¿å­˜ï¼Œè¦†ç›–æ—§çš„æŒ‘æˆ˜è€…æ¨¡å‹
        self.model.save(CHALLENGER_PATH)
        elapsed_time = time.time() - start_time
        print(f"âœ… è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {elapsed_time:.1f}ç§’, æ€»æ­¥æ•°: {self.model.num_timesteps:,}")
        print(f"âœ… æŒ‘æˆ˜è€…è®­ç»ƒå®Œæˆï¼Œæ–°å‚æ•°å·²ä¿å­˜è‡³ {os.path.basename(CHALLENGER_PATH)}")

    def _update_elo(self, player_a_name, player_b_name, player_a_win_rate):
        """æ ¹æ®èƒœç‡æ›´æ–°Eloã€‚"""
        player_a_elo = self.elo_ratings.get(player_a_name, self.default_elo)
        player_b_elo = self.elo_ratings.get(player_b_name, self.default_elo)

        expected_win_a = 1 / (1 + 10 ** ((player_b_elo - player_a_elo) / 400))
        
        new_player_a_elo = player_a_elo + self.elo_k_factor * (player_a_win_rate - expected_win_a)
        new_player_b_elo = player_b_elo - self.elo_k_factor * (player_a_win_rate - expected_win_a)
        
        self.elo_ratings[player_a_name] = new_player_a_elo
        self.elo_ratings[player_b_name] = new_player_b_elo
        
        print(f"Elo æ›´æ–° ({player_a_name} vs {player_b_name}, åŸºäºèƒœç‡ {player_a_win_rate:.2%}):")
        print(f"  - {player_a_name}: {player_a_elo:.0f} -> {new_player_a_elo:.0f} (Î” {new_player_a_elo - player_a_elo:+.1f})")
        print(f"  - {player_b_name}: {player_b_elo:.0f} -> {new_player_b_elo:.0f} (Î” {new_player_b_elo - player_b_elo:+.1f})")
        
    def _evaluate_and_update(self) -> bool:
        """è¯„ä¼°ã€å†³ç­–ã€æ›´æ–°Eloã€è½®æ¢å¯¹æ‰‹ã€åŒæ­¥ç¯å¢ƒçš„å®Œæ•´æµç¨‹ã€‚"""
        print(f"\nğŸ’¾ é˜¶æ®µäºŒ: {os.path.basename(CHALLENGER_PATH)} å‘ {os.path.basename(MAIN_OPPONENT_PATH)} å‘èµ·æŒ‘æˆ˜")
        
        print(f"\nâš”ï¸  é˜¶æ®µä¸‰: å¯åŠ¨é•œåƒå¯¹å±€è¯„ä¼°...")
        win_rate = evaluate_models(CHALLENGER_PATH, MAIN_OPPONENT_PATH, show_progress=True)
        
        print(f"\nğŸ‘‘ é˜¶æ®µå››: å†³ç­–...")
        challenger_name = os.path.basename(CHALLENGER_PATH)
        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)

        self._update_elo(challenger_name, main_opponent_name, win_rate)
        
        if win_rate > EVALUATION_THRESHOLD:
            print(f"ğŸ† æŒ‘æˆ˜æˆåŠŸ (èƒœç‡ {win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
            
            # 1. ç¡®å®šæ—§ä¸»å®°è€…å…¥æ± åçš„æ–°åå­—
            old_main_gen = self.latest_generation + 1
            new_opponent_name = f"opponent_{old_main_gen}.zip"
            new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)
            
            # 2. å°†æ—§ä¸»å®°è€…å¤åˆ¶åˆ°å¯¹æ‰‹æ± 
            shutil.copy(MAIN_OPPONENT_PATH, new_opponent_path)
            self.elo_ratings[new_opponent_name] = self.elo_ratings[main_opponent_name]
            print(f"æ—§ä¸»å®°è€… {main_opponent_name} å·²å­˜å…¥å¯¹æ‰‹æ± ï¼Œåä¸º {new_opponent_name}")
            
            # 3. æŒ‘æˆ˜è€…æ™‹å‡ä¸ºæ–°ä¸»å®°è€…
            shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
            # ä¸»å®°è€…çš„Eloç›´æ¥ç»§æ‰¿æ™‹å‡åçš„æŒ‘æˆ˜è€…Elo
            self.elo_ratings[main_opponent_name] = self.elo_ratings[challenger_name]
            print(f"æŒ‘æˆ˜è€…å·²æˆä¸ºæ–°ä¸»å®°è€…ï¼")

            # 4. æ›´æ–°å¯¹æ‰‹æ± ç»“æ„å¹¶é‡æ–°è®¡ç®—æƒé‡
            self._manage_opponent_pool(new_opponent_path=new_opponent_path)
            
            # 5. åœ¨æ‰€æœ‰å¹¶è¡Œç¯å¢ƒä¸­æ›´æ–°å¯¹æ‰‹æ± 
            print(f"ğŸ”¥ å‘é€æŒ‡ä»¤ï¼Œåœ¨æ‰€æœ‰ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒä¸­æ›´æ–°å¯¹æ‰‹æ± ...")
            self.env.env_method("reload_opponent_pool", new_pool=self.opponent_pool_for_env, new_weights=self.opponent_pool_weights)
            print("âœ… æ‰€æœ‰ç¯å¢ƒä¸­çš„å¯¹æ‰‹æ± å‡å·²æˆåŠŸæ›´æ–°ï¼")
            
            return True
        else:
            print(f"ğŸ›¡ï¸  æŒ‘æˆ˜å¤±è´¥ (èƒœç‡ {win_rate:.2%} <= {EVALUATION_THRESHOLD:.2%})ã€‚ä¸»å®°è€…ä¿æŒä¸å˜ã€‚")
            print("...æŒ‘æˆ˜è€…å°†ç»§ç»­è®­ç»ƒä»¥å‘èµ·ä¸‹ä¸€æ¬¡æŒ‘æˆ˜ã€‚")
            self._save_elo_and_generations() # ä»…ä¿å­˜Eloå’Œä»£æ•°æ›´æ–°
            return False

    def run(self):
        """å¯åŠ¨å¹¶æ‰§è¡Œå®Œæ•´çš„è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒæµç¨‹ã€‚"""
        try:
            self._prepare_environment_and_models()
            print("\n--- [æ­¥éª¤ 3/5] å¼€å§‹Eloè‡ªæˆ‘å¯¹å¼ˆä¸»å¾ªç¯ ---")
            successful_challenges = 0
            
            # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘å¥–åŠ±å¡‘å½¢è¡°å‡é€»è¾‘ ---
            total_decay_loops = min(TOTAL_TRAINING_LOOPS, SHAPING_DECAY_END_LOOP)
            if total_decay_loops > 0:
                # è®¡ç®—æ¯ä¸ªå¾ªç¯éœ€è¦è¡°å‡çš„é‡
                decay_per_loop = (SHAPING_COEF_INITIAL - SHAPING_COEF_FINAL) / total_decay_loops
            else:
                decay_per_loop = 0
            
            for i in range(1, TOTAL_TRAINING_LOOPS + 1):
                print(f"\n{'='*70}\nğŸ”„ è®­ç»ƒå¾ªç¯ {i}/{TOTAL_TRAINING_LOOPS} | æˆåŠŸæŒ‘æˆ˜æ¬¡æ•°: {successful_challenges}\n{'='*70}")
                try:
                    # --- åœ¨æ¯ä¸ªå¾ªç¯å¼€å§‹æ—¶æ›´æ–°å¡‘å½¢ç³»æ•° ---
                    if SHAPING_COEF_INITIAL > SHAPING_COEF_FINAL: # ä»…å½“éœ€è¦è¡°å‡æ—¶æ‰æ‰§è¡Œ
                        if i <= total_decay_loops:
                            # çº¿æ€§è¡°å‡
                            current_coef = SHAPING_COEF_INITIAL - (i * decay_per_loop)
                        else:
                            # è¡°å‡ç»“æŸåï¼Œä¿æŒæœ€ç»ˆå€¼
                            current_coef = SHAPING_COEF_FINAL
                        
                        # ä½¿ç”¨ set_attr æ›´æ–°æ‰€æœ‰å¹¶è¡Œç¯å¢ƒçš„å±æ€§
                        self.env.set_attr("shaping_coef", current_coef)
                        
                        # å®šæœŸæ‰“å°å½“å‰ç³»æ•°ä»¥ä¾›ç›‘æ§
                        if PPO_VERBOSE > 0 and (i < total_decay_loops + 1):
                            # get_attr ä¼šè¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªç¯å¢ƒä¸€ä¸ªå€¼ï¼Œæˆ‘ä»¬åªå–ç¬¬ä¸€ä¸ªæ¥æ˜¾ç¤º
                            actual_coef = self.env.get_attr("shaping_coef")[0]
                            print(f"      [INFO] å¥–åŠ±å¡‘å½¢ç³»æ•° (shaping_coef) å·²æ›´æ–°ä¸º: {actual_coef:.4f}")

                    self._train_learner()
                    if self._evaluate_and_update():
                        successful_challenges += 1
                except Exception as e:
                    print(f"âš ï¸ è®­ç»ƒå¾ªç¯ {i} å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    print("...ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯...")
                    continue
            
            self.model.save(FINAL_MODEL_PATH)
            print(f"\n--- [æ­¥éª¤ 4/5] è®­ç»ƒå®Œæˆï¼ ---")
            
        finally:
            print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆçš„Eloè¯„åˆ†å’Œæ¨¡å‹ä»£æ•°...")
            self._save_elo_and_generations()
            if self.env:
                print("\n--- [æ­¥éª¤ 5/5] æ¸…ç†ç¯å¢ƒ ---")
                self.env.close()
                print("âœ… èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == '__main__':
    trainer = SelfPlayTrainer()
    trainer.run()