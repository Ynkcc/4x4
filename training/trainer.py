# training/trainer.py

import os
import shutil
import time
import re
import numpy as np
from stable_baselines3.common.env_util import make_vec_env
# ã€ä¿®å¤ã€‘æ˜ç¡®å¯¼å…¥ SubprocVecEnv å’Œ DummyVecEnv ä»¥æé«˜ä»£ç æ¸…æ™°åº¦
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from sb3_contrib import MaskablePPO

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from utils.constants import *
# ã€ç§»é™¤ã€‘ä¸å†éœ€è¦å­¦ä¹ ç‡è°ƒåº¦å™¨
from game.environment import GameEnvironment
from training.evaluator import evaluate_models
# ã€ä¿®å¤ã€‘ä¸å†éœ€è¦ NeuralAgentï¼Œå› ä¸ºå®ƒçš„å•ä¾‹æ¨¡å¼åœ¨å¤šè¿›ç¨‹ä¸‹æœ‰é—®é¢˜

class SelfPlayTrainer:
    """
    Eloè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå™¨ã€‚
    ã€V3 å¢å¼ºç‰ˆã€‘å¼•å…¥äº†å¯¹æ‰‹æ± æœºåˆ¶ï¼Œå¹¶å®ç°äº†å‘¨æœŸæ€§çš„æ­¥æ•°é‡ç½®ã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œè®¾ç½®æ¨¡å‹å’Œç¯å¢ƒä¸ºNoneã€‚"""
        self.model = None
        self.env = None
        # ã€æ–°å¢ã€‘å¯¹æ‰‹æ± ç›¸å…³å±æ€§
        self.opponent_pool_paths = []
        self.opponent_pool_weights = []
        self.opponent_counter = 0  # å¯¹æ‰‹ç¼–å·è®¡æ•°å™¨
        self.max_recent_opponents = 3  # å›ºå®šé€‰å–æœ€è¿‘çš„3ä¸ªå¯¹æ‰‹æ¨¡å‹
        self._setup()

    def _setup(self):
        """
        æ‰§è¡Œæ‰€æœ‰å¯åŠ¨å‰çš„å‡†å¤‡å·¥ä½œã€‚
        """
        print("--- [æ­¥éª¤ 1/5] åˆå§‹åŒ–è®¾ç½® ---")
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        os.makedirs(TENSORBOARD_LOG_PATH, exist_ok=True)

        # ç¡®ä¿ä¸»å®°è€…æ¨¡å‹å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»åˆå§‹æ¨¡å‹å¤åˆ¶
        if not os.path.exists(MAIN_OPPONENT_PATH):
            print("æœªæ‰¾åˆ°ä¸»å®°è€…æ¨¡å‹ï¼Œå°†ä»æŒ‡å®šçš„åˆå§‹æ¨¡å‹å¼€å§‹å…¨æ–°è®­ç»ƒã€‚")
            initial_model_candidates = [SELF_PLAY_MODEL_PATH, CURRICULUM_MODEL_PATH]
            initial_model_found = None
            for candidate in initial_model_candidates:
                if os.path.exists(candidate):
                    initial_model_found = candidate
                    print(f"æ‰¾åˆ°åˆå§‹æ¨¡å‹: {candidate}")
                    break
            if not initial_model_found:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„åˆå§‹æ¨¡å‹ã€‚å°è¯•è¿‡çš„è·¯å¾„: {initial_model_candidates}")
            shutil.copy(initial_model_found, MAIN_OPPONENT_PATH)
            print(f"å·²å°†åˆå§‹æ¨¡å‹å¤åˆ¶ä¸ºç¬¬ä¸€ä¸ªä¸»å®°è€…: {MAIN_OPPONENT_PATH}")
        else:
            print(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„ä¸»å®°è€…æ¨¡å‹: {MAIN_OPPONENT_PATH}")

        # æ›´æ–°å¯¹æ‰‹æ± 
        self._initialize_opponent_counter()
        self._update_opponent_pool()

    def _initialize_opponent_counter(self):
        """
        ã€æ–°å¢ã€‘åˆå§‹åŒ–å¯¹æ‰‹è®¡æ•°å™¨ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»ç°æœ‰æ–‡ä»¶ä¸­æ¢å¤è®¡æ•°ã€‚
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰«æå·²å­˜åœ¨çš„å¯¹æ‰‹æ–‡ä»¶ï¼Œæ‰¾åˆ°æœ€å¤§çš„ç¼–å·
        max_num = 0
        opponent_pattern = re.compile(r'^opponent(\d+)\.zip$')
        
        for filename in os.listdir(SELF_PLAY_OUTPUT_DIR):
            match = opponent_pattern.match(filename)
            if match:
                try:
                    num = int(match.group(1))
                    max_num = max(max_num, num)
                except ValueError:
                    continue
        
        self.opponent_counter = max_num
        print(f"å¯¹æ‰‹è®¡æ•°å™¨åˆå§‹åŒ–ä¸º: {self.opponent_counter}")

    def _get_opponent_paths(self):
        """
        ã€ä¿®æ”¹ã€‘è·å–å½“å‰å¯¹æ‰‹æ± çš„è·¯å¾„ï¼ŒåŒ…å«ä¸»å®°è€…å’Œæœ€è¿‘çš„3ä¸ªå¯¹æ‰‹æ¨¡å‹ã€‚
        """
        paths = [MAIN_OPPONENT_PATH]  # ä¸»å®°è€…
        
        # æ·»åŠ æœ€è¿‘çš„3ä¸ªå¯¹æ‰‹æ¨¡å‹
        recent_opponents = []
        for i in range(max(1, self.opponent_counter - self.max_recent_opponents + 1), self.opponent_counter + 1):
            opp_path = os.path.join(SELF_PLAY_OUTPUT_DIR, f"opponent{i}.zip")
            if os.path.exists(opp_path):
                recent_opponents.append(opp_path)
        
        # æŒ‰ç¼–å·é™åºæ’åˆ—ï¼Œç¡®ä¿æœ€æ–°çš„åœ¨å‰é¢
        recent_opponents.sort(key=lambda x: int(re.search(r'opponent(\d+)\.zip', x).group(1)), reverse=True)
        
        # åªå–æœ€è¿‘çš„3ä¸ª
        paths.extend(recent_opponents[:self.max_recent_opponents])
        
        return paths

    def _update_opponent_pool(self):
        """
        ã€ä¿®æ”¹ã€‘æ ¹æ®ç£ç›˜ä¸Šçš„æ–‡ä»¶æ›´æ–°å½“å‰çš„å¯¹æ‰‹æ± å’Œæƒé‡ã€‚
        """
        print("æ­£åœ¨æ›´æ–°å¯¹æ‰‹æ± ...")
        
        # è·å–æ‰€æœ‰ç°æœ‰çš„å¯¹æ‰‹æ¨¡å‹è·¯å¾„
        self.opponent_pool_paths = self._get_opponent_paths()
        
        # è®¾ç½®æƒé‡ï¼šä¸»å®°è€…æƒé‡ä¸º2ï¼Œå…¶ä½™ä¸º1
        if len(self.opponent_pool_paths) == 1:
            # åªæœ‰ä¸»å®°è€…
            self.opponent_pool_weights = [1.0]
            print("å¯¹æ‰‹æ± ä»…åŒ…å«ä¸»å®°è€…æ¨¡å‹ã€‚")
        else:
            # ä¸»å®°è€… + æœ€è¿‘çš„å¯¹æ‰‹
            weights = [2.0] + [1.0] * (len(self.opponent_pool_paths) - 1)
            # å½’ä¸€åŒ–æƒé‡
            total_weight = sum(weights)
            self.opponent_pool_weights = [w / total_weight for w in weights]
            print(f"å¯¹æ‰‹æ± å·²æ›´æ–° (1ä¸ªä¸»å®°è€… + {len(self.opponent_pool_paths)-1}ä¸ªæœ€è¿‘å¯¹æ‰‹ï¼Œæ€»è®¡: {self.opponent_counter}ä¸ªå†å²å¯¹æ‰‹)ã€‚")
            
        for path, weight in zip(self.opponent_pool_paths, self.opponent_pool_weights):
            print(f"  - {os.path.basename(path)} (æƒé‡: {weight:.2f})")

    def _add_new_opponent(self):
        """
        ã€ä¿®æ”¹ã€‘æ·»åŠ æ–°å¯¹æ‰‹åˆ°å¯¹æ‰‹æ± ã€‚
        å½“æŒ‘æˆ˜æˆåŠŸæ—¶ï¼š
        1. æ—§ä¸»å®°è€…ä¿å­˜ä¸ºæ–°çš„å¯¹æ‰‹æ¨¡å‹ï¼ˆé€’å¢ç¼–å·ï¼‰
        2. æŒ‘æˆ˜è€…æˆä¸ºæ–°ä¸»å®°è€…
        3. ä¿ç•™æ‰€æœ‰å†å²å¯¹æ‰‹æ¨¡å‹ï¼Œä½†å¯¹æ‰‹æ± åªé€‰å–æœ€è¿‘çš„3ä¸ª
        """
        print("ğŸ”„ æ­£åœ¨æ·»åŠ æ–°å¯¹æ‰‹...")
        
        # å¢åŠ å¯¹æ‰‹è®¡æ•°å™¨
        self.opponent_counter += 1
        
        # ä¿å­˜æ—§ä¸»å®°è€…ä¸ºæ–°å¯¹æ‰‹
        new_opponent_path = os.path.join(SELF_PLAY_OUTPUT_DIR, f"opponent{self.opponent_counter}.zip")
        if os.path.exists(MAIN_OPPONENT_PATH):
            shutil.copy(MAIN_OPPONENT_PATH, new_opponent_path)
            print(f"æ—§ä¸»å®°è€…å·²ä¿å­˜ä¸º: {os.path.basename(new_opponent_path)}")
        
        # æŒ‘æˆ˜è€…æˆä¸ºæ–°ä¸»å®°è€…
        shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
        print(f"æŒ‘æˆ˜è€…å·²æˆä¸ºæ–°ä¸»å®°è€…: {os.path.basename(MAIN_OPPONENT_PATH)}")
        
        print(f"âœ… å¯¹æ‰‹æ± æ›´æ–°å®Œæˆï¼å½“å‰å…±æœ‰ {self.opponent_counter} ä¸ªå†å²å¯¹æ‰‹æ¨¡å‹")

    def _prepare_environment_and_models(self):
        """
        ã€ä¿®æ”¹ã€‘å‡†å¤‡ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œç¯å¢ƒã€‚
        ç°åœ¨å°†æ•´ä¸ªå¯¹æ‰‹æ± ä¿¡æ¯ä¼ é€’ç»™ç¯å¢ƒã€‚
        """
        print("\n--- [æ­¥éª¤ 2/5] å‡†å¤‡ç¯å¢ƒå’Œæ¨¡å‹ ---")
        
        print(f"åˆ›å»º {N_ENVS} ä¸ªå¹¶è¡Œçš„è®­ç»ƒç¯å¢ƒ...")
        vec_env_cls = SubprocVecEnv if N_ENVS > 1 else DummyVecEnv
        
        self.env = make_vec_env(
            GameEnvironment,
            n_envs=N_ENVS,
            vec_env_cls=vec_env_cls,
            env_kwargs={
                # ã€å…³é”®ç‚¹ã€‘å°†å¯¹æ‰‹æ± å’Œæƒé‡æ³¨å…¥æ¯ä¸ªç¯å¢ƒ
                'opponent_pool': self.opponent_pool_paths,
                'opponent_weights': self.opponent_pool_weights,
            }
        )
        
        print("åŠ è½½å­¦ä¹ è€…æ¨¡å‹...")
        # å­¦ä¹ è€…æ€»æ˜¯ä»å½“å‰æœ€å¼ºçš„ä¸»å®°è€…æ¨¡å‹å¼€å§‹å­¦ä¹ 
        learner_start_path = MAIN_OPPONENT_PATH
        self.model = MaskablePPO.load(
            learner_start_path,
            env=self.env,
            n_steps=512,
            learning_rate=INITIAL_LR,
            tensorboard_log=TENSORBOARD_LOG_PATH
        )
        
        # æ³¨é‡Šæ‰é‡ç½®æ­¥æ•°ï¼Œä¿æŒè®­ç»ƒç»Ÿè®¡ä¿¡æ¯çš„è¿ç»­æ€§
        print("é‡ç½®æ¨¡å‹åˆå§‹è®­ç»ƒæ­¥æ•°...")
        self.model.num_timesteps = 0
        self.model._total_timesteps = 0
        
        print("âœ… ç¯å¢ƒå’Œæ¨¡å‹å‡†å¤‡å®Œæˆï¼")

    def _train_learner(self, loop_number: int):
        """
        è®­ç»ƒå­¦ä¹ è€…æ¨¡å‹ã€‚
        """
        print(f"ğŸ‹ï¸  é˜¶æ®µä¸€: å­¦ä¹ è€…è¿›è¡Œ {STEPS_PER_LOOP:,} æ­¥è®­ç»ƒ...")
        self.model.learn(
            total_timesteps=STEPS_PER_LOOP,
            reset_num_timesteps=False,
            progress_bar=True
        )

    def _evaluate_and_update(self, loop_number: int) -> bool:
        """
        ã€ä¿®æ”¹ã€‘è¯„ä¼°æŒ‘æˆ˜è€…ï¼Œå¦‚æœæˆåŠŸï¼Œåˆ™è½®æ¢å¯¹æ‰‹æ± å¹¶é‡ç½®æ­¥æ•°ã€‚
        """
        print(f"\nğŸ’¾ é˜¶æ®µäºŒ: ä¿å­˜å­¦ä¹ è€…ä¸ºæŒ‘æˆ˜è€…æ¨¡å‹ -> {os.path.basename(CHALLENGER_PATH)}")
        self.model.save(CHALLENGER_PATH)
        time.sleep(0.5)
        
        print(f"\nâš”ï¸  é˜¶æ®µä¸‰: å¯åŠ¨Eloè¯„ä¼°...")
        win_rate = evaluate_models(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
        
        print(f"\nğŸ‘‘ é˜¶æ®µå››: å†³ç­–...")
        if win_rate > EVALUATION_THRESHOLD:
            print(f"ğŸ† æŒ‘æˆ˜æˆåŠŸ (èƒœç‡ {win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
            
            # æ­¥éª¤1: æ·»åŠ æ–°å¯¹æ‰‹å¹¶æ›´æ–°ä¸»å®°è€…
            self._add_new_opponent()
            
            # æ­¥éª¤2: æ›´æ–°å†…éƒ¨çš„å¯¹æ‰‹æ± é…ç½®
            self._update_opponent_pool()
            
            # æ­¥éª¤3: å‘½ä»¤æ‰€æœ‰å¹¶è¡Œç¯å¢ƒé‡æ–°åŠ è½½æ–°çš„å¯¹æ‰‹æ± 
            print(f"ğŸ”¥ å‘é€æŒ‡ä»¤ï¼Œåœ¨æ‰€æœ‰ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒä¸­æ›´æ–°å¯¹æ‰‹æ± ...")
            try:
                self.env.env_method(
                    "reload_opponent_pool",
                    new_pool=self.opponent_pool_paths,
                    new_weights=self.opponent_pool_weights
                )
                print("âœ… æ‰€æœ‰ç¯å¢ƒä¸­çš„å¯¹æ‰‹æ± å‡å·²æ›´æ–°ï¼")
                
                # æ­¥éª¤4: å°†å­¦ä¹ è€…æ¨¡å‹é‡ç½®ä¸ºæ–°ä¸»å®°è€…çš„çŠ¶æ€
                print("ğŸ§  ä¸ºäº†å­¦ä¹ çš„è¿ç»­æ€§ï¼Œå°†å­¦ä¹ è€…æ¨¡å‹é‡ç½®ä¸ºæ–°ä¸»å®°è€…çš„çŠ¶æ€...")
                # ...existing code...
                old_model = self.model
                new_model = MaskablePPO.load(MAIN_OPPONENT_PATH, env=self.env)
                # è¿ç§»æ—¥å¿—ä¸æ­¥æ•°ï¼Œç¡®ä¿è®­ç»ƒä¸å¯è§†åŒ–è¿ç»­
                if hasattr(old_model, "logger") and hasattr(new_model, "set_logger"):
                    new_model.set_logger(old_model.logger)
                new_model.num_timesteps = getattr(old_model, "num_timesteps", 0)
                if hasattr(old_model, "_total_timesteps"):
                    new_model._total_timesteps = old_model._total_timesteps
                self.model = new_model
                # ...existing code...
                
                # ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ­¥éª¤5: æ³¨é‡Šæ‰é‡ç½®è®­ç»ƒæ­¥æ•°ï¼Œä¿æŒTensorBoardæ—¥å¿—è¿ç»­æ€§
                # é‡ç½®æ­¥æ•°ä¼šå¯¼è‡´è®­ç»ƒæŒ‡æ ‡ä¸¢å¤±ï¼Œå½±å“è®­ç»ƒè¿‡ç¨‹çš„ç›‘æ§
                # print("ğŸ”„ ä¸€ä¸ªæ–°æ—¶ä»£å¼€å§‹ï¼Œé‡ç½®è®­ç»ƒæ­¥æ•°...")
                # self.model.num_timesteps = 0
                # self.model._total_timesteps = 0

                return True
            except Exception as e:
                print(f"âš ï¸ å¯¹æ‰‹æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
        else:
            print(f"ğŸ›¡ï¸  æŒ‘æˆ˜å¤±è´¥ (èƒœç‡ {win_rate:.2%} <= {EVALUATION_THRESHOLD:.2%})ã€‚ä¸»å®°è€…ä¸å¯¹æ‰‹æ± ä¿æŒä¸å˜ã€‚")
            return False

    def run(self):
        """
        å¯åŠ¨å¹¶æ‰§è¡Œå®Œæ•´çš„è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒæµç¨‹ã€‚
        """
        try:
            self._prepare_environment_and_models()

            print("\n--- [æ­¥éª¤ 3/5] å¼€å§‹Eloè‡ªæˆ‘å¯¹å¼ˆä¸»å¾ªç¯ ---")
            successful_challenges = 0
            
            for i in range(1, TOTAL_TRAINING_LOOPS + 1):
                print(f"\n{'='*70}")
                print(f"ğŸ”„ è®­ç»ƒå¾ªç¯ {i}/{TOTAL_TRAINING_LOOPS} | æˆåŠŸæŒ‘æˆ˜æ¬¡æ•°: {successful_challenges}")
                print(f"{'='*70}")
                
                try:
                    self._train_learner(i)
                    if self._evaluate_and_update(i):
                        successful_challenges += 1
                    
                except Exception as e:
                    print(f"âš ï¸ è®­ç»ƒå¾ªç¯ {i} å‡ºç°é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            self.model.save(FINAL_MODEL_PATH)
            print(f"\n--- [æ­¥éª¤ 4/5] è®­ç»ƒå®Œæˆï¼ ---")
            print(f"ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {FINAL_MODEL_PATH}")
            print(f"ğŸ“ˆ æ€»è®¡æˆåŠŸæŒ‘æˆ˜: {successful_challenges}/{TOTAL_TRAINING_LOOPS}")
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise
            
        finally:
            if hasattr(self, 'env') and self.env:
                print("\n--- [æ­¥éª¤ 5/5] æ¸…ç†ç¯å¢ƒ ---")
                self.env.close()
                print("âœ… èµ„æºæ¸…ç†å®Œæˆ")