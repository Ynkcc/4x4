# training/trainer.py

import os
import shutil
import time
import re
import json
import numpy as np
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from sb3_contrib import MaskablePPO

# ã€æ›´æ–°ã€‘å¯¼å…¥æ‰€æœ‰éœ€è¦çš„å¸¸é‡
from utils.constants import *
from game.environment import GameEnvironment
from game.policy import CustomActorCriticPolicy  # å¯¼å…¥è‡ªå®šä¹‰ç­–ç•¥
from training.evaluator import evaluate_models # ä½¿ç”¨é•œåƒè¯„ä¼°å™¨

def create_new_ppo_model(env=None, tensorboard_log=None):
    """
    åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„éšæœºåˆå§‹åŒ–çš„PPOæ¨¡å‹ã€‚
    """
    model = MaskablePPO(
        policy=CustomActorCriticPolicy,
        env=env,
        learning_rate=INITIAL_LR,
        clip_range=PPO_CLIP_RANGE,
        n_steps=PPO_N_STEPS,
        batch_size=PPO_BATCH_SIZE,
        n_epochs=PPO_N_EPOCHS,
        gae_lambda=PPO_GAE_LAMBDA,
        vf_coef=PPO_VF_COEF,
        ent_coef=PPO_ENT_COEF,
        max_grad_norm=PPO_MAX_GRAD_NORM,
        tensorboard_log=tensorboard_log,
        device=PPO_DEVICE,
        verbose=PPO_VERBOSE,
        policy_kwargs={
            'features_extractor_kwargs': {
                'features_dim': NETWORK_FEATURES_DIM,
                'num_res_blocks': NETWORK_NUM_RES_BLOCKS,
                'num_hidden_channels': NETWORK_NUM_HIDDEN_CHANNELS
            }
        }
    )
    
    return model

def load_ppo_model_with_hyperparams(model_path: str, env=None, tensorboard_log=None):
    """
    åŠ è½½PPOæ¨¡å‹å¹¶åº”ç”¨è‡ªå®šä¹‰è¶…å‚æ•°ã€‚
    """
    model = MaskablePPO.load(
        model_path,
        env=env,
        learning_rate=INITIAL_LR,
        clip_range=PPO_CLIP_RANGE,
        tensorboard_log=tensorboard_log,
        n_steps=PPO_N_STEPS,
        device=PPO_DEVICE
    )
    
    # åº”ç”¨å…¶ä»–è‡ªå®šä¹‰PPOè¶…å‚æ•°
    model.batch_size = PPO_BATCH_SIZE
    model.n_epochs = PPO_N_EPOCHS
    model.gae_lambda = PPO_GAE_LAMBDA
    model.vf_coef = PPO_VF_COEF
    model.ent_coef = PPO_ENT_COEF
    model.max_grad_norm = PPO_MAX_GRAD_NORM
    
    return model

class SelfPlayTrainer:
    """
    ã€V5 æœ€ç»ˆç‰ˆã€‘é›†æˆäº†åŠ¨æ€Eloè¯„ä¼°ã€å¯¹æ‰‹æ± è½®æ¢å’Œå¤šç¯å¢ƒå®æ—¶åŒæ­¥çš„è®­ç»ƒå™¨ã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œè®¾ç½®æ¨¡å‹å’Œç¯å¢ƒä¸ºNoneã€‚"""
        self.model = None
        self.env = None
        # ã€æ—¥å¿—ä¿®å¤ã€‘ä¸ºå½“å‰è®­ç»ƒè¿è¡Œå­˜å‚¨å”¯ä¸€çš„TensorBoardè·¯å¾„
        self.tensorboard_log_run_path = None

        # --- å¯¹æ‰‹æ± æ ¸å¿ƒå±æ€§ ---
        self.opponent_pool_paths = []
        self.opponent_pool_weights = []
        self.opponent_pool_paths_for_env = []

        # Eloè¯„åˆ†ç³»ç»Ÿ
        self.elo_ratings = {}
        # ã€æ›´æ–°ã€‘ä½¿ç”¨å¸¸é‡åˆå§‹åŒ–Eloå‚æ•°
        self.default_elo = ELO_DEFAULT
        self.elo_k_factor = ELO_K_FACTOR
        
        self._setup()

    def _setup(self):
        """
        æ‰§è¡Œæ‰€æœ‰å¯åŠ¨å‰çš„å‡†å¤‡å·¥ä½œã€‚
        """
        print("--- [æ­¥éª¤ 1/5] åˆå§‹åŒ–è®¾ç½® ---")
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        os.makedirs(TENSORBOARD_LOG_PATH, exist_ok=True)

        if not os.path.exists(MAIN_OPPONENT_PATH):
            print("æœªæ‰¾åˆ°ä¸»å®°è€…æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„åˆå§‹æ¨¡å‹...")
            initial_model_candidates = [SELF_PLAY_MODEL_PATH, CURRICULUM_MODEL_PATH]
            initial_model_found = None
            for candidate in initial_model_candidates:
                if os.path.exists(candidate):
                    initial_model_found = candidate
                    print(f"æ‰¾åˆ°åˆå§‹æ¨¡å‹: {candidate}")
                    break
            
            if initial_model_found:
                # å¦‚æœæ‰¾åˆ°äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¤åˆ¶å®ƒ
                shutil.copy(initial_model_found, MAIN_OPPONENT_PATH)
                print(f"å·²å°†åˆå§‹æ¨¡å‹å¤åˆ¶ä¸ºç¬¬ä¸€ä¸ªä¸»å®°è€…: {MAIN_OPPONENT_PATH}")
                
                # å°†åˆå§‹ä¸»å®°è€…ä¹ŸåŠ å…¥å¯¹æ‰‹æ± ï¼Œä½œä¸ºç¬¬ä¸€ä¸ªå¯¹æ‰‹
                initial_opponent_path = os.path.join(OPPONENT_POOL_DIR, "opponent_0.zip")
                if not os.path.exists(initial_opponent_path):
                     shutil.copy(initial_model_found, initial_opponent_path)
                     # åˆå§‹åŒ–Eloå¹¶ç«‹å³ä¿å­˜
                     self.elo_ratings['opponent_0.zip'] = self.default_elo
                     self._save_elo_ratings()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„éšæœºåˆå§‹åŒ–æ¨¡å‹
                print("æœªæ‰¾åˆ°ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†åˆ›å»ºå…¨æ–°çš„éšæœºåˆå§‹åŒ–æ¨¡å‹...")
                self._create_initial_model()

        self._load_opponent_pool_and_elo()

    def _create_initial_model(self):
        """
        åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„éšæœºåˆå§‹åŒ–æ¨¡å‹ä½œä¸ºèµ·å§‹ç‚¹ã€‚
        """
        print("æ­£åœ¨åˆ›å»ºä¸´æ—¶ç¯å¢ƒä»¥åˆå§‹åŒ–æ¨¡å‹...")
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç¯å¢ƒæ¥åˆå§‹åŒ–æ¨¡å‹
        temp_env = GameEnvironment()
        
        print("æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        new_model = create_new_ppo_model(env=temp_env)
        
        # ä¿å­˜æ–°åˆ›å»ºçš„æ¨¡å‹
        new_model.save(MAIN_OPPONENT_PATH)
        print(f"âœ… æ–°æ¨¡å‹å·²ä¿å­˜åˆ°: {MAIN_OPPONENT_PATH}")
        
        # å°†åˆå§‹æ¨¡å‹ä¹ŸåŠ å…¥å¯¹æ‰‹æ± ï¼Œä½œä¸ºç¬¬ä¸€ä¸ªå¯¹æ‰‹
        initial_opponent_path = os.path.join(OPPONENT_POOL_DIR, "opponent_0.zip")
        shutil.copy(MAIN_OPPONENT_PATH, initial_opponent_path)
        print(f"âœ… åˆå§‹æ¨¡å‹å·²å¤åˆ¶åˆ°å¯¹æ‰‹æ± : {initial_opponent_path}")
        
        # åˆå§‹åŒ–Eloè¯„åˆ†
        self.elo_ratings['opponent_0.zip'] = self.default_elo
        self.elo_ratings['main_opponent.zip'] = self.default_elo
        self._save_elo_ratings()
        print("âœ… Eloè¯„åˆ†å·²åˆå§‹åŒ–")
        
        # æ¸…ç†ä¸´æ—¶ç¯å¢ƒ
        temp_env.close()
        print("âœ… ä¸´æ—¶ç¯å¢ƒå·²æ¸…ç†")

    def _load_opponent_pool_and_elo(self):
        """
        ä»ç£ç›˜åŠ è½½æ‰€æœ‰å¯¹æ‰‹æ¨¡å‹ï¼Œå¹¶åŠ è½½Eloè¯„åˆ†ã€‚
        """
        print("æ­£åœ¨ä»ç£ç›˜åŠ è½½å¯¹æ‰‹æ± å’ŒEloè¯„åˆ†...")
        self.opponent_pool_paths = []
        
        elo_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        if os.path.exists(elo_file):
            try:
                with open(elo_file, 'r') as f:
                    self.elo_ratings = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"è­¦å‘Šï¼šè¯»å–Eloæ–‡ä»¶å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
                self.elo_ratings = {}
        
        opponent_files = [f for f in os.listdir(OPPONENT_POOL_DIR) if f.endswith('.zip')]
        opponent_files.sort(key=lambda x: int(re.search(r'opponent_(\d+)\.zip', x).group(1)))

        for filename in opponent_files:
            full_path = os.path.join(OPPONENT_POOL_DIR, filename)
            # ã€ä¿®å¤Bug 2ã€‘æ–°å¢æ£€æŸ¥ï¼Œç¡®ä¿Eloæ–‡ä»¶ä¸­çš„å¯¹æ‰‹æ¨¡å‹ç‰©ç†å­˜åœ¨
            if not os.path.exists(full_path):
                print(f"è­¦å‘Š: Eloä¸­å­˜åœ¨ä½†æ–‡ä»¶ç¼ºå¤±: {filename}ã€‚è·³è¿‡ã€‚")
                continue
            self.opponent_pool_paths.append(full_path)
            if filename not in self.elo_ratings:
                self.elo_ratings[filename] = self.default_elo
        
        print(f"æˆåŠŸåŠ è½½ {len(self.opponent_pool_paths)} ä¸ªå¯¹æ‰‹ã€‚")
        self._update_opponent_weights()
    
    def _save_elo_ratings(self):
        """å°†Eloè¯„åˆ†ä¿å­˜åˆ°JSONæ–‡ä»¶ã€‚"""
        elo_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        try:
            with open(elo_file, 'w') as f:
                json.dump(self.elo_ratings, f, indent=4)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜Eloè¯„åˆ†æ–‡ä»¶: {e}")

    def _update_elo(self, player_a_name, player_b_name, player_a_score):
        """
        æ ¹æ®é•œåƒå¯¹å±€çš„å®é™…å¾—åˆ†æ›´æ–°åŒæ–¹çš„Eloè¯„åˆ†ã€‚
        player_a_score æ˜¯ç©å®¶Açš„å¾—åˆ†ï¼ŒèŒƒå›´åœ¨0.0åˆ°1.0ä¹‹é—´ã€‚
        """
        player_a_elo = self.elo_ratings.get(player_a_name, self.default_elo)
        player_b_elo = self.elo_ratings.get(player_b_name, self.default_elo)

        # è®¡ç®—æœŸæœ›å¾—åˆ†
        expected_score_a = 1 / (1 + 10 ** ((player_b_elo - player_a_elo) / 400))
        
        # å®é™…å¾—åˆ†
        player_b_score = 1.0 - player_a_score
        
        # æ›´æ–°Eloè¯„åˆ†
        new_player_a_elo = player_a_elo + self.elo_k_factor * (player_a_score - expected_score_a)
        # Bçš„æœŸæœ›å¾—åˆ†æ˜¯ 1 - Açš„æœŸæœ›å¾—åˆ†
        expected_score_b = 1.0 - expected_score_a
        new_player_b_elo = player_b_elo + self.elo_k_factor * (player_b_score - expected_score_b)
        
        self.elo_ratings[player_a_name] = new_player_a_elo
        self.elo_ratings[player_b_name] = new_player_b_elo
        
        print(f"Elo æ›´æ–° ({player_a_name} vs {player_b_name}, åŸºäºå¾—åˆ† {player_a_score:.2%}):")
        print(f"  - {player_a_name}: {player_a_elo:.0f} -> {new_player_a_elo:.0f} (Î” {new_player_a_elo - player_a_elo:+.1f})")
        print(f"  - {player_b_name}: {player_b_elo:.0f} -> {new_player_b_elo:.0f} (Î” {new_player_b_elo - player_b_elo:+.1f})")

    def _update_opponent_weights(self):
        """
        æ ¹æ®Eloè¯„åˆ†è®¡ç®—é‡‡æ ·æƒé‡ã€‚
        """
        main_opponent_name = "main_opponent.zip"
        if main_opponent_name not in self.elo_ratings:
            self.elo_ratings[main_opponent_name] = self.default_elo
        main_elo = self.elo_ratings[main_opponent_name]
        
        weights = []
        if not self.opponent_pool_paths:
            self.opponent_pool_paths_for_env = [MAIN_OPPONENT_PATH]
            self.opponent_pool_weights = [1.0]
            return

        # è®¡ç®—æ± ä¸­æ¯ä¸ªå¯¹æ‰‹çš„æƒé‡
        for path in self.opponent_pool_paths:
            opp_name = os.path.basename(path)
            opp_elo = self.elo_ratings.get(opp_name, self.default_elo)
            elo_diff = abs(main_elo - opp_elo)
            # ã€æ›´æ–°ã€‘ä½¿ç”¨å¸¸é‡è®¾ç½®æ¸©åº¦å‚æ•°
            weight = np.exp(-elo_diff / ELO_WEIGHT_TEMPERATURE)
            weights.append(weight)
        
        # ã€ä¿®å¤Bug 3ã€‘å°†ä¸»å®°è€…æƒé‡ä»æ± æ€»å’Œçš„50%é™è‡³30%ï¼Œä»¥å¢åŠ å¤šæ ·æ€§
        main_opponent_weight = sum(weights) * 0.3 if weights else 1.0

        self.opponent_pool_paths_for_env = self.opponent_pool_paths + [MAIN_OPPONENT_PATH]
        all_weights = weights + [main_opponent_weight]

        total_weight = sum(all_weights)
        if total_weight == 0:
            num_opps = len(self.opponent_pool_paths_for_env)
            self.opponent_pool_weights = [1.0 / num_opps] * num_opps if num_opps > 0 else []
        else:
            self.opponent_pool_weights = [w / total_weight for w in all_weights]

        print("å¯¹æ‰‹æ± é‡‡æ ·æƒé‡å·²æ›´æ–°:")
        for path, weight in zip(self.opponent_pool_paths_for_env, self.opponent_pool_weights):
            elo = self.elo_ratings.get(os.path.basename(path), self.default_elo)
            print(f"  - {os.path.basename(path)} (Elo: {elo:.0f}, æƒé‡: {weight:.2%})")

    def _add_new_opponent(self, challenger_elo):
        """
        æŒ‘æˆ˜æˆåŠŸåï¼Œæ‰§è¡Œâ€œä¸»å®°è€…é™çº§å…¥æ±  -> æŒ‘æˆ˜è€…æ™‹å‡ä¸ºä¸»å®°è€… -> æ± å¤§å°ç®¡ç†â€çš„å®Œæ•´æµç¨‹ã€‚
        """
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œå¯¹æ‰‹æ± è½®æ¢...")

        # 1. ç¡®å®šæ–°å¯¹æ‰‹çš„æ–‡ä»¶å
        opponent_files = [f for f in os.listdir(OPPONENT_POOL_DIR) if f.endswith('.zip')]
        max_num = -1
        for f in opponent_files:
            match = re.search(r'opponent_(\d+)\.zip', f)
            if match:
                max_num = max(max_num, int(match.group(1)))
        new_opponent_num = max_num + 1
        new_opponent_name = f"opponent_{new_opponent_num}.zip"
        new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)

        # 2. æ—§ä¸»å®°è€…è¿›å…¥å¯¹æ‰‹æ± 
        old_main_name = "main_opponent.zip"
        if os.path.exists(MAIN_OPPONENT_PATH):
            shutil.copy(MAIN_OPPONENT_PATH, new_opponent_path)
            self.elo_ratings[new_opponent_name] = self.elo_ratings.get(old_main_name, self.default_elo)
            self.opponent_pool_paths.append(new_opponent_path)
            print(f"æ—§ä¸»å®°è€…å·²å­˜å…¥å¯¹æ‰‹æ± : {new_opponent_name} (Elo: {self.elo_ratings[new_opponent_name]:.0f})")

        # 3. æŒ‘æˆ˜è€…æˆä¸ºæ–°ä¸»å®°è€…
        shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
        self.elo_ratings[old_main_name] = challenger_elo
        print(f"æŒ‘æˆ˜è€…å·²æˆä¸ºæ–°ä¸»å®°è€… (Elo: {self.elo_ratings[old_main_name]:.0f})")

        # 4. ç®¡ç†å¯¹æ‰‹æ± å¤§å°
        if len(self.opponent_pool_paths) > MAX_OPPONENT_POOL_SIZE:
            pool_with_elo = [(p, self.elo_ratings.get(os.path.basename(p), self.default_elo)) for p in self.opponent_pool_paths]
            pool_with_elo.sort(key=lambda x: x[1])
            
            removed_opponent_path, removed_elo = pool_with_elo[0]
            removed_opponent_name = os.path.basename(removed_opponent_path)
            
            self.opponent_pool_paths.remove(removed_opponent_path)
            os.remove(removed_opponent_path)
            if removed_opponent_name in self.elo_ratings:
                del self.elo_ratings[removed_opponent_name]
            
            print(f"å¯¹æ‰‹æ± å·²æ»¡ï¼Œç§»é™¤Eloæœ€ä½çš„å¯¹æ‰‹: {removed_opponent_name} (Elo: {removed_elo:.0f})")
            
        self._save_elo_ratings()

    def _prepare_environment_and_models(self):
        """
        å‡†å¤‡ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œç¯å¢ƒã€‚
        """
        print("\n--- [æ­¥éª¤ 2/5] å‡†å¤‡ç¯å¢ƒå’Œæ¨¡å‹ ---")
        
        # ã€æ—¥å¿—ä¿®å¤ã€‘ä¸ºæœ¬æ¬¡è®­ç»ƒè¿è¡Œåˆ›å»ºå”¯ä¸€çš„TensorBoardæ—¥å¿—è·¯å¾„
        run_name = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
        self.tensorboard_log_run_path = os.path.join(TENSORBOARD_LOG_PATH, run_name)
        print(f"TensorBoard æ—¥å¿—å°†ä¿å­˜åˆ°: {self.tensorboard_log_run_path}")

        print(f"åˆ›å»º {N_ENVS} ä¸ªå¹¶è¡Œçš„è®­ç»ƒç¯å¢ƒ...")
        vec_env_cls = SubprocVecEnv if N_ENVS > 1 else DummyVecEnv
        
        self.env = make_vec_env(
            GameEnvironment,
            n_envs=N_ENVS,
            vec_env_cls=vec_env_cls,
            env_kwargs={
                'opponent_pool': self.opponent_pool_paths_for_env,
                'opponent_weights': self.opponent_pool_weights,
            }
        )
        
        print("åŠ è½½å­¦ä¹ è€…æ¨¡å‹...")
        self.model = load_ppo_model_with_hyperparams(
            MAIN_OPPONENT_PATH,
            env=self.env,
            tensorboard_log=self.tensorboard_log_run_path
        )
        
        print("âœ… ç¯å¢ƒå’Œæ¨¡å‹å‡†å¤‡å®Œæˆï¼")

    def _train_learner(self, loop_number: int):
        """è®­ç»ƒå­¦ä¹ è€…æ¨¡å‹ã€‚"""
        print(f"ğŸ‹ï¸  é˜¶æ®µä¸€: å­¦ä¹ è€…è¿›è¡Œ {STEPS_PER_LOOP:,} æ­¥è®­ç»ƒ...")
        
        start_time = time.time()
        
        self.model.learn(
            total_timesteps=STEPS_PER_LOOP,
            reset_num_timesteps=False,
            progress_bar=PPO_SHOW_PROGRESS 
        )
        
        elapsed_time = time.time() - start_time
        print(f"âœ… è®­ç»ƒå®Œæˆ! ç”¨æ—¶: {elapsed_time:.1f}ç§’, æ€»æ­¥æ•°: {self.model.num_timesteps:,}")

    def _evaluate_and_update(self, loop_number: int) -> bool:
        """
        ã€å·²é‡æ„ã€‘è¯„ä¼°ã€å†³ç­–ã€æ›´æ–°Eloã€è½®æ¢å¯¹æ‰‹ã€åŒæ­¥ç¯å¢ƒçš„å®Œæ•´æµç¨‹ã€‚
        """
        print(f"\nğŸ’¾ é˜¶æ®µäºŒ: ä¿å­˜å­¦ä¹ è€…ä¸ºæŒ‘æˆ˜è€…æ¨¡å‹ -> {os.path.basename(CHALLENGER_PATH)}")
        self.model.save(CHALLENGER_PATH)
        time.sleep(0.5)
        
        print(f"\nâš”ï¸  é˜¶æ®µä¸‰: å¯åŠ¨é•œåƒå¯¹å±€è¯„ä¼°...")
        win_rate = evaluate_models(CHALLENGER_PATH, MAIN_OPPONENT_PATH, show_progress=True)
        
        print(f"\nğŸ‘‘ é˜¶æ®µå››: å†³ç­–...")
        challenger_name = os.path.basename(CHALLENGER_PATH)
        main_opponent_name = os.path.basename(MAIN_OPPONENT_PATH)
        
        # å¦‚æœæŒ‘æˆ˜è€…æ˜¯ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œç»™å®ƒä¸€ä¸ªåŸºäºä¸»å®°è€…çš„åˆå§‹Elo
        if challenger_name not in self.elo_ratings:
            main_elo = self.elo_ratings.get(main_opponent_name, self.default_elo)
            self.elo_ratings[challenger_name] = main_elo

        # ç›´æ¥æ›´æ–°åŒæ–¹çš„Eloè¯„åˆ†
        self._update_elo(challenger_name, main_opponent_name, win_rate)
        
        challenger_elo = self.elo_ratings[challenger_name]

        if win_rate > EVALUATION_THRESHOLD:
            print(f"ğŸ† æŒ‘æˆ˜æˆåŠŸ (èƒœç‡ {win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
            
            # æŒ‘æˆ˜è€…æ™‹å‡ï¼Œå…¶Eloåˆ†æ•°èµ‹ç»™æ–°çš„ä¸»å®°è€…
            self._add_new_opponent(challenger_elo) 
            self._update_opponent_weights()
            
            print(f"ğŸ”¥ å‘é€æŒ‡ä»¤ï¼Œåœ¨æ‰€æœ‰ {N_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒä¸­æ›´æ–°å¯¹æ‰‹æ± ...")
            try:
                results = self.env.env_method(
                    "reload_opponent_pool",
                    new_pool=self.opponent_pool_paths_for_env,
                    new_weights=self.opponent_pool_weights
                )
                if all(results):
                    print("âœ… æ‰€æœ‰ç¯å¢ƒä¸­çš„å¯¹æ‰‹æ± å‡å·²æˆåŠŸæ›´æ–°ï¼")
                else:
                    print("âš ï¸ éƒ¨åˆ†ç¯å¢ƒæœªèƒ½æˆåŠŸæ›´æ–°å¯¹æ‰‹æ± ã€‚")

                print("ğŸ§  æŒ‘æˆ˜è€…å·²æˆä¸ºæ–°ä¸»å®°è€…ï¼Œè®­ç»ƒå™¨å°†ç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€...")
                return True

            except Exception as e:
                raise RuntimeError(f"åœ¨æ›´æ–°å¹¶è¡Œç¯å¢ƒä¸­çš„å¯¹æ‰‹æ± æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

        else:
            print(f"ğŸ›¡ï¸  æŒ‘æˆ˜å¤±è´¥ (èƒœç‡ {win_rate:.2%} <= {EVALUATION_THRESHOLD:.2%})ã€‚")
            
            # å…³é”®é€»è¾‘ï¼šå³ä½¿æŒ‘æˆ˜å¤±è´¥ï¼Œä¸»å®°è€…ä¹Ÿæ›´æ–°ä¸ºåˆšåˆšè®­ç»ƒè¿‡çš„ã€æ›´å¼ºçš„ç‰ˆæœ¬
            print("... ä¸»å®°è€…æ¨¡å‹å°†æ›´æ–°ä¸ºåˆšåˆšè®­ç»ƒè¿‡çš„ã€æ›´å¼ºçš„ç‰ˆæœ¬ï¼ˆå³æŒ‘æˆ˜è€…ï¼‰ã€‚")
            shutil.copy(CHALLENGER_PATH, MAIN_OPPONENT_PATH)
            
            # åŒæ—¶ï¼Œå°†æŒ‘æˆ˜è€…çš„Eloåˆ†æ•°èµ‹ç»™ä¸»å®°è€…
            self.elo_ratings[main_opponent_name] = self.elo_ratings[challenger_name]
            
            # ä¿å­˜æ›´æ–°åçš„Elo
            self._save_elo_ratings()
            
            # ä»å†…å­˜ä¸­ç§»é™¤ä¸´æ—¶çš„æŒ‘æˆ˜è€…Eloè®°å½•
            if challenger_name in self.elo_ratings:
                del self.elo_ratings[challenger_name]

            # åŠ è½½æ›´æ–°åçš„ä¸»å®°è€…æ¨¡å‹ï¼Œç»§ç»­ä¸‹ä¸€è½®è®­ç»ƒ
            # è¿™ä¸€æ­¥ç¡®ä¿äº†è®­ç»ƒçš„è¿ç»­æ€§
            self.model = load_ppo_model_with_hyperparams(
                MAIN_OPPONENT_PATH,
                env=self.env,
                tensorboard_log=self.tensorboard_log_run_path
            )

            return False

    def run(self):
        """
        å¯åŠ¨å¹¶æ‰§è¡Œå®Œæ•´çš„è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒæµç¨‹ã€‚
        """
        try:
            self._prepare_environment_and_models()
            print("\n--- [æ­¥éª¤ 3/5] å¼€å§‹Eloè‡ªæˆ‘å¯¹å¼ˆä¸»å¾ªç¯ ---")
            successful_challenges = 0
            
            for i in range(1, TOTAL_TRAINING_LOOPS + 1):
                print(f"\n{'='*70}\nğŸ”„ è®­ç»ƒå¾ªç¯ {i}/{TOTAL_TRAINING_LOOPS} | æˆåŠŸæŒ‘æˆ˜æ¬¡æ•°: {successful_challenges}\n{'='*70}")
                try:
                    self._train_learner(i)
                    if self._evaluate_and_update(i):
                        successful_challenges += 1
                except Exception as e:
                    print(f"âš ï¸ è®­ç»ƒå¾ªç¯ {i} å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                    print("...ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯...")
                    continue
            
            self.model.save(FINAL_MODEL_PATH)
            print(f"\n--- [æ­¥éª¤ 4/5] è®­ç»ƒå®Œæˆï¼ ---")
            print(f"ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {FINAL_MODEL_PATH}")
            print(f"ğŸ“ˆ æ€»è®¡æˆåŠŸæŒ‘æˆ˜: {successful_challenges}/{TOTAL_TRAINING_LOOPS}")
            
        finally:
            print("\næ­£åœ¨ä¿å­˜æœ€ç»ˆçš„Eloè¯„åˆ†...")
            self._save_elo_ratings()
            if hasattr(self, 'env') and self.env:
                print("\n--- [æ­¥éª¤ 5/5] æ¸…ç†ç¯å¢ƒ ---")
                self.env.close()
                print("âœ… èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == '__main__':
    trainer = SelfPlayTrainer()
    trainer.run()