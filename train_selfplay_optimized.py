# train_selfplay_optimized.py - ä¼˜åŒ–çš„è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒè„šæœ¬
import os
import shutil
from stable_baselines3.common.env_util import make_vec_env
from sb3_contrib import MaskablePPO
from Game import GameEnvironment
from custom_policy import CustomActorCriticPolicy
from opponent_model_manager import shared_opponent_manager

def main():
    # è®¾ç½®å’Œåˆå§‹åŒ–
    CURRICULUM_MODEL_PATH = "cnn_curriculum_models/final_model_cnn.zip"
    SELF_PLAY_MODEL_DIR = "self_play_models_optimized"
    LEARNER_MODEL_PATH = os.path.join(SELF_PLAY_MODEL_DIR, "learner.zip")
    OPPONENT_MODEL_PATH = os.path.join(SELF_PLAY_MODEL_DIR, "opponent.zip")
    
    TOTAL_TRAINING_LOOPS = 100  # æ€»å…±æ›´æ–°å¤šå°‘æ¬¡å¯¹æ‰‹
    STEPS_PER_LOOP = 50000      # æ¯æ¬¡æ›´æ–°å¯¹æ‰‹ä¹‹é—´ï¼Œå­¦ä¹ è€…è®­ç»ƒå¤šå°‘æ­¥
    
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒ...")
    print(f"ğŸ“Š æ€»è®­ç»ƒå¾ªç¯æ•°: {TOTAL_TRAINING_LOOPS}")
    print(f"ğŸ”„ æ¯å¾ªç¯è®­ç»ƒæ­¥æ•°: {STEPS_PER_LOOP}")
    print("âš¡ ä½¿ç”¨å…±äº«å¯¹æ‰‹æ¨¡å‹ç®¡ç†å™¨ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½")
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    os.makedirs(SELF_PLAY_MODEL_DIR, exist_ok=True)
    
    # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ æœ€ç»ˆæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CURRICULUM_MODEL_PATH):
        raise FileNotFoundError(f"è¯¾ç¨‹å­¦ä¹ æœ€ç»ˆæ¨¡å‹ä¸å­˜åœ¨: {CURRICULUM_MODEL_PATH}")
    
    # å…³é”®çš„åˆå§‹æ­¥éª¤ï¼šä½¿ç”¨è¯¾ç¨‹å­¦ä¹ çš„æœ€ç»ˆæ¨¡å‹ä½œä¸ºç¬¬ä¸€ä¸ªå¯¹æ‰‹
    print(f"ğŸ“¦ å¤åˆ¶è¯¾ç¨‹å­¦ä¹ æœ€ç»ˆæ¨¡å‹ {CURRICULUM_MODEL_PATH} ä½œä¸ºåˆå§‹å¯¹æ‰‹...")
    shutil.copy(CURRICULUM_MODEL_PATH, OPPONENT_MODEL_PATH)
    
    # é¢„åŠ è½½å¯¹æ‰‹æ¨¡å‹åˆ°å…±äº«ç®¡ç†å™¨ï¼ˆé¿å…æ¯ä¸ªç¯å¢ƒé‡å¤åŠ è½½ï¼‰
    print("ğŸ”§ é¢„åŠ è½½å¯¹æ‰‹æ¨¡å‹åˆ°å…±äº«ç®¡ç†å™¨...")
    shared_opponent_manager.load_model(OPPONENT_MODEL_PATH)
    model_info = shared_opponent_manager.get_model_info()
    print(f"ğŸ“‹ å¯¹æ‰‹æ¨¡å‹ä¿¡æ¯: {model_info}")
    
    # åˆ›å»ºç¯å¢ƒ - ç°åœ¨ä½¿ç”¨å…±äº«æ¨¡å‹ç®¡ç†å™¨
    print("ğŸŒ åˆ›å»ºä¼˜åŒ–çš„è‡ªæˆ‘å¯¹å¼ˆç¯å¢ƒ...")
    env = make_vec_env(
        GameEnvironment,
        n_envs=8,
        env_kwargs={
            'curriculum_stage': 4,  # å§‹ç»ˆæ˜¯å®Œæ•´æ¸¸æˆ
            'opponent_policy': OPPONENT_MODEL_PATH
        }
    )
    
    # åŠ è½½å­¦ä¹ è€…æ¨¡å‹ - ä»è¯¾ç¨‹å­¦ä¹ çš„æœ€ç»ˆæ¨¡å‹å¼€å§‹
    print(f"ğŸ¤– åŠ è½½å­¦ä¹ è€…æ¨¡å‹ä» {CURRICULUM_MODEL_PATH}...")
    model = MaskablePPO.load(
        CURRICULUM_MODEL_PATH,
        env=env,
        learning_rate=3e-4,
        tensorboard_log="./self_play_tensorboard_logs_optimized/"
    )
    
    print("ğŸ¯ å¼€å§‹ä¼˜åŒ–ç‰ˆè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå¾ªç¯...")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    for i in range(1, TOTAL_TRAINING_LOOPS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ è®­ç»ƒå¾ªç¯ {i}/{TOTAL_TRAINING_LOOPS}")
        print(f"{'='*60}")
        
        # (a) è®­ç»ƒå­¦ä¹ è€…
        print(f"ğŸ‹ï¸  è®­ç»ƒå­¦ä¹ è€… {STEPS_PER_LOOP:,} æ­¥...")
        model.learn(
            total_timesteps=STEPS_PER_LOOP,
            reset_num_timesteps=False,  # ä¿æŒè¿ç»­çš„æ—¶é—´æ­¥è®¡æ•°
            progress_bar=True
        )
        
        # (b) ä¿å­˜å­¦ä¹ è€…
        print("ğŸ’¾ ä¿å­˜å­¦ä¹ è€…æ¨¡å‹...")
        model.save(LEARNER_MODEL_PATH)
        
        # (c) æ›´æ–°å¯¹æ‰‹ - ç”¨å½“å‰å­¦ä¹ è€…è¦†ç›–å¯¹æ‰‹
        print("ğŸ”„ æ›´æ–°å¯¹æ‰‹æ¨¡å‹...")
        shutil.copy(LEARNER_MODEL_PATH, OPPONENT_MODEL_PATH)
        
        # (d) ã€å…³é”®ä¼˜åŒ–ã€‘æ›´æ–°å…±äº«æ¨¡å‹ç®¡ç†å™¨ä¸­çš„å¯¹æ‰‹æ¨¡å‹
        print("âš¡ æ›´æ–°å…±äº«æ¨¡å‹ç®¡ç†å™¨...")
        shared_opponent_manager.load_model(OPPONENT_MODEL_PATH)
        
        # (e) é‡å»ºç¯å¢ƒ - ç°åœ¨æ›´é«˜æ•ˆï¼Œå› ä¸ºæ¨¡å‹å·²ç»åœ¨å…±äº«ç®¡ç†å™¨ä¸­
        print("ğŸŒ é‡å»ºç¯å¢ƒ...")
        env.close()
        env = make_vec_env(
            GameEnvironment,
            n_envs=8,
            env_kwargs={
                'curriculum_stage': 4,
                'opponent_policy': OPPONENT_MODEL_PATH
            }
        )
        model.set_env(env)
        
        print(f"âœ… å¾ªç¯ {i} å®Œæˆï¼")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if i % 10 == 0:
            checkpoint_path = os.path.join(SELF_PLAY_MODEL_DIR, f"checkpoint_loop_{i}.zip")
            model.save(checkpoint_path)
            print(f"ğŸ“‹ ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {checkpoint_path}")
    
    # æœ€ç»ˆæ”¶å°¾
    final_model_path = os.path.join(SELF_PLAY_MODEL_DIR, "final_selfplay_model_optimized.zip")
    model.save(final_model_path)
    print(f"\nğŸ‰ ä¼˜åŒ–ç‰ˆè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“¦ æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")
    
    # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæœæ€»ç»“
    model_info = shared_opponent_manager.get_model_info()
    print(f"\nğŸ“Š ä¼˜åŒ–æ€»ç»“:")
    print(f"   âœ… å…±äº«æ¨¡å‹ç®¡ç†å™¨çŠ¶æ€: {model_info}")
    print(f"   âš¡ é¿å…äº† {8 * TOTAL_TRAINING_LOOPS} æ¬¡é‡å¤æ¨¡å‹åŠ è½½")
    print(f"   ğŸ’¾ èŠ‚çœäº†å¤§é‡å†…å­˜å’ŒåŠ è½½æ—¶é—´")
    
    env.close()

if __name__ == "__main__":
    main()
