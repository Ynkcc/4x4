# rl_code/rllib_version/scripts/evaluate.py

import os
import sys
import time
import numpy as np
from tqdm import tqdm
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥RLlibå’Œé¡¹ç›®æ¨¡å—
import ray
from ray.rllib.policy.policy import Policy
from ray.rllib.models import ModelCatalog
from ray.tune.registry import register_env

from core.environment import GameEnvironment
from core.policy import RLLibCustomNetwork
from utils.constants import *

# æ³¨å†Œè‡ªå®šä¹‰ç¯å¢ƒå’Œæ¨¡å‹
register_env("dark_chess_env", lambda config: GameEnvironment())
ModelCatalog.register_custom_model("custom_torch_model", RLLibCustomNetwork)


class EvaluationAgent:
    """ç”¨äºè¯„ä¼°çš„AIä»£ç†åŒ…è£…å™¨"""
    def __init__(self, policy: Policy, name: str):
        self.policy = policy
        self.name = name

    def predict(self, observation: dict, deterministic: bool = True) -> int:
        """ä½¿ç”¨ç­–ç•¥è¿›è¡Œé¢„æµ‹"""
        # RLlib ç­–ç•¥éœ€è¦ä¸€ä¸ª observation_space å­—å…¸
        # æˆ‘ä»¬çš„ç¯å¢ƒçš„ observation å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œæ‰€ä»¥ç›´æ¥ä¼ å…¥
        action, _, _ = self.policy.compute_single_action(
            obs=observation,
            deterministic=deterministic
        )
        return int(action)

def play_one_game(env: GameEnvironment, red_player: EvaluationAgent, black_player: EvaluationAgent, seed: int) -> int:
    """è¿›è¡Œä¸€å±€å®Œæ•´çš„æ¸¸æˆï¼Œè¿”å›è·èƒœæ–¹ (1 for red, -1 for black, 0 for draw)"""
    env.reset(seed=seed)
    
    # å¼ºåˆ¶çº¢æ–¹å…ˆæ‰‹
    env.current_player = 1
    
    while True:
        current_player_agent = red_player if env.current_player == 1 else black_player
        
        obs = env.get_state()
        action_mask = env.action_masks()
        obs['action_mask'] = action_mask # å°†æ©ç æ·»åŠ åˆ°è§‚å¯Ÿä¸­
        
        if not np.any(action_mask):
            return -env.current_player # å½“å‰ç©å®¶æ— æ£‹å¯èµ°ï¼Œå¯¹æ–¹è·èƒœ

        action = current_player_agent.predict(obs)
        _, terminated, truncated, winner = env._internal_apply_action(action)
        
        if terminated or truncated:
            return winner if winner is not None else 0

        env.current_player *= -1

def find_latest_checkpoint(directory: str) -> str | None:
    """åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„RLlibæ£€æŸ¥ç‚¹ã€‚"""
    try:
        # PPO_xxxx ç›®å½•
        ppo_dirs = [os.path.join(directory, d) for d in os.listdir(directory) if d.startswith("PPO_")]
        if not ppo_dirs: return None
        latest_experiment = max(ppo_dirs, key=os.path.getmtime)
        
        # checkpoint_xxxx ç›®å½•
        checkpoint_dirs = [os.path.join(latest_experiment, d) for d in os.listdir(latest_experiment) if d.startswith("checkpoint_")]
        if not checkpoint_dirs: return None
        return max(checkpoint_dirs, key=os.path.getmtime)
    except FileNotFoundError:
        return None

def evaluate_models(model1_checkpoint_path: str, model2_checkpoint_path: str):
    """
    æ‰§è¡Œé•œåƒå¯¹å±€è¯„ä¼°ã€‚
    """
    if EVALUATION_GAMES % 2 != 0:
        raise ValueError(f"EVALUATION_GAMES ({EVALUATION_GAMES}) å¿…é¡»æ˜¯å¶æ•°ï¼Œæ‰èƒ½è¿›è¡Œå®Œç¾çš„é•œåƒå¯¹å±€ã€‚")
    num_groups = EVALUATION_GAMES // 2

    print("=" * 70)
    print("           âš”ï¸  RLlib æ¨¡å‹é•œåƒå¯¹å±€è¯„ä¼°ç³»ç»Ÿ âš”ï¸")
    print("=" * 70)

    try:
        # ä»æ£€æŸ¥ç‚¹æ¢å¤ç­–ç•¥
        policy1 = Policy.from_checkpoint(model1_checkpoint_path)
        policy2 = Policy.from_checkpoint(model2_checkpoint_path)
        
        agent1 = EvaluationAgent(policy1, os.path.basename(os.path.dirname(model1_checkpoint_path)))
        agent2 = EvaluationAgent(policy2, os.path.basename(os.path.dirname(model2_checkpoint_path)))
        
        print(f"è¯„æµ‹æ¨¡å‹ A: {agent1.name}")
        print(f"è¯„æµ‹æ¨¡å‹ B: {agent2.name}")
        print(f"å¯¹å±€ç»„æ•°: {num_groups} (æ€»è®¡ {EVALUATION_GAMES} å±€æ¸¸æˆ)")
        print("-" * 70)
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    eval_env = GameEnvironment()
    scores = {
        'model1_wins': 0, 'model2_wins': 0, 'draws': 0,
        'model1_as_red_wins': 0, 'model2_as_red_wins': 0,
    }

    start_time = time.time()
    for i in tqdm(range(num_groups), desc="æ­£åœ¨è¿›è¡Œé•œåƒå¯¹å±€è¯„ä¼°"):
        game_seed = int(time.time_ns() + i) % (2**32 - 1)

        # ç¬¬ä¸€å±€: æ¨¡å‹Aæ‰§çº¢ vs æ¨¡å‹Bæ‰§é»‘
        winner_1 = play_one_game(eval_env, red_player=agent1, black_player=agent2, seed=game_seed)
        if winner_1 == 1:
            scores['model1_wins'] += 1
            scores['model1_as_red_wins'] += 1
        elif winner_1 == -1:
            scores['model2_wins'] += 1
        else:
            scores['draws'] += 1

        # ç¬¬äºŒå±€: æ¨¡å‹Bæ‰§çº¢ vs æ¨¡å‹Aæ‰§é»‘ (é•œåƒ)
        winner_2 = play_one_game(eval_env, red_player=agent2, black_player=agent1, seed=game_seed)
        if winner_2 == 1:
            scores['model2_wins'] += 1
            scores['model2_as_red_wins'] += 1
        elif winner_2 == -1:
            scores['model1_wins'] += 1
        else:
            scores['draws'] += 1
            
    eval_env.close()
    end_time = time.time()

    total_games = num_groups * 2
    total_decisive_games = scores['model1_wins'] + scores['model2_wins']
    win_rate_model1 = scores['model1_wins'] / total_decisive_games if total_decisive_games > 0 else 0.0

    print("\n" + "=" * 70)
    print("           ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ ğŸ“Š")
    print("=" * 70)
    print(f"æ€»è®¡ç”¨æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"å¹³å‡æ¯å±€ç”¨æ—¶: {(end_time - start_time) / total_games:.2f} ç§’\n")
    print(f"--- æ€»ä½“æˆ˜ç»© (å…± {total_games} å±€) ---")
    print(f"  - {agent1.name} èƒœ: {scores['model1_wins']} å±€")
    print(f"  - {agent2.name} èƒœ: {scores['model2_wins']} å±€")
    print(f"  - å¹³å±€: {scores['draws']} å±€\n")
    print(f"--- èƒœç‡è®¡ç®— (åŸºäºéå¹³å±€) ---")
    print(f"  - {agent1.name} èƒœç‡: {win_rate_model1:.2%}")
    print(f"  - {agent2.name} èƒœç‡: {1.0 - win_rate_model1:.2%}\n")
    print(f"--- å…ˆæ‰‹ï¼ˆçº¢æ–¹ï¼‰è¡¨ç°åˆ†æ ---")
    print(f"  - {agent1.name} æ‰§çº¢èƒœå±€: {scores['model1_as_red_wins']} / {num_groups} ({scores['model1_as_red_wins']/num_groups:.2%})")
    print(f"  - {agent2.name} æ‰§çº¢èƒœå±€: {scores['model2_as_red_wins']} / {num_groups} ({scores['model2_as_red_wins']/num_groups:.2%})")
    print("=" * 70)


if __name__ == '__main__':
    # åˆå§‹åŒ– Ray
    ray.init(local_mode=True) # åœ¨æœ¬åœ°æ¨¡å¼ä¸‹è¿è¡Œï¼Œæ–¹ä¾¿è°ƒè¯•

    # --- é…ç½®è¦è¯„ä¼°çš„æ¨¡å‹æ£€æŸ¥ç‚¹ ---
    # è„šæœ¬ä¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
    # å¦‚æœè¦æŒ‡å®šç‰¹å®šæ¨¡å‹ï¼Œè¯·ç›´æ¥æä¾›æ£€æŸ¥ç‚¹ç›®å½•çš„å®Œæ•´è·¯å¾„
    
    # ç¤ºä¾‹1: è¯„ä¼°æœ€æ–°çš„ä¸¤ä¸ªè®­ç»ƒè¿è¡Œ
    # MODEL_A_CHECKPOINT = find_latest_checkpoint(TENSORBOARD_LOG_PATH) # é€šå¸¸æ˜¯ä¸»å®°è€…
    # MODEL_B_CHECKPOINT = find_latest_checkpoint(...) # å¯èƒ½æ˜¯å¦ä¸€ä¸ªåˆ†æ”¯çš„æ¨¡å‹
    
    # ç¤ºä¾‹2: æ‰‹åŠ¨æŒ‡å®šè·¯å¾„
    # æ³¨æ„: è·¯å¾„å¿…é¡»æŒ‡å‘ checkpoint_xxxxx ç›®å½•ï¼Œè€Œä¸æ˜¯PPO_...ç›®å½•
    MODEL_A_CHECKPOINT = "/path/to/your/project/tensorboard_logs/self_play_final/PPO_dark_chess_multi_agent_.../checkpoint_000100"
    MODEL_B_CHECKPOINT = "/path/to/your/project/tensorboard_logs/self_play_final/PPO_dark_chess_multi_agent_.../checkpoint_000090"
    
    print("è¯·æ³¨æ„ï¼šè¯·åœ¨è„šæœ¬ä¸­ä¿®æ”¹ MODEL_A_CHECKPOINT å’Œ MODEL_B_CHECKPOINT çš„è·¯å¾„ã€‚")
    
    # if MODEL_A_CHECKPOINT and MODEL_B_CHECKPOINT:
    #     evaluate_models(MODEL_A_CHECKPOINT, MODEL_B_CHECKPOINT)
    # else:
    #     print("é”™è¯¯: æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œè¯·æ£€æŸ¥ TENSORBOARD_LOG_PATH æˆ–æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ã€‚")

    ray.shutdown()