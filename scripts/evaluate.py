# rl_code/rllib_version/scripts/evaluate.py

import os
import sys
import time
import numpy as np
from tqdm import tqdm

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import ray
from ray.rllib.algorithms.algorithm import Algorithm # å¯¼å…¥ Algorithm
from ray.rllib.policy.policy import Policy
from ray.tune.registry import register_env

# å‡è®¾ä½ çš„ç¯å¢ƒå’Œå¸¸é‡å®šä¹‰åœ¨è¿™é‡Œ
from core.environment import DarkChessEnv # æ”¹ä¸º MultiAgent ç¯å¢ƒçš„åº•å±‚
from core.multi_agent_env import RLLibMultiAgentEnv
from utils.constants import *

# æ³¨å†Œç¯å¢ƒ
register_env("dark_chess_env", lambda config: RLLibMultiAgentEnv(config))

class EvaluationAgent:
    """ç”¨äºè¯„ä¼°çš„AIä»£ç†åŒ…è£…å™¨"""
    def __init__(self, policy: Policy, name: str):
        self.policy = policy
        self.name = name
        # RLModule API ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
        self.is_rl_module = not hasattr(self.policy, 'compute_single_action')

    def predict(self, observation: dict, deterministic: bool = True) -> int:
        if self.is_rl_module:
            # æ–°ç‰ˆ RLModule API
            import torch
            # RLModule éœ€è¦ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦
            obs_tensor = {k: torch.from_numpy(np.expand_dims(v, axis=0)) for k, v in observation.items()}
            
            if deterministic:
                action_dist_inputs = self.policy.forward_inference({"obs": obs_tensor})[Columns.ACTION_DIST_INPUTS]
                action = torch.argmax(action_dist_inputs, dim=1).item()
            else:
                action_dist_inputs = self.policy.forward_exploration({"obs": obs_tensor})[Columns.ACTION_DIST_INPUTS]
                dist_class = self.policy.get_exploration_action_dist_cls()
                action_dist = dist_class.from_logits(action_dist_inputs)
                action = action_dist.sample().item()
        else:
            # æ—§ç‰ˆ ModelV2 API
            action, _, _ = self.policy.compute_single_action(
                obs=observation,
                deterministic=deterministic
            )
        return int(action)

def play_one_game(env: DarkChessEnv, red_player: EvaluationAgent, black_player: EvaluationAgent, seed: int) -> int:
    """è¿›è¡Œä¸€å±€å®Œæ•´çš„æ¸¸æˆï¼Œè¿”å›è·èƒœæ–¹ (1 for red, -1 for black, 0 for draw)"""
    env.reset(seed=seed)
    
    while True:
        current_player_agent = red_player if env.current_player == 1 else black_player
        
        # obs å­—å…¸éœ€è¦åŒ…å« action_mask
        obs = env.get_state()
        action_mask = env.action_masks()
        obs['action_mask'] = action_mask
        
        if not np.any(action_mask):
            return -env.current_player

        action = current_player_agent.predict(obs)
        _, terminated, truncated, winner = env.step(action)
        
        if terminated or truncated:
            return winner if winner is not None else 0


def find_latest_checkpoint_from_experiment(experiment_path: str) -> str | None:
    """åœ¨æŒ‡å®šçš„å®éªŒç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹ã€‚"""
    try:
        checkpoint_dirs = [
            os.path.join(experiment_path, d)
            for d in os.listdir(experiment_path)
            if d.startswith("checkpoint_") and os.path.isdir(os.path.join(experiment_path, d))
        ]
        if not checkpoint_dirs:
            return None
        return max(checkpoint_dirs, key=os.path.getmtime)
    except FileNotFoundError:
        return None

def evaluate_models(model1_checkpoint_path: str, model2_checkpoint_path: str):
    """æ‰§è¡Œé•œåƒå¯¹å±€è¯„ä¼°ã€‚"""
    if EVALUATION_GAMES % 2 != 0:
        raise ValueError(f"EVALUATION_GAMES ({EVALUATION_GAMES}) å¿…é¡»æ˜¯å¶æ•°ï¼Œæ‰èƒ½è¿›è¡Œå®Œç¾çš„é•œåƒå¯¹å±€ã€‚")
    num_groups = EVALUATION_GAMES // 2

    print("=" * 70)
    print("           âš”ï¸  RLlib æ¨¡å‹é•œåƒå¯¹å±€è¯„ä¼°ç³»ç»Ÿ âš”ï¸")
    print("=" * 70)

    try:
        # --- æ ‡å‡†åŒ–æ¨¡å‹åŠ è½½ ---
        print(f"æ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤ç®—æ³• A: {model1_checkpoint_path}")
        algo1 = Algorithm.from_checkpoint(model1_checkpoint_path)
        print(f"æ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤ç®—æ³• B: {model2_checkpoint_path}")
        algo2 = Algorithm.from_checkpoint(model2_checkpoint_path)
        
        # --- è·å–ä¸»ç­–ç•¥ ---
        policy1 = algo1.get_policy(MAIN_POLICY_ID)
        policy2 = algo2.get_policy(MAIN_POLICY_ID)
        
        agent1 = EvaluationAgent(policy1, os.path.basename(os.path.dirname(model1_checkpoint_path)))
        agent2 = EvaluationAgent(policy2, os.path.basename(os.path.dirname(model2_checkpoint_path)))
        
        print(f"\nè¯„æµ‹æ¨¡å‹ A: {agent1.name}")
        print(f"è¯„æµ‹æ¨¡å‹ B: {agent2.name}")
        print(f"å¯¹å±€ç»„æ•°: {num_groups} (æ€»è®¡ {EVALUATION_GAMES} å±€æ¸¸æˆ)")
        print("-" * 70)
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # ä½¿ç”¨åº•å±‚çš„å•æ™ºèƒ½ä½“ç¯å¢ƒè¿›è¡Œè¯„ä¼°
    eval_env = DarkChessEnv()
    scores = {'model1_wins': 0, 'model2_wins': 0, 'draws': 0, 'model1_as_red_wins': 0, 'model2_as_red_wins': 0}

    start_time = time.time()
    for i in tqdm(range(num_groups), desc="æ­£åœ¨è¿›è¡Œé•œåƒå¯¹å±€è¯„ä¼°"):
        game_seed = int(time.time() * 1000 + i) % (2**32 - 1)

        winner_1 = play_one_game(eval_env, red_player=agent1, black_player=agent2, seed=game_seed)
        if winner_1 == 1: scores['model1_wins'] += 1; scores['model1_as_red_wins'] += 1
        elif winner_1 == -1: scores['model2_wins'] += 1
        else: scores['draws'] += 1

        winner_2 = play_one_game(eval_env, red_player=agent2, black_player=agent1, seed=game_seed)
        if winner_2 == 1: scores['model2_wins'] += 1; scores['model2_as_red_wins'] += 1
        elif winner_2 == -1: scores['model1_wins'] += 1
        else: scores['draws'] += 1
            
    eval_env.close()
    end_time = time.time()
    
    # ... [ç»“æœæ‰“å°éƒ¨åˆ†ä¿æŒä¸å˜] ...
    total_games = num_groups * 2
    total_decisive_games = scores['model1_wins'] + scores['model2_wins']
    win_rate_model1 = scores['model1_wins'] / total_decisive_games if total_decisive_games > 0 else 0.0

    print("\n" + "=" * 70)
    print("           ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ ğŸ“Š")
    print("=" * 70)
    print(f"æ€»è®¡ç”¨æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"å¹³å‡æ¯å±€ç”¨æ—¶: {(end_time - start_time) / total_games:.2f} ç§’\n")
    print(f"--- æ€»ä½“æˆ˜ç»© (å…± {total_games} å±€) ---")
    print(f"  - {agent1.name} èƒœ: {scores['model1_wins']} å±€")
    print(f"  - {agent2.name} èƒœ: {scores['model2_wins']} å±€")
    print(f"  - å¹³å±€: {scores['draws']} å±€\n")
    print(f"--- èƒœç‡è®¡ç®— (åŸºäºéå¹³å±€) ---")
    print(f"  - {agent1.name} èƒœç‡: {win_rate_model1:.2%}")
    print(f"  - {agent2.name} èƒœç‡: {1.0 - win_rate_model1:.2%}\n")
    print(f"--- å…ˆæ‰‹ï¼ˆçº¢æ–¹ï¼‰è¡¨ç°åˆ†æ ---")
    print(f"  - {agent1.name} æ‰§çº¢èƒœå±€: {scores['model1_as_red_wins']} / {num_groups} ({scores['model1_as_red_wins']/num_groups:.2%})")
    print(f"  - {agent2.name} æ‰§çº¢èƒœå±€: {scores['model2_as_red_wins']} / {num_groups} ({scores['model2_as_red_wins']/num_groups:.2%})")
    print("=" * 70)


if __name__ == '__main__':
    ray.init(local_mode=True)

    # --- é…ç½®è¦è¯„ä¼°çš„æ¨¡å‹æ£€æŸ¥ç‚¹ ---
    # ã€é‡è¦ã€‘è¯·æä¾›å®éªŒç›®å½•çš„è·¯å¾„ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
    # ä¾‹å¦‚: .../tensorboard_logs/self_play_final/PPO_self_play_experiment_.../
    
    # ç¤ºä¾‹:
    MODEL_A_EXPERIMENT_PATH = "/path/to/your/project/tensorboard_logs/self_play_final/PPO_self_play_experiment_2023-10-27_10-00-00_.../"
    MODEL_B_EXPERIMENT_PATH = "/path/to/your/project/tensorboard_logs/self_play_final/PPO_self_play_experiment_2023-10-26_18-00-00_.../"

    print("è¯·æ³¨æ„ï¼šè¯·åœ¨è„šæœ¬ä¸­ä¿®æ”¹ MODEL_A_EXPERIMENT_PATH å’Œ MODEL_B_EXPERIMENT_PATH çš„è·¯å¾„ã€‚")
    
    # checkpoint_a = find_latest_checkpoint_from_experiment(MODEL_A_EXPERIMENT_PATH)
    # checkpoint_b = find_latest_checkpoint_from_experiment(MODEL_B_EXPERIMENT_PATH)
    
    # if checkpoint_a and checkpoint_b:
    #     evaluate_models(checkpoint_a, checkpoint_b)
    # else:
    #     print("é”™è¯¯: æœªèƒ½åœ¨æŒ‡å®šçš„å®éªŒç›®å½•ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ã€‚")

    ray.shutdown()