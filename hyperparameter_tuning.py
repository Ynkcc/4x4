# hyperparameter_tuning.py

import os
import warnings
import optuna
import time
import traceback

# ç¦ç”¨TensorFlowè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore', category=UserWarning, module='google.protobuf')

from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv
from sb3_contrib import MaskablePPO

# å¯¼å…¥é¡¹ç›®ä¸­çš„æ¨¡å—
from game.environment import GameEnvironment
from game.policy import CustomActorCriticPolicy
from training.evaluator import evaluate_models
# å¯¼å…¥ç»è¿‡æ•´ç†çš„å¸¸é‡
from utils.constants import (
    N_ENVS,
    MAIN_OPPONENT_PATH
)

# --- è°ƒä¼˜é…ç½® ---
# ä¼˜åŒ–è¯•éªŒçš„æ€»æ¬¡æ•°
N_TRIALS = 50
# æ¯æ¬¡è¯•éªŒä¸­ï¼Œæ¨¡åž‹è®­ç»ƒçš„æ€»æ­¥æ•° (ä¸ºäº†å¿«é€Ÿè¿­ä»£ï¼Œå¯ä»¥è®¾ç½®å¾—æ¯”ä¸»è®­ç»ƒå°)
TRAINING_TIMESTEPS_PER_TRIAL = 2048 * 10
# è¯„ä¼°æ—¶ä½¿ç”¨çš„åŸºå‡†æ¨¡åž‹
BASELINE_MODEL_PATH = MAIN_OPPONENT_PATH
# ä¼˜åŒ–ç»“æžœå­˜å‚¨è·¯å¾„
TUNING_OUTPUT_DIR = "./tuning_results/"
# Optunaæ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽæŒä¹…åŒ–å­˜å‚¨ç ”ç©¶ç»“æžœ
OPTUNA_DB_PATH = os.path.join(TUNING_OUTPUT_DIR, "tuning_study.db")


def objective(trial: optuna.Trial) -> float:
    """
    Optunaçš„ç›®æ ‡å‡½æ•°ï¼Œç”¨äºŽå®šä¹‰ã€è®­ç»ƒå’Œè¯„ä¼°ä¸€ç»„è¶…å‚æ•°ã€‚
    """
    print(f"\n{'='*50}\n trial {trial.number}/{N_TRIALS} is starting...\n{'='*50}")

    # --- 1. å®šä¹‰è¶…å‚æ•°çš„æœç´¢ç©ºé—´ ---
    # PPO æ ¸å¿ƒå‚æ•°
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_categorical("n_steps", [512, 1024, 2048, 4096])
    batch_size = trial.suggest_categorical("batch_size", [64, 128, 256, 512, 1024])
    n_epochs = trial.suggest_int("n_epochs", 5, 20)
    clip_range = trial.suggest_categorical("clip_range", [0.1, 0.2, 0.3])
    gae_lambda = trial.suggest_float("gae_lambda", 0.9, 0.99)
    ent_coef = trial.suggest_float("ent_coef", 0.0, 0.1)
    vf_coef = trial.suggest_float("vf_coef", 0.2, 0.8)

    # è‡ªå®šä¹‰ç½‘ç»œæž¶æž„å‚æ•°
    net_features_dim = trial.suggest_categorical("net_features_dim", [128, 256, 512])
    net_num_res_blocks = trial.suggest_int("net_num_res_blocks", 2, 8)
    net_num_hidden_channels = trial.suggest_categorical("net_num_hidden_channels", [32, 64, 128])

    # å‰ªæžè§„åˆ™ï¼šç¡®ä¿ n_steps >= batch_size
    if n_steps < batch_size:
        raise optuna.exceptions.TrialPruned()

    # --- 2. ä½¿ç”¨å»ºè®®çš„è¶…å‚æ•°åˆ›å»ºçŽ¯å¢ƒå’Œæ¨¡åž‹ ---
    run_name = f"trial_{trial.number}_{int(time.time())}"
    # å°†ä¸´æ—¶æ¨¡åž‹å­˜æ”¾åœ¨å­ç›®å½•ä¸­ï¼Œä¿æŒæ ¹ç›®å½•æ•´æ´
    temp_model_dir = os.path.join(TUNING_OUTPUT_DIR, "temp_models")
    os.makedirs(temp_model_dir, exist_ok=True)
    trial_model_save_path = os.path.join(temp_model_dir, f"{run_name}.zip")

    try:
        # åˆ›å»ºè®­ç»ƒçŽ¯å¢ƒ (ç›´æŽ¥è‡ªæˆ‘å¯¹å¼ˆ)
        env = make_vec_env(GameEnvironment, n_envs=N_ENVS, vec_env_cls=DummyVecEnv)

        model = MaskablePPO(
            policy=CustomActorCriticPolicy,
            env=env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            clip_range=clip_range,
            gae_lambda=gae_lambda,
            ent_coef=ent_coef,
            vf_coef=vf_coef,
            device='auto',
            verbose=0, # åœ¨è°ƒä¼˜æ—¶å…³é—­SB3çš„è¯¦ç»†æ—¥å¿—
            policy_kwargs={
                'features_extractor_kwargs': {
                    'features_dim': net_features_dim,
                    'num_res_blocks': net_num_res_blocks,
                    'num_hidden_channels': net_num_hidden_channels
                }
            }
        )

        # --- 3. è®­ç»ƒæ¨¡åž‹ ---
        print(f"Training trial {trial.number} for {TRAINING_TIMESTEPS_PER_TRIAL} timesteps...")
        model.learn(total_timesteps=TRAINING_TIMESTEPS_PER_TRIAL, progress_bar=False)
        model.save(trial_model_save_path)

        env.close() # æ¸…ç†çŽ¯å¢ƒ

        # --- 4. è¯„ä¼°æ¨¡åž‹ ---
        # ç¡®ä¿åŸºå‡†æ¨¡åž‹å­˜åœ¨
        if not os.path.exists(BASELINE_MODEL_PATH):
            print(f"è­¦å‘Š: åŸºå‡†æ¨¡åž‹ {BASELINE_MODEL_PATH} ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡è¯„ä¼°å¹¶è¿”å›ž0.0ã€‚") #
            return 0.0

        print(f"Evaluating trial {trial.number} against baseline model...")
        # evaluate_models è¿”å›žæŒ‘æˆ˜è€…çš„èƒœçŽ‡
        win_rate = evaluate_models(
            challenger_path=trial_model_save_path,
            main_opponent_path=BASELINE_MODEL_PATH,
            show_progress=False # åœ¨è°ƒä¼˜æ—¶å…³é—­è¯„ä¼°çš„è¿›åº¦æ¡
        )
        print(f"Trial {trial.number} result: win_rate = {win_rate:.2%}")

    except Exception as e:
        print(f"Trial {trial.number} failed with error: {e}")
        traceback.print_exc()
        # å¦‚æžœå‡ºé”™ï¼Œè¿”å›žä¸€ä¸ªå¾ˆå·®çš„ç»“æžœ
        return 0.0
    finally:
        # æ¸…ç†æœ¬æ¬¡è¯•éªŒäº§ç”Ÿçš„æ¨¡åž‹æ–‡ä»¶
        if os.path.exists(trial_model_save_path):
            os.remove(trial_model_save_path)

    return win_rate


if __name__ == '__main__':
    # --- ä¸»ç¨‹åºå…¥å£ ---
    print("ðŸš€ Starting hyperparameter tuning with Optuna...")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(TUNING_OUTPUT_DIR, exist_ok=True)

    # åˆ›å»ºä¸€ä¸ªç ”ç©¶(study)ï¼Œå¹¶ä½¿ç”¨SQLiteè¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨
    # è¿™æ ·å³ä½¿ç¨‹åºä¸­æ–­ï¼Œä¹Ÿå¯ä»¥ä»Žä¸Šæ¬¡çš„è¿›åº¦ç»§ç»­
    storage_name = f"sqlite:///{OPTUNA_DB_PATH}"
    study = optuna.create_study(
        study_name="dark_chess_ppo_tuning",
        direction="maximize",
        sampler=optuna.samplers.TPESampler(),
        storage=storage_name,
        load_if_exists=True # å¦‚æžœæ•°æ®åº“æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åŠ è½½ä¹‹å‰çš„ç ”ç©¶
    )

    try:
        # è¿è¡Œä¼˜åŒ–
        study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)
    except KeyboardInterrupt:
        print("\nðŸ›‘ Tuning process interrupted by user.")

    # --- æ‰“å°ä¼˜åŒ–ç»“æžœ ---
    print("\n" + "="*50)
    print("          ðŸ“Š Hyperparameter Tuning Results ðŸ“Š")
    print("="*50)
    print(f"Study name: {study.study_name}")
    print(f"Storage: {storage_name}")
    print(f"Number of finished trials: {len(study.trials)}")

    print("\n--- Best Trial ---")
    best_trial = study.best_trial
    print(f"  Value (Win Rate): {best_trial.value:.4f}")

    print("  Best Parameters: ")
    for key, value in best_trial.params.items():
        print(f"    {key}: {value}")

    # --- ä¿å­˜æœ€ä½³å‚æ•°åˆ°æ–‡ä»¶ ---
    best_params_path = os.path.join(TUNING_OUTPUT_DIR, "best_params.txt")
    with open(best_params_path, "w") as f:
        f.write("Best trial:\n")
        f.write(f"  Value: {best_trial.value}\n")
        f.write("  Params:\n")
        for key, value in best_trial.params.items():
            f.write(f"    {key}: {value}\n")
    print(f"\nâœ… Best parameters saved to: {best_params_path}")

    print("\n--- All Trials Summary (Top 10) ---")
    df = study.trials_dataframe(attrs=("number", "value", "params", "state"))
    print(df.sort_values("value", ascending=False).head(10))

    print(f"\nâœ… Tuning finished! You can find the best parameters above.")
    print("To visualize results, you can use optuna-dashboard:")
    print("  pip install optuna-dashboard")
    print(f"  optuna-dashboard {storage_name}")