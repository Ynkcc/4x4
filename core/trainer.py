# rllib_version_complete/core/trainer.py

import os
import shutil
import torch
import numpy as np
import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.policy.policy import PolicySpec
from ray.rllib.models import ModelCatalog
from ray.tune.registry import register_env
from typing import Dict, Any, Optional

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from core.multi_agent_env import RLLibMultiAgentEnv
from core.policy import RLLibCustomNetwork
from callbacks.self_play_callback import SelfPlayCallback
from utils import elo
from utils.constants import *

class RLLibSelfPlayTrainer:
    """
    ä½¿ç”¨ Ray RLlib çš„è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå™¨ã€‚
    """
    def __init__(self):
        self._setup_directories()
        self.elo_ratings = elo.load_elo_ratings()
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å¯¹æ‰‹æ± ï¼Œå°†åœ¨è¿è¡Œæ—¶åŠ¨æ€å¡«å……
        self.opponent_policies_specs: Dict[str, PolicySpec] = {}

    def _setup_directories(self):
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•ã€‚"""
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        os.makedirs(TENSORBOARD_LOG_PATH, exist_ok=True)

    def _get_opponent_sampling_distribution(self) -> Dict[str, float]:
        """
        æ ¹æ®Eloå·®å¼‚è®¡ç®—å¯¹æ‰‹çš„é‡‡æ ·æ¦‚çŽ‡ã€‚
        å¯¹æ‰‹æ± ä¸­é™¤äº†åŽ†å²æ¨¡åž‹ï¼Œè¿˜åŒ…æ‹¬å½“å‰çš„ä¸»ç­–ç•¥è‡ªèº«ã€‚
        """
        opponents = [f for f in os.listdir(OPPONENT_POOL_DIR) if f.endswith('.pt')]
        main_elo = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
        
        weights = {}
        # ä¸ºä¸»ç­–ç•¥è®¾ç½®ä¸€ä¸ªåŸºç¡€æƒé‡ï¼Œä½¿å…¶æœ‰ä¸€å®šæ¦‚çŽ‡ä¸Žè‡ªå·±å¯¹æˆ˜
        weights[MAIN_POLICY_ID] = 1.0 
        
        for opp_name in opponents:
            opp_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{opp_name.replace('.pt', '')}"
            opp_elo = self.elo_ratings.get(opp_policy_id, ELO_DEFAULT)
            # ä½¿ç”¨æ¸©åº¦å‚æ•°æ¥å¹³æ»‘Eloå·®å¼‚ï¼Œä½¿å¾—Eloç›¸è¿‘çš„å¯¹æ‰‹æ›´å®¹æ˜“è¢«é€‰ä¸­
            weight = np.exp(-abs(main_elo - opp_elo) / ELO_WEIGHT_TEMPERATURE)
            weights[opp_policy_id] = weight

        total_weight = sum(weights.values())
        if total_weight == 0:
             return {MAIN_POLICY_ID: 1.0}
        return {k: v / total_weight for k, v in weights.items()}

    def _load_opponent_policies(self):
        """ä»Žå¯¹æ‰‹æ± ç›®å½•åŠ è½½æ‰€æœ‰å¯¹æ‰‹ç­–ç•¥ï¼Œå¹¶ä¸ºå®ƒä»¬åˆ›å»ºPolicySpecã€‚"""
        self.opponent_policies_specs.clear()
        for filename in os.listdir(OPPONENT_POOL_DIR):
            if filename.endswith('.pt'):
                policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{filename.replace('.pt', '')}"
                self.opponent_policies_specs[policy_id] = PolicySpec()
        print(f"ä»Žæ± ä¸­åŠ è½½äº† {len(self.opponent_policies_specs)} ä¸ªå¯¹æ‰‹ç­–ç•¥ã€‚")

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚"""
        print("--- [æ­¥éª¤ 1/4] åˆå§‹åŒ– Ray å’ŒçŽ¯å¢ƒ ---")
        ray.init(num_gpus=1 if PPO_DEVICE == 'cuda' else 0, local_mode=False)
        
        ModelCatalog.register_custom_model("custom_torch_model", RLLibCustomNetwork)
        register_env("dark_chess_multi_agent", lambda config: RLLibMultiAgentEnv(config))
        self._load_opponent_policies()

        print("--- [æ­¥éª¤ 2/4] é…ç½® PPO ç®—æ³• ---")
        
        # åŠ¨æ€æ›´æ–°çš„å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒ
        opponent_dist = self._get_opponent_sampling_distribution()
        
        def policy_mapping_fn(agent_id, episode, worker, **kwargs):
            if agent_id == "player1":
                return MAIN_POLICY_ID
            else: # player2
                # ä»Žå¯¹æ‰‹æ± ï¼ˆåŒ…å«ä¸»ç­–ç•¥ï¼‰ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªç­–ç•¥
                policies = list(opponent_dist.keys())
                probabilities = list(opponent_dist.values())
                if not policies or sum(probabilities) == 0:
                    return MAIN_POLICY_ID
                return np.random.choice(policies, p=probabilities)

        config = (
            PPOConfig()
            .environment("dark_chess_multi_agent")
            .framework("torch")
            .env_runners(num_env_runners=N_ENVS, rollout_fragment_length="auto")
            .training(
                model={"custom_model": "custom_torch_model"},
                lr=INITIAL_LR,
                clip_param=PPO_CLIP_RANGE,
                train_batch_size=PPO_N_STEPS * N_ENVS,
                minibatch_size=PPO_BATCH_SIZE,
                num_epochs=PPO_N_EPOCHS,
                lambda_=PPO_GAE_LAMBDA,
                vf_loss_coeff=PPO_VF_COEF,
                entropy_coeff=PPO_ENT_COEF,
            )
            .multi_agent(
                policies={MAIN_POLICY_ID: PolicySpec()} | self.opponent_policies_specs,
                policy_mapping_fn=policy_mapping_fn,
                policies_to_train=[MAIN_POLICY_ID],
            )
            .resources(num_gpus=1 if PPO_DEVICE == 'cuda' else 0)
            .callbacks(SelfPlayCallback) # ä½¿ç”¨ç±»å¼•ç”¨
            .api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)
        )

        latest_checkpoint = self._find_latest_checkpoint(TENSORBOARD_LOG_PATH)
        if latest_checkpoint:
            print(f"--- å‘çŽ°æ£€æŸ¥ç‚¹ï¼Œä»Ž {latest_checkpoint} æ¢å¤è®­ç»ƒ ---")
            algo = config.build()
            algo.restore(latest_checkpoint)
        else:
            print("--- æœªå‘çŽ°æ£€æŸ¥ç‚¹ï¼Œå¼€å§‹æ–°çš„è®­ç»ƒ ---")
            algo = config.build()

        # ä¸ºæ± ä¸­çš„éžè®­ç»ƒç­–ç•¥åŠ è½½æƒé‡
        main_policy = algo.get_policy(MAIN_POLICY_ID)
        for policy_id in self.opponent_policies_specs.keys():
            opponent_model_name = policy_id.replace(OPPONENT_POLICY_ID_PREFIX, "") + ".pt"
            model_path = os.path.join(OPPONENT_POOL_DIR, opponent_model_name)
            if os.path.exists(model_path):
                 print(f"ä¸ºç­–ç•¥ {policy_id} åŠ è½½æƒé‡: {model_path}")
                 policy = algo.get_policy(policy_id)
                 # åˆ›å»ºä¸€ä¸ªä¸Žä¸»ç­–ç•¥ç›¸åŒç±»åž‹çš„æ¨¡åž‹å®žä¾‹ï¼Œç„¶åŽåŠ è½½çŠ¶æ€
                 state_dict = torch.load(model_path, map_location=policy.device)
                 policy.model.load_state_dict(state_dict)
                 # ç¡®ä¿å¯¹æ‰‹ç­–ç•¥ä¸è®­ç»ƒ
                 policy.lock_weights()

        print("--- [æ­¥éª¤ 3/4] å¼€å§‹è‡ªæˆ‘å¯¹å¼ˆä¸»å¾ªçŽ¯ ---")
        for i in range(1, TOTAL_TRAINING_LOOPS + 1):
            print(f"\n{'='*70}\nðŸ”„ è®­ç»ƒå¾ªçŽ¯ {i}/{TOTAL_TRAINING_LOOPS}\n{'='*70}")
            
            # æ¯è½®è®­ç»ƒå‰éƒ½æ›´æ–°ä¸€æ¬¡å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒ
            opponent_dist = self._get_opponent_sampling_distribution()
            print("å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒ:", {k: f"{v:.2%}" for k, v in opponent_dist.items()})

            result = algo.train()
            
            checkpoint_dir = algo.save(checkpoint_dir=TENSORBOARD_LOG_PATH)
            print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {checkpoint_dir}")

        print("\n--- [æ­¥éª¤ 4/4] è®­ç»ƒå®Œæˆï¼ ---")
        algo.stop()
        ray.shutdown()

    def _find_latest_checkpoint(self, directory: str) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„RLlibæ£€æŸ¥ç‚¹ç›®å½•ã€‚"""
        try:
            checkpoints = [os.path.join(directory, d) for d in os.listdir(directory) if d.startswith("PPO_")]
            if not checkpoints:
                return None
            # æ‰¾åˆ°æœ€æ–°çš„å®žéªŒç›®å½•
            latest_experiment = max(checkpoints, key=os.path.getmtime)
            
            # åœ¨å®žéªŒç›®å½•ä¸­æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
            checkpoint_dirs = [os.path.join(latest_experiment, d) for d in os.listdir(latest_experiment) if d.startswith("checkpoint_")]
            if not checkpoint_dirs:
                return None
            return max(checkpoint_dirs, key=os.path.getmtime)
        except FileNotFoundError:
            return None
