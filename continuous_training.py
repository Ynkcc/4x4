# train_and_eval_simple.py
import os
import warnings

# ç¦ç”¨TensorFlowè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ç¦ç”¨INFOå’ŒWARNINGæ—¥å¿—
warnings.filterwarnings('ignore', category=UserWarning, module='google.protobuf')

# continuous_training.py
"""
ç»Ÿä¸€çš„æŒç»­è®­ç»ƒè„šæœ¬ - æ•´åˆè¯„ä¼°ã€è®­ç»ƒã€åˆ†æåŠŸèƒ½
æ¯æ¬¡è¿è¡Œè®­ç»ƒ81920æ­¥ï¼Œæ”¯æŒä»ä»»ä½•é˜¶æ®µç»§ç»­è®­ç»ƒ
"""
import os
import numpy as np
from datetime import datetime
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from sb3_contrib import MaskablePPO

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from game.environment import (
    GameEnvironment, PieceType, PIECE_VALUES,
    REVEAL_ACTIONS_COUNT, REGULAR_MOVE_ACTIONS_COUNT
)
from game.policy import CustomActorCriticPolicy
from training.simple_agent import SimpleAgent
from utils.scheduler import linear_schedule
from utils.constants import INITIAL_LR, N_ENVS

class ContinuousTrainer:
    """æŒç»­è®­ç»ƒç®¡ç†å™¨"""

    def __init__(self):
        # è®­ç»ƒé…ç½®
        self.STEPS_PER_SESSION = 81920  # æ¯æ¬¡è®­ç»ƒæ­¥æ•°
        self.EVALUATION_GAMES = 1000    # è¯„ä¼°å±€æ•°
        # --- ã€ä¿®æ”¹ä¸€ã€‘: å¤§å¹…é™ä½å­¦ä¹ ç‡ï¼Œä» 5e-4 é™è‡³ 5e-5 ---
        self.ENHANCED_LR = 5e-5

        # è·¯å¾„é…ç½®
        self.BASE_MODEL_PATH = "./models/self_play_final/main_opponent.zip"
        self.CURRENT_MODEL_PATH = "./models/continuous_train/current_model.zip"
        self.BACKUP_MODEL_PATH = "./models/continuous_train/backup_model.zip"
        self.LOG_DIR = "./tensorboard_logs/continuous_train/"
        self.PROGRESS_FILE = "./models/continuous_train/training_progress.txt"

        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(self.CURRENT_MODEL_PATH), exist_ok=True)
        os.makedirs(self.LOG_DIR, exist_ok=True)

        # åˆå§‹åŒ–SimpleAgent
        self._setup_opponent()

        # åŠ è½½è®­ç»ƒè¿›åº¦
        self.session_count = 0
        self.best_winrate = 0.0
        self._load_progress()

    def _setup_opponent(self):
        """è®¾ç½®æ™ºèƒ½å¯¹æ‰‹"""
        print("æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½å¯¹æ‰‹...")
        temp_env = GameEnvironment()
        action_to_coords = temp_env.action_to_coords
        temp_env.close()

        self.simple_opponent = SimpleAgent(
            action_to_coords=action_to_coords,
            piece_values=PIECE_VALUES,
            piece_types=PieceType,
            reveal_actions_count=REVEAL_ACTIONS_COUNT,
            regular_move_actions_count=REGULAR_MOVE_ACTIONS_COUNT
        )
        print("âœ“ æ™ºèƒ½å¯¹æ‰‹åˆå§‹åŒ–å®Œæˆ")

    def _load_progress(self):
        """åŠ è½½è®­ç»ƒè¿›åº¦"""
        if os.path.exists(self.PROGRESS_FILE):
            try:
                with open(self.PROGRESS_FILE, 'r') as f:
                    lines = f.readlines()
                    for line in lines:
                        if line.startswith("session_count:"):
                            self.session_count = int(line.split(":")[1].strip())
                        elif line.startswith("best_winrate:"):
                            self.best_winrate = float(line.split(":")[1].strip())
                print(f"âœ“ åŠ è½½è¿›åº¦: ä¼šè¯{self.session_count}, æœ€ä½³èƒœç‡{self.best_winrate:.2%}")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è¿›åº¦å¤±è´¥: {e}")

    def _save_progress(self, winrate):
        """ä¿å­˜è®­ç»ƒè¿›åº¦"""
        try:
            with open(self.PROGRESS_FILE, 'w') as f:
                f.write(f"session_count: {self.session_count}\n")
                f.write(f"best_winrate: {winrate}\n")
                f.write(f"last_update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

    def _get_current_model_path(self):
        """è·å–å½“å‰åº”è¯¥ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„"""
        if os.path.exists(self.CURRENT_MODEL_PATH):
            return self.CURRENT_MODEL_PATH
        elif os.path.exists(self.BASE_MODEL_PATH):
            return self.BASE_MODEL_PATH
        else:
            raise FileNotFoundError("æ‰¾ä¸åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")

    def evaluate_model(self, model_path: str, n_games: int = None) -> dict:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        :param model_path: æ¨¡å‹è·¯å¾„
        :param n_games: è¯„ä¼°å±€æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        :return: è¯„ä¼°ç»“æœå­—å…¸
        """
        if n_games is None:
            n_games = self.EVALUATION_GAMES

        print(f"\n--- [è¯„ä¼°é˜¶æ®µ] ---")
        print(f"æ¨¡å‹: {os.path.basename(model_path)}")
        print(f"å¯¹æ‰‹: SimpleAgent (æ™ºèƒ½ç­–ç•¥)")
        print(f"è¯„ä¼°å±€æ•°: {n_games}")

        eval_env = None
        try:
            eval_env = make_vec_env(
                GameEnvironment,
                n_envs=N_ENVS,
                vec_env_cls=DummyVecEnv,
                env_kwargs={
                    'curriculum_stage': 4,
                    'opponent_agent': self.simple_opponent
                }
            )

            model = MaskablePPO.load(model_path, env=eval_env, device='auto')

            games_played = 0
            wins = 0
            draws = 0
            losses = 0
            all_rewards = []

            obs = eval_env.reset()

            while games_played < n_games:
                action_masks = np.array(eval_env.env_method("action_masks"), dtype=np.int32)
                action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
                obs, rewards, dones, infos = eval_env.step(action)

                for i, done in enumerate(dones):
                    if done:
                        games_played += 1
                        winner = infos[i].get('winner')
                        episode_reward = rewards[i]
                        all_rewards.append(episode_reward)

                        if winner == 1:
                            wins += 1
                        elif winner == -1:
                            losses += 1
                        else:
                            draws += 1

                        if games_played % 100 == 0:
                            print(f"  è¯„ä¼°è¿›åº¦: {games_played}/{n_games} | å½“å‰æˆ˜ç»©: {wins}èƒœ/{losses}è´Ÿ/{draws}å¹³", end="\r")

                        if games_played >= n_games:
                            break

            print("\n--- è¯„ä¼°å®Œæˆ ---")
            total_decisive = wins + losses
            winrate = wins / total_decisive if total_decisive > 0 else 0.0
            avg_reward = np.mean(all_rewards) if all_rewards else 0.0

            result = {
                'wins': wins,
                'losses': losses,
                'draws': draws,
                'winrate': winrate,
                'avg_reward': avg_reward,
                'total_games': games_played
            }

            print(f"æœ€ç»ˆæˆ˜ç»©: {wins}èƒœ / {losses}è´Ÿ / {draws}å¹³")
            print(f"èƒœç‡: {winrate:.2%}")
            print(f"å¹³å‡å¥–åŠ±: {avg_reward:.3f}")

            return result

        finally:
            if eval_env:
                eval_env.close()

    def train_session(self, model_path: str) -> str:
        """
        æ‰§è¡Œä¸€æ¬¡è®­ç»ƒä¼šè¯
        :param model_path: èµ·å§‹æ¨¡å‹è·¯å¾„
        :return: è®­ç»ƒåæ¨¡å‹è·¯å¾„
        """
        print(f"\n--- [è®­ç»ƒä¼šè¯ #{self.session_count + 1}] ---")
        print(f"èµ·å§‹æ¨¡å‹: {os.path.basename(model_path)}")
        print(f"è®­ç»ƒæ­¥æ•°: {self.STEPS_PER_SESSION:,}")
        print(f"å­¦ä¹ ç‡: {self.ENHANCED_LR}") # å·²é™ä½
        print(f"å¹¶è¡Œç¯å¢ƒæ•°: {N_ENVS}")
        print("-" * 50)

        train_env = None
        try:
            # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
            train_env = make_vec_env(
                GameEnvironment,
                n_envs=N_ENVS,
                vec_env_cls=DummyVecEnv,  # ä½¿ç”¨DummyVecEnvé¿å…æ®µé”™è¯¯
                env_kwargs={
                    'curriculum_stage': 4,
                    'opponent_agent': self.simple_opponent
                }
            )

            # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹
            if os.path.exists(model_path):
                model = MaskablePPO.load(
                    model_path,
                    env=train_env,
                    learning_rate=self.ENHANCED_LR,
                    tensorboard_log=self.LOG_DIR,
                    # --- ã€ä¿®æ”¹äºŒã€‘: å¢åŠ  n_steps ä½¿æ›´æ–°æ›´ç¨³å®š ---
                    n_steps=4096,
                    # --- ã€ä¿®æ”¹ä¸‰ã€‘: æ˜ç¡® gamma å€¼ ---
                    gamma=0.99
                )
                print(f"âœ“ ä»ç°æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ (å·²åº”ç”¨æ–°çš„ç¨³å®šåŒ–è¶…å‚æ•°)")
            else:
                model = MaskablePPO(
                    CustomActorCriticPolicy,
                    train_env,
                    learning_rate=linear_schedule(self.ENHANCED_LR),
                    verbose=1,
                    tensorboard_log=self.LOG_DIR,
                    gamma=0.99,
                    n_steps=4096 # åŒæ ·åº”ç”¨äºæ–°æ¨¡å‹
                )
                print(f"âœ“ åˆ›å»ºæ–°æ¨¡å‹å¼€å§‹è®­ç»ƒ")

            # è®¾ç½®æ¨¡å‹å‚æ•°
            model.verbose = 1
            model.tensorboard_log = self.LOG_DIR

            # å¼€å§‹è®­ç»ƒ
            print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
            model.learn(
                total_timesteps=self.STEPS_PER_SESSION,
                progress_bar=True,
                reset_num_timesteps=False
            )

            # ä¿å­˜æ¨¡å‹
            # å¤‡ä»½å½“å‰æ¨¡å‹
            if os.path.exists(self.CURRENT_MODEL_PATH):
                # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨ shutil.move ä»£æ›¿ os.rename, æ›´å¥å£®
                import shutil
                shutil.move(self.CURRENT_MODEL_PATH, self.BACKUP_MODEL_PATH)


            # ä¿å­˜æ–°è®­ç»ƒçš„æ¨¡å‹
            model.save(self.CURRENT_MODEL_PATH)
            print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {self.CURRENT_MODEL_PATH}")

            self.session_count += 1
            return self.CURRENT_MODEL_PATH

        except KeyboardInterrupt:
            print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            return model_path
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return model_path
        finally:
            if train_env:
                train_env.close()

    def run_continuous_training(self):
        """è¿è¡ŒæŒç»­è®­ç»ƒæµç¨‹"""
        print("=" * 60)
        print("           ğŸ¤– æŒç»­è®­ç»ƒç³»ç»Ÿ ğŸ¤–")
        print("=" * 60)
        print(f"æ¯æ¬¡è®­ç»ƒæ­¥æ•°: {self.STEPS_PER_SESSION:,}")
        print(f"å½“å‰ä¼šè¯: #{self.session_count}")
        print(f"å†å²æœ€ä½³èƒœç‡: {self.best_winrate:.2%}")
        print("=" * 60)

        # 1. è·å–å½“å‰æ¨¡å‹
        try:
            current_model = self._get_current_model_path()
            print(f"âœ“ ä½¿ç”¨æ¨¡å‹: {os.path.basename(current_model)}")
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return

        # 2. è®­ç»ƒå‰è¯„ä¼°
        print(f"\nğŸ“Š è®­ç»ƒå‰è¯„ä¼°...")
        pre_result = self.evaluate_model(current_model)
        pre_winrate = pre_result['winrate']

        # 3. æ‰§è¡Œè®­ç»ƒ
        trained_model = self.train_session(current_model)

        # 4. è®­ç»ƒåè¯„ä¼°
        print(f"\nğŸ“Š è®­ç»ƒåè¯„ä¼°...")
        post_result = self.evaluate_model(trained_model)
        post_winrate = post_result['winrate']

        # 5. ç»“æœåˆ†æ
        improvement = post_winrate - pre_winrate
        relative_improvement = (improvement / pre_winrate * 100) if pre_winrate > 0 else 0

        print(f"\n" + "=" * 60)
        print(f"           ğŸ“ˆ è®­ç»ƒä¼šè¯ #{self.session_count} ç»“æœ")
        print("=" * 60)
        print(f"è®­ç»ƒå‰èƒœç‡: {pre_winrate:.2%}")
        print(f"è®­ç»ƒåèƒœç‡: {post_winrate:.2%}")
        print(f"ç»å¯¹æå‡: {improvement:+.2%}")
        if pre_winrate > 0:
            print(f"ç›¸å¯¹æå‡: {relative_improvement:+.1f}%")

        # æ›´æ–°æœ€ä½³è®°å½•
        if post_winrate > self.best_winrate:
            self.best_winrate = post_winrate
            print(f"ğŸ‰ æ–°çš„æœ€ä½³èƒœç‡è®°å½•ï¼")

            # åˆ›å»ºæœ€ä½³æ¨¡å‹å¤‡ä»½
            best_model_path = f"./models/continuous_train/best_model_session_{self.session_count}.zip"
            if os.path.exists(trained_model):
                # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨ shutil.copy2 ä¿è¯å…ƒæ•°æ®ä¹Ÿè¢«å¤åˆ¶
                import shutil
                shutil.copy2(trained_model, best_model_path)
                print(f"âœ“ æœ€ä½³æ¨¡å‹å·²å¤‡ä»½åˆ°: {best_model_path}")
        
        # ä¿å­˜è¿›åº¦
        self._save_progress(post_winrate)

        # è®­ç»ƒå»ºè®®
        print(f"\nğŸ’¡ å»ºè®®:")
        if improvement > 0.001: # è®¾ç½®ä¸€ä¸ªå°çš„é˜ˆå€¼ï¼Œé¿å…å™ªéŸ³
            print(f"   âœ“ è®­ç»ƒæœ‰æ•ˆï¼å»ºè®®ç»§ç»­è®­ç»ƒ")
        else:
            print(f"   âš ï¸ æœ¬æ¬¡è®­ç»ƒèƒœç‡æœªæå‡ï¼Œä½†å‚æ•°å·²è°ƒæ•´ï¼Œå»ºè®®å†è§‚å¯Ÿä¸€è½®ã€‚")

        print(f"\nğŸ”„ è¦ç»§ç»­ä¸‹ä¸€è½®è®­ç»ƒï¼Œè¯·å†æ¬¡è¿è¡Œæ­¤è„šæœ¬")
        print("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    trainer = ContinuousTrainer()
    trainer.run_continuous_training()

if __name__ == "__main__":
    main()