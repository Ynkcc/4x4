# rllib_version_complete/callbacks/self_play_callback.py

import os
import torch
import numpy as np
import json
from typing import Dict, Any, List

from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import RolloutWorker
from ray.rllib.evaluation.episode_v2 import EpisodeV2
from ray.rllib.policy import Policy
from ray.rllib.policy.policy import PolicySpec
from ray.rllib.utils.typing import PolicyID
from ray.rllib.algorithms.algorithm import Algorithm

from utils.constants import *
from utils import elo

class SelfPlayCallback(DefaultCallbacks):
    """
    å¤„ç†è‡ªæˆ‘å¯¹å¼ˆä¸­çš„æ¨¡å‹è¯„ä¼°ã€Eloæ›´æ–°å’Œå¯¹æ‰‹æ± ç®¡ç†çš„æ ¸å¿ƒå›è°ƒ (é‡æ„ç‰ˆ)ã€‚
    èŒè´£:
    - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è‡ªæˆ‘å¯¹å¼ˆçŠ¶æ€ (Elo, æ¨¡å‹æ± , ä»£æ•°)ã€‚
    - åœ¨å¯åŠ¨æ—¶åŠ¨æ€åŠ è½½å¯¹æ‰‹ç­–ç•¥ã€‚
    - åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–° Elo å’Œæ¨¡å‹æ± ã€‚
    - åŠ¨æ€æ›´æ–°å¹¶å¹¿æ’­å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒã€‚
    """
    def __init__(self):
        super().__init__()
        self.win_rates_buffer: Dict[str, list] = {}
        self.elo_ratings: Dict[str, float] = {}
        self.model_generations: Dict[str, int] = {}
        self.latest_generation: int = 0
        self.long_term_pool_paths: List[str] = []
        self.short_term_pool_paths: List[str] = []
        self.long_term_power_of_2: int = 1
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        self._load_state()

    # --- çŠ¶æ€ç®¡ç† ---
    def _load_state(self):
        """ä»JSONæ–‡ä»¶åŠ è½½Eloã€æ¨¡å‹ä»£æ•°å’Œæ¨¡å‹æ± çŠ¶æ€ã€‚"""
        state_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        if os.path.exists(state_file):
            try:
                with open(state_file, 'r') as f:
                    data = json.load(f)
                    self.elo_ratings = data.get("elo", {MAIN_POLICY_ID: ELO_DEFAULT})
                    self.model_generations = data.get("generations", {})
                    self.latest_generation = data.get("latest_generation", 0)
                    self.long_term_pool_paths = data.get("long_term_pool_paths", [])
                    self.short_term_pool_paths = data.get("short_term_pool_paths", [])
                    self.long_term_power_of_2 = data.get("long_term_power_of_2", 1)
            except (json.JSONDecodeError, IOError, KeyError) as e:
                print(f"è­¦å‘Šï¼šè¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥æˆ–æ ¼å¼ä¸å®Œæ•´: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
        else:
             self.elo_ratings = {MAIN_POLICY_ID: ELO_DEFAULT}
             print("æœªæ‰¾åˆ°çŠ¶æ€æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼åˆå§‹åŒ–ã€‚")

    def _save_state(self):
        """å°†Eloã€æ¨¡å‹ä»£æ•°å’Œæ¨¡å‹æ± çŠ¶æ€ä¿å­˜åˆ°åŒä¸€ä¸ªJSONæ–‡ä»¶ã€‚"""
        state_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        data = {
            "elo": self.elo_ratings,
            "generations": self.model_generations,
            "latest_generation": self.latest_generation,
            "long_term_pool_paths": self.long_term_pool_paths,
            "short_term_pool_paths": self.short_term_pool_paths,
            "long_term_power_of_2": self.long_term_power_of_2,
        }
        try:
            with open(state_file, 'w') as f:
                json.dump(data, f, indent=4)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜çŠ¶æ€æ–‡ä»¶: {e}")

    # --- å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒ ---
    def _get_opponent_sampling_distribution(self) -> Dict[str, float]:
        """æ ¹æ®Eloå·®å¼‚è®¡ç®—å¯¹æ‰‹çš„é‡‡æ ·æ¦‚ç‡ã€‚"""
        main_elo = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
        
        weights = {}
        # 1. åŒ…æ‹¬ä¸»ç­–ç•¥è‡ªèº«
        weights[MAIN_POLICY_ID] = 1.0 
        
        # 2. åŒ…æ‹¬æ‰€æœ‰æ± ä¸­å¯¹æ‰‹
        all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
        for opp_name in all_opponents:
            opp_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{opp_name.replace('.pt', '')}"
            opp_elo = self.elo_ratings.get(opp_policy_id, ELO_DEFAULT)
            weight = np.exp(-abs(main_elo - opp_elo) / ELO_WEIGHT_TEMPERATURE)
            weights[opp_policy_id] = weight

        total_weight = sum(weights.values())
        if total_weight == 0:
             return {MAIN_POLICY_ID: 1.0}
        return {k: v / total_weight for k, v in weights.items()}
    
    def _update_sampler_on_workers(self, worker: RolloutWorker):
        """è®¡ç®—å¹¶æ›´æ–°æ‰€æœ‰ worker çš„é‡‡æ ·åˆ†å¸ƒã€‚"""
        dist = self._get_opponent_sampling_distribution()
        # å°†åˆ†å¸ƒæ³¨å…¥åˆ°æ¯ä¸ª workerï¼Œä¾› policy_mapping_fn ä½¿ç”¨
        worker.foreach_policy(lambda p, pid: setattr(p, 'sampler_dist', dist))
        worker.sampler_dist = dist # ä¹Ÿä¸º worker æœ¬èº«è®¾ç½®

    # --- RLlib å›è°ƒé’©å­ ---
    def on_algorithm_init(self, *, algorithm: "Algorithm", **kwargs):
        """åœ¨ç®—æ³•åˆå§‹åŒ–æ—¶ï¼ŒåŠ¨æ€æ·»åŠ æ‰€æœ‰ç°æœ‰çš„å¯¹æ‰‹ç­–ç•¥ã€‚"""
        print("--- Callback: on_algorithm_init ---")
        all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
        main_policy = algorithm.get_policy(MAIN_POLICY_ID)
        
        for opp_name in all_opponents:
            policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{opp_name.replace('.pt', '')}"
            print(f"  - åŠ¨æ€æ·»åŠ å†å²å¯¹æ‰‹ç­–ç•¥: {policy_id}")
            algorithm.add_policy(
                policy_id=policy_id,
                policy_cls=type(main_policy),
                policy_spec=PolicySpec(), # ä½¿ç”¨é»˜è®¤ spec
            )
        
        # åœ¨æ‰€æœ‰ worker ä¸ŠåŠ è½½æƒé‡å¹¶æ›´æ–°é‡‡æ ·å™¨
        algorithm.workers.foreach_worker(self._load_weights_and_update_sampler)
    
    def _load_weights_and_update_sampler(self, worker: RolloutWorker):
        """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºåœ¨ worker ä¸ŠåŠ è½½æƒé‡å’Œæ›´æ–°é‡‡æ ·å™¨ã€‚"""
        all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
        for opp_name in all_opponents:
            policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{opp_name.replace('.pt', '')}"
            model_path = os.path.join(OPPONENT_POOL_DIR, opp_name)
            if os.path.exists(model_path):
                policy = worker.get_policy(policy_id)
                if policy:
                    state_dict = torch.load(model_path, map_location=policy.device)
                    policy.model.load_state_dict(state_dict)
                    policy.lock_weights() # ç¡®ä¿å¯¹æ‰‹ç­–ç•¥ä¸è¢«è®­ç»ƒ
        
        # æ›´æ–°è¯¥ worker çš„é‡‡æ ·åˆ†å¸ƒ
        self._update_sampler_on_workers(worker)

    def on_episode_end(
        self, *, worker: "RolloutWorker", base_env: BaseEnv, policies: Dict[PolicyID, Policy],
        episode: EpisodeV2, env_index: int, **kwargs,
    ):
        """åœ¨æ¯å±€æ¸¸æˆç»“æŸæ—¶è¢«è°ƒç”¨ï¼Œè®°å½•èƒœè´Ÿç»“æœã€‚"""
        # ... (æ­¤éƒ¨åˆ†é€»è¾‘ä¸å˜) ...
        main_policy_agent_id = None
        opponent_agent_id = None
        for agent_id, policy_id in episode.policy_for.items():
            if policy_id == MAIN_POLICY_ID: main_policy_agent_id = agent_id
            else: opponent_agent_id = agent_id
        if main_policy_agent_id and opponent_agent_id:
            winner = episode.last_info_for(main_policy_agent_id).get("winner")
            if winner is not None:
                env = base_env.get_sub_environments()[env_index]
                main_player_id = env._agent_to_player_map[main_policy_agent_id]
                opponent_policy_id = episode.policy_for[opponent_agent_id]
                if opponent_policy_id not in self.win_rates_buffer:
                    self.win_rates_buffer[opponent_policy_id] = []
                if winner == 0: self.win_rates_buffer[opponent_policy_id].append(0.5)
                elif winner == main_player_id: self.win_rates_buffer[opponent_policy_id].append(1.0)
                else: self.win_rates_buffer[opponent_policy_id].append(0.0)

    def on_train_result(self, *, algorithm: Algorithm, result: dict, **kwargs):
        """åœ¨æ¯æ¬¡ `algo.train()` åè¢«è°ƒç”¨ï¼Œæ‰§è¡Œè¯„ä¼°å’Œæ›´æ–°é€»è¾‘ã€‚"""
        # ... (æ­¤éƒ¨åˆ†é€»è¾‘åŸºæœ¬ä¸å˜, ä½†æ·»åŠ äº†æ›´æ–° worker çš„æ­¥éª¤) ...
        total_games_played = sum(len(res) for res in self.win_rates_buffer.values())
        if total_games_played < EVALUATION_GAMES: return

        print("\n--- è¯„ä¼°å‘¨æœŸç»“æŸï¼Œå¼€å§‹å¤„ç†ç»“æœ ---")
        main_policy = algorithm.get_policy(MAIN_POLICY_ID)
        
        # 1. è®¡ç®—å¹³å‡èƒœç‡å¹¶æ›´æ–°Elo
        challenger_wins = 0
        for opponent_id, results in self.win_rates_buffer.items():
            if not results: continue
            wins = sum(1 for r in results if r == 1.0)
            challenger_wins += wins
            avg_win_rate = np.mean(results)
            self.elo_ratings = elo.update_elo(self.elo_ratings, MAIN_POLICY_ID, opponent_id, avg_win_rate)
            print(f"  - vs {opponent_id:<30}: èƒœç‡ = {avg_win_rate:.2%} ({wins}/{len(results)} å±€)")

        # 2. æ£€æŸ¥ä¸»ç­–ç•¥æ˜¯å¦æ»¡è¶³æ™‹çº§æ¡ä»¶
        overall_win_rate = challenger_wins / total_games_played
        if overall_win_rate > EVALUATION_THRESHOLD:
            print(f"\nğŸ† æŒ‘æˆ˜æˆåŠŸ! (æ€»èƒœç‡ {overall_win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
            
            new_opponent_name = f"challenger_{algorithm.iteration}.pt"
            new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)
            torch.save(main_policy.model.state_dict(), new_opponent_path)
            
            new_opponent_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{new_opponent_name.replace('.pt', '')}"
            self.elo_ratings[new_opponent_policy_id] = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
            print(f"  - æ–°å¯¹æ‰‹ {new_opponent_name} å·²å­˜å…¥æ± ä¸­ï¼ŒEloè®¾ç½®ä¸º {self.elo_ratings[new_opponent_policy_id]:.0f}")

            self._manage_opponent_pool(new_opponent_name, algorithm)
            
            # åŠ¨æ€æ·»åŠ æ–°ç­–ç•¥åˆ°è®­ç»ƒå™¨
            print(f"  - åŠ¨æ€æ·»åŠ æ–°ç­–ç•¥ {new_opponent_policy_id} åˆ°è®­ç»ƒå™¨ä¸­...")
            algorithm.add_policy(
                policy_id=new_opponent_policy_id,
                policy_cls=type(main_policy),
                policy_spec=PolicySpec(),
                # åœ¨æ‰€æœ‰ worker ä¸ŠåŒæ­¥æ–°ç­–ç•¥å¹¶åŠ è½½å…¶æƒé‡
                workers=algorithm.workers
            )
            
            # æ‰‹åŠ¨åœ¨æœ¬åœ° worker è®¾ç½®æƒé‡ï¼Œå¹¶å¹¿æ’­åˆ°è¿œç¨‹ workers
            new_policy_local = algorithm.get_policy(new_opponent_policy_id)
            new_policy_local.model.load_state_dict(main_policy.model.state_dict())
            new_policy_local.lock_weights()
            algorithm.workers.sync_weights(policies=[new_opponent_policy_id])

            # åœ¨æ‰€æœ‰ worker ä¸Šæ›´æ–°é‡‡æ ·åˆ†å¸ƒ
            algorithm.workers.foreach_worker(self._update_sampler_on_workers)
        else:
            print(f"\nğŸ›¡ï¸  æŒ‘æˆ˜å¤±è´¥ (æ€»èƒœç‡ {overall_win_rate:.2%} <= {EVALUATION_THRESHOLD:.2%})ã€‚ä¸»ç­–ç•¥å°†ç»§ç»­è®­ç»ƒã€‚")

        self.win_rates_buffer.clear()
        self._save_state()

    def _manage_opponent_pool(self, new_opponent_name: str, algorithm: Algorithm):
        """ã€ä» trainer.py ç§»æ¤ã€‘ç®¡ç†é•¿æœŸå’ŒçŸ­æœŸå¯¹æ‰‹æ± ã€‚"""
        # ... (æ­¤éƒ¨åˆ†é€»è¾‘ä¸å˜) ...
        print("\n--- æ­£åœ¨æ›´æ–°å¯¹æ‰‹æ±  ---")
        self.latest_generation += 1
        self.model_generations[new_opponent_name] = self.latest_generation
        added_to_long_term = False
        long_term_pool_with_gens = sorted([(p, self.model_generations.get(p, 0)) for p in self.long_term_pool_paths], key=lambda x: x[1])
        self.long_term_pool_paths = [p for p, _ in long_term_pool_with_gens]
        long_term_gens = [g for _, g in long_term_pool_with_gens]

        if not self.long_term_pool_paths:
            self.long_term_pool_paths.append(new_opponent_name)
            added_to_long_term = True
        else:
            required_gap = 2 ** self.long_term_power_of_2
            actual_gap = self.latest_generation - long_term_gens[-1]
            if actual_gap >= required_gap:
                if len(self.long_term_pool_paths) >= LONG_TERM_POOL_SIZE:
                    self.long_term_power_of_2 += 1
                    new_required_gap = 2 ** self.long_term_power_of_2
                    retained_pool = [self.long_term_pool_paths[0]]
                    last_kept_gen = long_term_gens[0]
                    for i in range(1, len(long_term_gens)):
                        if (long_term_gens[i] - last_kept_gen) >= new_required_gap:
                            retained_pool.append(self.long_term_pool_paths[i])
                            last_kept_gen = long_term_gens[i]
                    self.long_term_pool_paths = retained_pool
                    new_last_gen = self.model_generations.get(self.long_term_pool_paths[-1], 0)
                    if len(self.long_term_pool_paths) < LONG_TERM_POOL_SIZE and (self.latest_generation - new_last_gen) >= new_required_gap:
                        self.long_term_pool_paths.append(new_opponent_name)
                        added_to_long_term = True
                else:
                    self.long_term_pool_paths.append(new_opponent_name)
                    added_to_long_term = True

        if not added_to_long_term:
            self.short_term_pool_paths.append(new_opponent_name)
            self.short_term_pool_paths.sort(key=lambda p: self.model_generations.get(p, 0), reverse=True)
            if len(self.short_term_pool_paths) > SHORT_TERM_POOL_SIZE:
                self.short_term_pool_paths = self.short_term_pool_paths[:SHORT_TERM_POOL_SIZE]
    
        current_pool_names = set(self.short_term_pool_paths + self.long_term_pool_paths)
        
        policies_on_workers = set(algorithm.workers.local_worker().policy_map.keys())
        policies_to_remove = []
        
        for pid in policies_on_workers:
            if pid.startswith(OPPONENT_POLICY_ID_PREFIX):
                model_name = pid.replace(OPPONENT_POLICY_ID_PREFIX, "") + ".pt"
                if model_name not in current_pool_names:
                    policies_to_remove.append(pid)

        for policy_id_to_remove in policies_to_remove:
            print(f"âœ‚ï¸  æ¸…ç†è¿‡æ—¶å¯¹æ‰‹ç­–ç•¥: {policy_id_to_remove}")
            try:
                algorithm.remove_policy(policy_id_to_remove, workers=algorithm.workers)
                print(f"    - æˆåŠŸä»RLlibä¸­ç§»é™¤ç­–ç•¥: {policy_id_to_remove}")
                model_filename = policy_id_to_remove.replace(OPPONENT_POLICY_ID_PREFIX, "") + ".pt"
                model_path = os.path.join(OPPONENT_POOL_DIR, model_filename)
                if os.path.exists(model_path):
                    os.remove(model_path)
                    print(f"    - æˆåŠŸåˆ é™¤æ¨¡å‹æ–‡ä»¶: {model_filename}")
                self.elo_ratings.pop(policy_id_to_remove, None)
                self.model_generations.pop(model_filename, None)
            except Exception as e:
                print(f"    - è­¦å‘Š: ç§»é™¤ç­–ç•¥ {policy_id_to_remove} æ—¶å‡ºé”™: {e}")

        print("\n--- å¯¹æ‰‹æ± çŠ¶æ€æ›´æ–°å®Œæ¯• ---")
        print(f"çŸ­æœŸæ±  ({len(self.short_term_pool_paths)}/{SHORT_TERM_POOL_SIZE}): {self.short_term_pool_paths}")
        print(f"é•¿æœŸæ±  ({len(self.long_term_pool_paths)}/{LONG_TERM_POOL_SIZE}): {self.long_term_pool_paths}")
        print(f"é•¿æœŸæ± ä»£æ•°å·®å€¼æŒ‡æ•°: {self.long_term_power_of_2} (å½“å‰è¦æ±‚å·®å€¼ >= {2**self.long_term_power_of_2})")