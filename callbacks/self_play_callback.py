# src_code/callbacks/self_play_callback.py

import os
import torch
import numpy as np
import json
from typing import Dict, Any, List
import logging

from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import RolloutWorker
from ray.rllib.evaluation.episode_v2 import EpisodeV2
from ray.rllib.policy import Policy
from ray.rllib.utils.typing import PolicyID
from ray.rllib.algorithms.algorithm import Algorithm
from ray.rllib.core.rl_module.rl_module import RLModuleSpec
# æ·»åŠ æ–°ç‰ˆAPIçš„import
from ray.rllib.env.env_runner import EnvRunner
try:
    from ray.rllib.env.multi_agent_episode import MultiAgentEpisode
except ImportError:
    MultiAgentEpisode = None

from utils.constants import *
from utils import elo

logger = logging.getLogger(__name__)

class SelfPlayCallback(DefaultCallbacks):
    """
    å¤„ç†è‡ªæˆ‘å¯¹å¼ˆä¸­çš„æ¨¡å‹è¯„ä¼°ã€Eloæ›´æ–°å’Œå¯¹æ‰‹æ± ç®¡ç†çš„æ ¸å¿ƒå›è°ƒ (å·²é€‚é…æ–°ç‰ˆ API)ã€‚
    """
    def __init__(self):
        super().__init__()
        self.win_rates_buffer: Dict[str, list] = {}
        self.state_file_path = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        
        self.elo_ratings: Dict[str, float] = {MAIN_POLICY_ID: ELO_DEFAULT}
        self.model_generations: Dict[str, int] = {}
        self.latest_generation: int = 0
        self.long_term_pool_paths: List[str] = []
        self.short_term_pool_paths: List[str] = []
        self.long_term_power_of_2: int = 1

        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        
        self._load_state()

    def _load_state(self):
        """ä»çŠ¶æ€æ–‡ä»¶åŠ è½½ Elo è¯„çº§å’Œæ¨¡å‹ç”Ÿæˆæ•°æ®ã€‚"""
        if os.path.exists(self.state_file_path):
            with open(self.state_file_path, 'r') as f:
                state = json.load(f)
                self.elo_ratings = state.get("elo_ratings", {MAIN_POLICY_ID: ELO_DEFAULT})
                self.model_generations = state.get("model_generations", {})
                self.latest_generation = state.get("latest_generation", 0)
                self.long_term_pool_paths = state.get("long_term_pool_paths", [])
                self.short_term_pool_paths = state.get("short_term_pool_paths", [])
                self.long_term_power_of_2 = state.get("long_term_power_of_2", 1)
                logger.info(f"æˆåŠŸä» {self.state_file_path} åŠ è½½äº† Elo çŠ¶æ€ã€‚")
        else:
            logger.info("æœªæ‰¾åˆ° Elo çŠ¶æ€æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼åˆå§‹åŒ–ã€‚")

    def _save_state(self):
        """ä¿å­˜å½“å‰çš„ Elo è¯„çº§å’Œæ¨¡å‹ç”Ÿæˆæ•°æ®ã€‚"""
        state = {
            "elo_ratings": self.elo_ratings,
            "model_generations": self.model_generations,
            "latest_generation": self.latest_generation,
            "long_term_pool_paths": self.long_term_pool_paths,
            "short_term_pool_paths": self.short_term_pool_paths,
            "long_term_power_of_2": self.long_term_power_of_2,
        }
        with open(self.state_file_path, 'w') as f:
            json.dump(state, f, indent=4)
        logger.info(f"Elo çŠ¶æ€å·²æˆåŠŸä¿å­˜åˆ° {self.state_file_path}ã€‚")

    def _get_opponent_sampling_distribution(self) -> Dict[str, float]:
        """æ ¹æ®Eloå·®å¼‚è®¡ç®—å¯¹æ‰‹çš„é‡‡æ ·æ¦‚ç‡ã€‚"""
        main_elo = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
        
        weights = {}
        # [é²æ£’æ€§ä¿®å¤] å§‹ç»ˆç¡®ä¿ä¸»ç­–ç•¥ï¼ˆè‡ªæˆ‘å¯¹å¼ˆï¼‰åœ¨é‡‡æ ·æ± ä¸­
        weights[MAIN_POLICY_ID] = 1.0 
        
        all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
        for opp_name in all_opponents:
            # ç¡®ä¿æ–‡ä»¶åå’Œç­–ç•¥IDæ­£ç¡®å¯¹åº”
            opp_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{os.path.splitext(opp_name)[0]}"
            opp_elo = self.elo_ratings.get(opp_policy_id, ELO_DEFAULT)
            weight = np.exp(-abs(main_elo - opp_elo) / ELO_WEIGHT_TEMPERATURE)
            weights[opp_policy_id] = weight

        total_weight = sum(weights.values())
        return {k: v / total_weight for k, v in weights.items()} if total_weight > 0 else {MAIN_POLICY_ID: 1.0}

    def _update_sampler_on_workers(self, algorithm: Algorithm):
        """è®¡ç®—æœ€æ–°çš„é‡‡æ ·åˆ†å¸ƒå¹¶å°†å…¶å¹¿æ’­åˆ°æ‰€æœ‰ EnvRunnerã€‚"""
        dist = self._get_opponent_sampling_distribution()
        
        def set_sampler(env_runner):
            # å°†é‡‡æ ·åˆ†å¸ƒé™„åŠ åˆ° env_runner å¯¹è±¡ä¸Šï¼Œä»¥ä¾¿ policy_mapping_fn è®¿é—®
            env_runner.sampler_dist = dist
            # å…¼å®¹æ—§ç‰ˆAPI
            if hasattr(env_runner, 'worker_index'):
                logger.debug(f"EnvRunner {env_runner.worker_index} sampler updated: {dist}")
            else:
                logger.debug(f"EnvRunner sampler updated: {dist}")
        
        # ä½¿ç”¨ foreach_env_runner ç¡®ä¿åœ¨æ‰€æœ‰ rollout worker ä¸Šè®¾ç½®
        algorithm.env_runner_group.foreach_env_runner(set_sampler)
        logger.info("å·²å°†æœ€æ–°çš„å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒå¹¿æ’­åˆ°æ‰€æœ‰ EnvRunnersã€‚")

    def on_algorithm_init(self, *, algorithm: "Algorithm", **kwargs):
        """åœ¨ç®—æ³•åˆå§‹åŒ–æ—¶ï¼ŒåŠ¨æ€æ·»åŠ æ‰€æœ‰ç°æœ‰çš„å¯¹æ‰‹æ¨¡å—ã€‚"""
        logger.info("--- Callback: on_algorithm_init ---")
        
        all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
        main_module = algorithm.get_module(MAIN_POLICY_ID)

        for opp_name in all_opponents:
            module_id = f"{OPPONENT_POLICY_ID_PREFIX}{os.path.splitext(opp_name)[0]}"
            # åªæœ‰å½“æ¨¡å—ä¸å­˜åœ¨æ—¶æ‰æ·»åŠ 
            if not algorithm.get_module(module_id):
                 logger.info(f"åŠ¨æ€æ·»åŠ å†å²å¯¹æ‰‹æ¨¡å—: {module_id}")
                 algorithm.add_module(
                    module_id=module_id,
                    module_spec=RLModuleSpec.from_module(main_module),
                )

        def setup_worker_modules(env_runner):
            # [é²æ£’æ€§ä¿®å¤] ç¡®ä¿ env_runner ä¸Šæœ‰ sampler_dist å±æ€§
            if not hasattr(env_runner, "sampler_dist"):
                env_runner.sampler_dist = {}

            for opp_name in all_opponents:
                module_id = f"{OPPONENT_POLICY_ID_PREFIX}{os.path.splitext(opp_name)[0]}"
                model_path = os.path.join(OPPONENT_POOL_DIR, opp_name)
                
                module = env_runner.module.get(module_id)
                if module and os.path.exists(model_path):
                    try:
                        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸ŠåŠ è½½æ¨¡å‹
                        state_dict = torch.load(model_path, map_location=module.device)
                        module.load_state_dict(state_dict)
                        # å…¼å®¹æ—§ç‰ˆAPI
                        if hasattr(env_runner, 'worker_index'):
                            logger.info(f"EnvRunner {env_runner.worker_index}: æˆåŠŸåŠ è½½æ¨¡å— {module_id}")
                        else:
                            logger.info(f"EnvRunner: æˆåŠŸåŠ è½½æ¨¡å— {module_id}")
                    except Exception as e:
                        # å…¼å®¹æ—§ç‰ˆAPI
                        if hasattr(env_runner, 'worker_index'):
                            logger.error(f"EnvRunner {env_runner.worker_index}: åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {e}")
                        else:
                            logger.error(f"EnvRunner: åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {e}")

        algorithm.env_runner_group.foreach_env_runner(setup_worker_modules)
        self._update_sampler_on_workers(algorithm)

    def on_episode_end(
        self, *, 
        episode, 
        env_index: int,
        worker: "RolloutWorker" = None,
        base_env: BaseEnv = None, 
        env_runner: "RolloutWorker" = None,
        **kwargs,
    ):
        """åœ¨æ–°ç‰ˆ API ä¸­ï¼Œå¤„ç†ä¸åŒç±»å‹çš„episodeå¯¹è±¡ã€‚"""
        # å…¼å®¹æ€§å¤„ç†ï¼šä½¿ç”¨env_runnerå¦‚æœworkerä¸ºNone
        if worker is None and env_runner is not None:
            worker = env_runner
            
        # å¦‚æœæ²¡æœ‰workerï¼Œæ— æ³•è·å–ç¯å¢ƒä¿¡æ¯ï¼Œç›´æ¥è¿”å›
        if worker is None:
            return
            
        main_agent_id, opponent_agent_id, opponent_module_id = None, None, None

        # å¤„ç†ä¸åŒç±»å‹çš„episodeå¯¹è±¡
        if hasattr(episode, 'agent_for_module_map'):
            # æ—§ç‰ˆEpisodeV2å¯¹è±¡
            agent_module_map = episode.agent_for_module_map
        elif hasattr(episode, 'agent_ids') and hasattr(episode, 'module_for'):
            # æ–°ç‰ˆMultiAgentEpisodeå¯¹è±¡
            agent_module_map = {}
            for agent_id in episode.agent_ids:
                try:
                    module_id = episode.module_for(agent_id)
                    agent_module_map[agent_id] = module_id
                except:
                    # å¦‚æœæ— æ³•è·å–module_idï¼Œè·³è¿‡è¿™ä¸ªagent
                    continue
        else:
            # æ— æ³•å¤„ç†çš„episodeç±»å‹
            return

        # ç¡®å®šå“ªä¸ª agent æ˜¯ main_policy
        for agent_id, module_id in agent_module_map.items():
            if module_id == MAIN_POLICY_ID:
                main_agent_id = agent_id
                break
        
        # å¦‚æœæ²¡æœ‰ main_policy å‚ä¸ï¼ˆä¸å¤ªå¯èƒ½å‘ç”Ÿï¼‰ï¼Œåˆ™é€€å‡º
        if main_agent_id is None:
            return

        # æ‰¾åˆ°å¯¹æ‰‹ agent å’Œ module
        for agent_id, module_id in agent_module_map.items():
            if agent_id != main_agent_id:
                opponent_agent_id = agent_id
                opponent_module_id = module_id
                break
        
        if not all([opponent_agent_id, opponent_module_id]): 
            return

        # è·å–æ¸¸æˆç»“æœä¿¡æ¯ï¼Œå…¼å®¹ä¸åŒçš„episodeç±»å‹
        winner = None
        if hasattr(episode, 'last_info_for'):
            # EpisodeV2ç±»å‹
            last_info = episode.last_info_for(main_agent_id) or {}
            winner = last_info.get("winner")
        elif hasattr(episode, 'get_infos'):
            # MultiAgentEpisodeç±»å‹
            try:
                infos = episode.get_infos(agent_ids=[main_agent_id])
                if infos and len(infos) > 0:
                    # è·å–æœ€åä¸€ä¸ªinfo
                    last_info = infos[-1].get(main_agent_id, {})
                    winner = last_info.get("winner")
            except:
                pass
        
        if winner is None: 
            return

        # è·å–ç¯å¢ƒä¿¡æ¯
        if base_env is None:
            return
            
        env = base_env.get_sub_environments()[env_index]
        main_player_num = env._agent_to_player_map.get(main_agent_id)
        if main_player_num is None: 
            return

        if opponent_module_id not in self.win_rates_buffer:
            self.win_rates_buffer[opponent_module_id] = []
        
        if winner == 0: 
            self.win_rates_buffer[opponent_module_id].append(0.5)
        elif winner == main_player_num: 
            self.win_rates_buffer[opponent_module_id].append(1.0)
        else: 
            self.win_rates_buffer[opponent_module_id].append(0.0)

    def on_train_result(self, *, algorithm: Algorithm, result: dict, **kwargs):
        """è¯„ä¼°å’Œæ›´æ–°é€»è¾‘ï¼Œé€‚é…æ–° APIã€‚"""
        total_games_played = sum(len(res) for res in self.win_rates_buffer.values())
        if total_games_played < EVALUATION_GAMES:
            return

        logger.info("\n--- è¯„ä¼°å‘¨æœŸç»“æŸï¼Œå¼€å§‹å¤„ç†ç»“æœ ---")
        main_module = algorithm.get_module(MAIN_POLICY_ID)
        
        all_results = []
        for opponent, results in self.win_rates_buffer.items():
            win_rate = np.mean(results)
            logger.info(f"  - vs {opponent}: Win Rate = {win_rate:.2f} ({len(results)} games)")
            
            main_elo = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
            opponent_elo = self.elo_ratings.get(opponent, ELO_DEFAULT)
            
            # ä½¿ç”¨æ­£ç¡®çš„ Elo æ›´æ–°å‡½æ•°
            updated_elos = elo.update_elo(self.elo_ratings, MAIN_POLICY_ID, opponent, win_rate)
            self.elo_ratings.update(updated_elos)
            
            logger.info(f"    Elo: {main_elo:.0f} -> {self.elo_ratings[MAIN_POLICY_ID]:.0f} | Opponent Elo: {opponent_elo:.0f} -> {self.elo_ratings[opponent]:.0f}")
            all_results.extend(results)

        overall_win_rate = np.mean(all_results) if all_results else 0.0
        result["overall_win_rate"] = overall_win_rate
        logger.info(f"\n  ç»¼åˆèƒœç‡: {overall_win_rate:.2f}")

        if overall_win_rate > EVALUATION_THRESHOLD:
            logger.info(f"ğŸ† æŒ‘æˆ˜æˆåŠŸ! æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
            self.latest_generation += 1
            
            new_opponent_name = f"challenger_{self.latest_generation}.pt"
            new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)
            torch.save(main_module.state_dict(), new_opponent_path)
            
            new_opponent_module_id = f"{OPPONENT_POLICY_ID_PREFIX}{os.path.splitext(new_opponent_name)[0]}"
            self.elo_ratings[new_opponent_module_id] = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
            self.model_generations[new_opponent_module_id] = self.latest_generation

            logger.info(f"  - åŠ¨æ€æ·»åŠ æ–°æ¨¡å— {new_opponent_module_id} åˆ°è®­ç»ƒå™¨ä¸­...")
            algorithm.add_module(
                module_id=new_opponent_module_id,
                module_spec=RLModuleSpec.from_module(main_module),
            )
            
            # ç¡®ä¿æ–°æ·»åŠ çš„æ¨¡å—æ˜¯ä¸å¯è®­ç»ƒçš„
            def set_module_trainable(learner):
                if new_opponent_module_id in learner.module:
                    learner.remove_module_to_update(new_opponent_module_id)

            algorithm.learner_group.foreach_learner(set_module_trainable)
            
            self._manage_opponent_pool(new_opponent_name, algorithm)
            self._update_sampler_on_workers(algorithm)
        else:
            logger.info(f"ğŸ›¡ï¸ æŒ‘æˆ˜å¤±è´¥ã€‚ä¸»ç­–ç•¥å°†ç»§ç»­è®­ç»ƒã€‚")

        self.win_rates_buffer.clear()
        self._save_state()

    def _manage_opponent_pool(self, new_opponent_name: str, algorithm: Algorithm):
        """ç®¡ç†çŸ­æœŸå’Œé•¿æœŸå¯¹æ‰‹æ± ã€‚"""
        self.short_term_pool_paths.append(new_opponent_name)
        if len(self.short_term_pool_paths) > SHORT_TERM_POOL_MAX_SIZE:
            to_remove_name = self.short_term_pool_paths.pop(0)
            module_id_to_remove = f"{OPPONENT_POLICY_ID_PREFIX}{os.path.splitext(to_remove_name)[0]}"
            self._remove_module_from_algorithm(algorithm, module_id_to_remove, to_remove_name)

        if self.latest_generation >= self.long_term_power_of_2:
            self.long_term_pool_paths.append(new_opponent_name)
            self.long_term_power_of_2 *= 2
            logger.info(f"  - æ–°æ¨¡å‹è¢«æå‡åˆ°é•¿æœŸå¯¹æ‰‹æ± ã€‚ä¸‹ä¸€ä¸ªæå‡ç‚¹: ç¬¬ {self.long_term_power_of_2} ä»£ã€‚")

        if len(self.long_term_pool_paths) > LONG_TERM_POOL_MAX_SIZE:
            to_remove_name = self.long_term_pool_paths.pop(0)
            module_id_to_remove = f"{OPPONENT_POLICY_ID_PREFIX}{os.path.splitext(to_remove_name)[0]}"
            # ç¡®ä¿ä¸ä¼šæŠŠè‡ªå·±ä»çŸ­æœŸæ± ä¸­ç§»é™¤ï¼ˆå¦‚æœå®ƒåŒæ—¶åœ¨é•¿æœŸæ± ä¸­ï¼‰
            if to_remove_name not in self.short_term_pool_paths:
                 self._remove_module_from_algorithm(algorithm, module_id_to_remove, to_remove_name)

    def _remove_module_from_algorithm(self, algorithm: Algorithm, module_id_to_remove: str, model_filename: str):
        """ä»ç®—æ³•ä¸­ç§»é™¤æ¨¡å—å¹¶åˆ é™¤ç›¸å…³æ–‡ä»¶ã€‚"""
        logger.info(f"âœ‚ï¸  æ¸…ç†è¿‡æ—¶å¯¹æ‰‹æ¨¡å—: {module_id_to_remove}")
        try:
            if algorithm.get_module(module_id_to_remove):
                algorithm.remove_module(module_id_to_remove)
                logger.info(f"    - æˆåŠŸä» RLlib è®­ç»ƒå™¨ä¸­ç§»é™¤æ¨¡å—: {module_id_to_remove}")
            
            model_path = os.path.join(OPPONENT_POOL_DIR, model_filename)
            if os.path.exists(model_path):
                os.remove(model_path)
                logger.info(f"    - æˆåŠŸåˆ é™¤æ¨¡å‹æ–‡ä»¶: {model_path}")

            if module_id_to_remove in self.elo_ratings:
                del self.elo_ratings[module_id_to_remove]
            if module_id_to_remove in self.model_generations:
                del self.model_generations[module_id_to_remove]
        except Exception as e:
            logger.warning(f"    - ç§»é™¤æ¨¡å— {module_id_to_remove} æ—¶å‡ºé”™: {e}")