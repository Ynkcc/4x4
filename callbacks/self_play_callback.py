# rllib_version_complete/callbacks/self_play_callback.py

import os
import torch
import numpy as np
from typing import Dict

from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import RolloutWorker
from ray.rllib.evaluation.episode_v2 import EpisodeV2
from ray.rllib.policy import Policy
from ray.rllib.policy.policy import PolicySpec
from ray.rllib.utils.typing import PolicyID

from utils.constants import *
from utils import elo

class SelfPlayCallback(DefaultCallbacks):
    """
    å¤„ç†è‡ªæˆ‘å¯¹å¼ˆä¸­çš„æ¨¡å‹è¯„ä¼°ã€Eloæ›´æ–°å’Œå¯¹æ‰‹æ± ç®¡ç†çš„æ ¸å¿ƒå›è°ƒã€‚
    """
    def __init__(self):
        super().__init__()
        # å­˜å‚¨ä¸»ç­–ç•¥ä¸æ¯ä¸ªå¯¹æ‰‹çš„èƒœè´Ÿå…³ç³»: {opponent_id: [win, loss, draw, ...]}
        self.win_rates_buffer: Dict[str, list] = {}
        self.elo_ratings = elo.load_elo_ratings()

    def on_episode_end(
        self,
        *,
        worker: "RolloutWorker",
        base_env: BaseEnv,
        policies: Dict[PolicyID, Policy],
        episode: EpisodeV2,
        env_index: int,
        **kwargs,
    ):
        """åœ¨æ¯å±€æ¸¸æˆç»“æŸæ—¶è¢«è°ƒç”¨ï¼Œè®°å½•èƒœè´Ÿç»“æœã€‚"""
        # ç¡®å®šä¸»ç­–ç•¥å’Œå¯¹æ‰‹ç­–ç•¥åˆ†åˆ«æ§åˆ¶å“ªä¸ªagent
        main_policy_agent_id = None
        opponent_agent_id = None
        
        for agent_id, policy_id in episode.policy_for.items():
            if policy_id == MAIN_POLICY_ID:
                main_policy_agent_id = agent_id
            else:
                opponent_agent_id = agent_id
        
        # ç¡®ä¿è¿™æ˜¯ä¸€åœºä¸»ç­–ç•¥ vs å…¶ä»–ç­–ç•¥çš„å¯¹å±€
        if main_policy_agent_id and opponent_agent_id:
            winner = episode.last_info_for(main_policy_agent_id).get("winner")
            if winner is not None:
                # è·å–ç¯å¢ƒå®ä¾‹ä»¥æŸ¥è¯¢agentä¸playerçš„æ˜ å°„å…³ç³»
                env = base_env.get_sub_environments()[env_index]
                main_player_id = env._agent_to_player_map[main_policy_agent_id]
                
                opponent_policy_id = episode.policy_for[opponent_agent_id]

                if opponent_policy_id not in self.win_rates_buffer:
                    self.win_rates_buffer[opponent_policy_id] = []

                # è®°å½•ç»“æœï¼š1.0ä»£è¡¨èƒœåˆ©, 0.5ä»£è¡¨å¹³å±€, 0.0ä»£è¡¨å¤±è´¥
                if winner == 0:
                    self.win_rates_buffer[opponent_policy_id].append(0.5)
                elif winner == main_player_id:
                    self.win_rates_buffer[opponent_policy_id].append(1.0)
                else:
                    self.win_rates_buffer[opponent_policy_id].append(0.0)

    def on_train_result(self, *, algorithm, result: dict, **kwargs):
        """åœ¨æ¯æ¬¡ `algo.train()` åè¢«è°ƒç”¨ï¼Œæ‰§è¡Œè¯„ä¼°å’Œæ›´æ–°é€»è¾‘ã€‚"""
        main_policy = algorithm.get_policy(MAIN_POLICY_ID)
        
        # --- 1. è®¡ç®—å¹³å‡èƒœç‡å¹¶æ›´æ–°Elo ---
        total_games_played = 0
        challenger_wins = 0

        print("\n--- è¯„ä¼°ä¸Eloæ›´æ–° ---")
        for opponent_id, results in self.win_rates_buffer.items():
            if not results:
                continue
            
            num_games = len(results)
            total_games_played += num_games
            wins = sum(1 for r in results if r == 1.0)
            challenger_wins += wins
            
            avg_win_rate = np.mean(results)
            self.elo_ratings = elo.update_elo(
                self.elo_ratings, MAIN_POLICY_ID, opponent_id, avg_win_rate
            )
            print(f"  - vs {opponent_id:<30}: "
                  f"èƒœç‡ = {avg_win_rate:.2%} ({wins}/{num_games} å±€)")

        # --- 2. æ£€æŸ¥ä¸»ç­–ç•¥æ˜¯å¦æ»¡è¶³æ™‹çº§æ¡ä»¶ ---
        if total_games_played >= EVALUATION_GAMES:
            overall_win_rate = challenger_wins / total_games_played if total_games_played > 0 else 0.0
            
            if overall_win_rate > EVALUATION_THRESHOLD:
                print(f"\nğŸ† æŒ‘æˆ˜æˆåŠŸ! (æ€»èƒœç‡ {overall_win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
                
                # a. å°†å½“å‰ä¸»ç­–ç•¥æ¨¡å‹å­˜å…¥å¯¹æ‰‹æ± 
                # æ³¨æ„ï¼šä¿å­˜çš„æ˜¯state_dictï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹
                new_opponent_name = f"challenger_{algorithm.iteration}.pt"
                new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)
                torch.save(main_policy.model.state_dict(), new_opponent_path)

                # b. ä¸ºæ–°å¯¹æ‰‹è®¾ç½®Elo
                new_opponent_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{new_opponent_name.replace('.pt', '')}"
                self.elo_ratings[new_opponent_policy_id] = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
                print(f"  - æ–°å¯¹æ‰‹ {new_opponent_name} å·²å­˜å…¥æ± ä¸­ï¼ŒEloè®¾ç½®ä¸º {self.elo_ratings[new_opponent_policy_id]:.0f}")

                # c. åŠ¨æ€åœ°å°†æ–°ç­–ç•¥æ·»åŠ åˆ°æ­£åœ¨è¿è¡Œçš„ç®—æ³•ä¸­
                # è¿™å…è®¸åœ¨ä¸é‡å¯è®­ç»ƒçš„æƒ…å†µä¸‹å¼•å…¥æ–°å¯¹æ‰‹
                new_policy_spec = PolicySpec()
                algorithm.add_policy(
                    policy_id=new_opponent_policy_id,
                    policy_cls=type(main_policy),
                    policy_spec=new_policy_spec,
                )
                
                # d. ä¸ºæ–°æ·»åŠ çš„ç­–ç•¥åŠ è½½æƒé‡å¹¶é”å®š
                new_policy = algorithm.get_policy(new_opponent_policy_id)
                new_policy.model.load_state_dict(main_policy.model.state_dict())
                new_policy.lock_weights() # ç¡®ä¿æ–°åŠ å…¥çš„å¯¹æ‰‹ä¸è¢«è®­ç»ƒ
                
                print(f"  - æ–°ç­–ç•¥ {new_opponent_policy_id} å·²è¢«åŠ¨æ€æ·»åŠ åˆ°è®­ç»ƒå™¨ä¸­ã€‚")

                # e. æ¸…ç©ºèƒœç‡è®°å½•å™¨ï¼Œä»¥ä¾¿ä¸‹ä¸€è½®è¯„ä¼°
                self.win_rates_buffer.clear()
        
        # --- 3. ä¿å­˜æ›´æ–°åçš„Eloè¯„åˆ† ---
        elo.save_elo_ratings(self.elo_ratings)
