# rllib_version_complete/callbacks/self_play_callback.py

import os
import torch
import numpy as np
import json
from typing import Dict, Any, List
import random

from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.env import BaseEnv
from ray.rllib.evaluation import RolloutWorker
from ray.rllib.evaluation.episode_v2 import EpisodeV2
from ray.rllib.policy import Policy
from ray.rllib.policy.policy import PolicySpec
from ray.rllib.utils.typing import PolicyID
from ray.rllib.algorithms.algorithm import Algorithm

from utils.constants import *
from utils import elo

class SelfPlayCallback(DefaultCallbacks):
    """
    å¤„ç†è‡ªæˆ‘å¯¹å¼ˆä¸­çš„æ¨¡å‹è¯„ä¼°ã€Eloæ›´æ–°å’Œå¯¹æ‰‹æ± ç®¡ç†çš„æ ¸å¿ƒå›è°ƒ (é‡æ„ç‰ˆ)ã€‚
    èŒè´£:
    - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è‡ªæˆ‘å¯¹å¼ˆçŠ¶æ€ (Elo, æ¨¡å‹æ± , ä»£æ•°)ã€‚
    - åœ¨å¯åŠ¨æ—¶åŠ¨æ€åŠ è½½å¯¹æ‰‹ç­–ç•¥ã€‚
    - åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–° Elo å’Œæ¨¡å‹æ± ã€‚
    - åŠ¨æ€æ›´æ–°å¹¶å¹¿æ’­å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒã€‚
    """
    def __init__(self):
        super().__init__()
        self.win_rates_buffer: Dict[str, list] = {}
        self.elo_ratings: Dict[str, float] = {}
        self.model_generations: Dict[str, int] = {}
        self.latest_generation: int = 0
        self.long_term_pool_paths: List[str] = []
        self.short_term_pool_paths: List[str] = []
        self.long_term_power_of_2: int = 1
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(OPPONENT_POOL_DIR, exist_ok=True)
        os.makedirs(SELF_PLAY_OUTPUT_DIR, exist_ok=True)
        self._load_state()

    # --- çŠ¶æ€ç®¡ç† ---
    def _load_state(self):
        """ä»JSONæ–‡ä»¶åŠ è½½Eloã€æ¨¡å‹ä»£æ•°å’Œæ¨¡å‹æ± çŠ¶æ€ã€‚"""
        state_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        if os.path.exists(state_file):
            try:
                with open(state_file, 'r') as f:
                    data = json.load(f)
                    self.elo_ratings = data.get("elo", {MAIN_POLICY_ID: ELO_DEFAULT})
                    self.model_generations = data.get("generations", {})
                    self.latest_generation = data.get("latest_generation", 0)
                    self.long_term_pool_paths = data.get("long_term_pool_paths", [])
                    self.short_term_pool_paths = data.get("short_term_pool_paths", [])
                    self.long_term_power_of_2 = data.get("long_term_power_of_2", 1)
            except (json.JSONDecodeError, IOError, KeyError) as e:
                print(f"è­¦å‘Šï¼šè¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥æˆ–æ ¼å¼ä¸å®Œæ•´: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
        else:
             self.elo_ratings = {MAIN_POLICY_ID: ELO_DEFAULT}
             print("æœªæ‰¾åˆ°çŠ¶æ€æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼åˆå§‹åŒ–ã€‚")

    def _save_state(self):
        """å°†Eloã€æ¨¡å‹ä»£æ•°å’Œæ¨¡å‹æ± çŠ¶æ€ä¿å­˜åˆ°åŒä¸€ä¸ªJSONæ–‡ä»¶ã€‚"""
        state_file = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
        data = {
            "elo": self.elo_ratings,
            "generations": self.model_generations,
            "latest_generation": self.latest_generation,
            "long_term_pool_paths": self.long_term_pool_paths,
            "short_term_pool_paths": self.short_term_pool_paths,
            "long_term_power_of_2": self.long_term_power_of_2,
        }
        try:
            with open(state_file, 'w') as f:
                json.dump(data, f, indent=4)
        except IOError as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜çŠ¶æ€æ–‡ä»¶: {e}")

    # --- å¯¹æ‰‹é‡‡æ ·åˆ†å¸ƒ ---
    def _get_opponent_sampling_distribution(self) -> Dict[str, float]:
        """æ ¹æ®Eloå·®å¼‚è®¡ç®—å¯¹æ‰‹çš„é‡‡æ ·æ¦‚ç‡ã€‚"""
        main_elo = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
        
        weights = {}
        # 1. åŒ…æ‹¬ä¸»ç­–ç•¥è‡ªèº« (ç”¨äºè‡ªæˆ‘å¯¹å¼ˆ)
        weights[MAIN_POLICY_ID] = 1.0 
        
        # 2. åŒ…æ‹¬æ‰€æœ‰æ± ä¸­å¯¹æ‰‹
        all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
        for opp_name in all_opponents:
            opp_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{opp_name.replace('.pt', '')}"
            opp_elo = self.elo_ratings.get(opp_policy_id, ELO_DEFAULT)
            # ä½¿ç”¨æ¸©åº¦å‚æ•°è°ƒæ•´é‡‡æ ·æƒé‡
            weight = np.exp(-abs(main_elo - opp_elo) / ELO_WEIGHT_TEMPERATURE)
            weights[opp_policy_id] = weight

        total_weight = sum(weights.values())
        if total_weight == 0:
             return {MAIN_POLICY_ID: 1.0}
        return {k: v / total_weight for k, v in weights.items()}
    
    def _update_sampler_on_workers(self, worker: RolloutWorker):
        """è®¡ç®—å¹¶æ›´æ–°æ‰€æœ‰ worker çš„é‡‡æ ·åˆ†å¸ƒã€‚"""
        dist = self._get_opponent_sampling_distribution()
        # å°†åˆ†å¸ƒæ³¨å…¥åˆ°æ¯ä¸ª workerï¼Œä¾› policy_mapping_fn ä½¿ç”¨
        # setattr(worker, 'sampler_dist', dist)
        worker.sampler_dist = dist

    # --- RLlib å›è°ƒé’©å­ ---
    def on_algorithm_init(self, *, algorithm: "Algorithm", **kwargs):
        """åœ¨ç®—æ³•åˆå§‹åŒ–æ—¶ï¼ŒåŠ¨æ€æ·»åŠ æ‰€æœ‰ç°æœ‰çš„å¯¹æ‰‹ç­–ç•¥ã€‚"""
        print("--- Callback: on_algorithm_init ---")
        
        def setup_worker(worker: RolloutWorker):
            """åœ¨æ¯ä¸ª worker ä¸Šæ‰§è¡Œçš„è®¾ç½®å‡½æ•°"""
            all_opponents = self.long_term_pool_paths + self.short_term_pool_paths
            main_policy = worker.get_policy(MAIN_POLICY_ID)
            
            for opp_name in all_opponents:
                policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{opp_name.replace('.pt', '')}"
                model_path = os.path.join(OPPONENT_POOL_DIR, opp_name)

                if policy_id not in worker.policy_map:
                    print(f"  - Worker {worker.worker_index}: åŠ¨æ€æ·»åŠ å†å²å¯¹æ‰‹ç­–ç•¥: {policy_id}")
                    worker.add_policy(
                        policy_id=policy_id,
                        policy_cls=type(main_policy),
                        policy_spec=PolicySpec(),
                    )

                if os.path.exists(model_path):
                    policy = worker.get_policy(policy_id)
                    if policy:
                        try:
                            state_dict = torch.load(model_path, map_location=policy.device)
                            policy.model.load_state_dict(state_dict)
                            policy.lock_weights() # ç¡®ä¿å¯¹æ‰‹ç­–ç•¥ä¸è¢«è®­ç»ƒ
                        except Exception as e:
                            print(f"  - Worker {worker.worker_index}: åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {e}")

            # æ›´æ–°è¯¥ worker çš„é‡‡æ ·åˆ†å¸ƒ
            self._update_sampler_on_workers(worker)

        # åœ¨æ‰€æœ‰ worker ä¸Šæ‰§è¡Œè®¾ç½®
        algorithm.workers.foreach_worker(setup_worker)


    def on_episode_end(
        self, *, worker: "RolloutWorker", base_env: BaseEnv, policies: Dict[PolicyID, Policy],
        episode: EpisodeV2, env_index: int, **kwargs,
    ):
        """åœ¨æ¯å±€æ¸¸æˆç»“æŸæ—¶è¢«è°ƒç”¨ï¼Œè®°å½•èƒœè´Ÿç»“æœã€‚"""
        # ç¡®å®šä¸»ç­–ç•¥å’Œå¯¹æ‰‹ç­–ç•¥çš„ agent_id
        main_agent_id, opponent_agent_id = None, None
        opponent_policy_id = None

        for agent_id, policy_id in episode.policy_for.items():
            if policy_id == MAIN_POLICY_ID:
                main_agent_id = agent_id
            else:
                opponent_agent_id = agent_id
                opponent_policy_id = policy_id
        
        # ç¡®ä¿åŒæ–¹éƒ½å‚ä¸äº†æ¸¸æˆ
        if not all([main_agent_id, opponent_agent_id, opponent_policy_id]):
            return

        # ä» info å­—å…¸ä¸­è·å–èƒœåˆ©è€…ä¿¡æ¯
        # RLLibMultiAgentEnv ä¼šä¸ºä¸¤ä¸ª agent éƒ½æä¾› info
        last_info = episode.last_info_for(main_agent_id) or episode.last_info_for(opponent_agent_id)
        if not last_info:
            return
            
        winner = last_info.get("winner")
        if winner is None:
            return

        # è·å–ç¯å¢ƒå®ä¾‹ä»¥æ˜ å°„ agent_id åˆ° player number (1 or -1)
        env = base_env.get_sub_environments()[env_index]
        main_player_num = env._agent_to_player_map.get(main_agent_id)

        if main_player_num is None:
            return

        # è®°å½•èƒœç‡
        if opponent_policy_id not in self.win_rates_buffer:
            self.win_rates_buffer[opponent_policy_id] = []
        
        if winner == 0: # å¹³å±€
            self.win_rates_buffer[opponent_policy_id].append(0.5)
        elif winner == main_player_num: # ä¸»ç­–ç•¥è·èƒœ
            self.win_rates_buffer[opponent_policy_id].append(1.0)
        else: # ä¸»ç­–ç•¥å¤±è´¥
            self.win_rates_buffer[opponent_policy_id].append(0.0)


    def on_train_result(self, *, algorithm: Algorithm, result: dict, **kwargs):
        """åœ¨æ¯æ¬¡ `algo.train()` åè¢«è°ƒç”¨ï¼Œæ‰§è¡Œè¯„ä¼°å’Œæ›´æ–°é€»è¾‘ã€‚"""
        total_games_played = sum(len(res) for res in self.win_rates_buffer.values())
        if total_games_played < EVALUATION_GAMES:
            return

        print("\n--- è¯„ä¼°å‘¨æœŸç»“æŸï¼Œå¼€å§‹å¤„ç†ç»“æœ ---")
        main_policy = algorithm.get_policy(MAIN_POLICY_ID)
        
        # 1. è®¡ç®—å¹³å‡èƒœç‡å¹¶æ›´æ–°Elo
        challenger_total_wins = 0
        challenger_total_games = 0
        for opponent_id, results in self.win_rates_buffer.items():
            if not results: continue
            
            wins = sum(r for r in results if r == 1.0)
            num_games = len(results)
            challenger_total_wins += wins
            challenger_total_games += num_games
            
            avg_win_rate = np.mean(results)
            self.elo_ratings = elo.update_elo(self.elo_ratings, MAIN_POLICY_ID, opponent_id, avg_win_rate)
            print(f"  - vs {opponent_id:<30}: èƒœç‡ = {avg_win_rate:.2%} ({int(wins)}/{num_games} å±€)")

        # 2. æ£€æŸ¥ä¸»ç­–ç•¥æ˜¯å¦æ»¡è¶³æ™‹çº§æ¡ä»¶
        if challenger_total_games == 0:
             overall_win_rate = 0
        else:
             overall_win_rate = challenger_total_wins / challenger_total_games

        if overall_win_rate > EVALUATION_THRESHOLD:
            print(f"\nğŸ† æŒ‘æˆ˜æˆåŠŸ! (æ€»èƒœç‡ {overall_win_rate:.2%} > {EVALUATION_THRESHOLD:.2%})ï¼æ–°ä¸»å®°è€…è¯ç”Ÿï¼")
            
            new_opponent_name = f"challenger_{algorithm.iteration}.pt"
            new_opponent_path = os.path.join(OPPONENT_POOL_DIR, new_opponent_name)
            torch.save(main_policy.model.state_dict(), new_opponent_path)
            
            new_opponent_policy_id = f"{OPPONENT_POLICY_ID_PREFIX}{new_opponent_name.replace('.pt', '')}"
            self.elo_ratings[new_opponent_policy_id] = self.elo_ratings.get(MAIN_POLICY_ID, ELO_DEFAULT)
            print(f"  - æ–°å¯¹æ‰‹ {new_opponent_name} å·²å­˜å…¥æ± ä¸­ï¼ŒEloè®¾ç½®ä¸º {self.elo_ratings[new_opponent_policy_id]:.0f}")

            self._manage_opponent_pool(new_opponent_name, algorithm)
            
            # åŠ¨æ€æ·»åŠ æ–°ç­–ç•¥åˆ°è®­ç»ƒå™¨
            print(f"  - åŠ¨æ€æ·»åŠ æ–°ç­–ç•¥ {new_opponent_policy_id} åˆ°è®­ç»ƒå™¨ä¸­...")
            algorithm.add_policy(
                policy_id=new_opponent_policy_id,
                policy_cls=type(main_policy),
                # ä»ä¸»ç­–ç•¥å…‹éš†æƒé‡
                weights=main_policy.get_weights(),
                policy_state=main_policy.get_state(),
            )
            
            # é”å®šæ–°å¯¹æ‰‹ç­–ç•¥çš„æƒé‡å¹¶æ›´æ–°é‡‡æ ·åˆ†å¸ƒ
            def setup_new_opponent(worker: RolloutWorker):
                if worker.get_policy(new_opponent_policy_id):
                    worker.get_policy(new_opponent_policy_id).lock_weights()
                self._update_sampler_on_workers(worker)

            algorithm.workers.foreach_worker(setup_new_opponent)
        else:
            print(f"\nğŸ›¡ï¸  æŒ‘æˆ˜å¤±è´¥ (æ€»èƒœç‡ {overall_win_rate:.2%} <= {EVALUATION_THRESHOLD:.2%})ã€‚ä¸»ç­–ç•¥å°†ç»§ç»­è®­ç»ƒã€‚")

        self.win_rates_buffer.clear()
        self._save_state()

    def _manage_opponent_pool(self, new_opponent_name: str, algorithm: Algorithm):
        """ç®¡ç†é•¿æœŸå’ŒçŸ­æœŸå¯¹æ‰‹æ± ã€‚"""
        print("\n--- æ­£åœ¨æ›´æ–°å¯¹æ‰‹æ±  ---")
        self.latest_generation += 1
        self.model_generations[new_opponent_name] = self.latest_generation
        added_to_long_term = False

        # --- æ›´æ–°é•¿æœŸæ±  ---
        long_term_pool_with_gens = sorted([(p, self.model_generations.get(p, 0)) for p in self.long_term_pool_paths], key=lambda x: x[1])
        self.long_term_pool_paths = [p for p, _ in long_term_pool_with_gens]
        long_term_gens = [g for _, g in long_term_pool_with_gens]

        if not self.long_term_pool_paths:
            self.long_term_pool_paths.append(new_opponent_name)
            added_to_long_term = True
        else:
            required_gap = 2 ** self.long_term_power_of_2
            actual_gap = self.latest_generation - long_term_gens[-1]
            if actual_gap >= required_gap:
                if len(self.long_term_pool_paths) >= LONG_TERM_POOL_SIZE:
                    # æ± å·²æ»¡ï¼Œéœ€è¦æ ¹æ®æ–°çš„ä»£æ•°å·®è·è¦æ±‚è¿›è¡Œç­›é€‰
                    self.long_term_power_of_2 += 1
                    new_required_gap = 2 ** self.long_term_power_of_2
                    retained_pool = [self.long_term_pool_paths[0]]
                    last_kept_gen = long_term_gens[0]
                    for i in range(1, len(long_term_gens)):
                        if (long_term_gens[i] - last_kept_gen) >= new_required_gap:
                            retained_pool.append(self.long_term_pool_paths[i])
                            last_kept_gen = long_term_gens[i]
                    self.long_term_pool_paths = retained_pool

                # åœ¨ç­›é€‰åæ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç©ºé—´
                if len(self.long_term_pool_paths) < LONG_TERM_POOL_SIZE:
                    self.long_term_pool_paths.append(new_opponent_name)
                    added_to_long_term = True
            
        # --- æ›´æ–°çŸ­æœŸæ±  ---
        if not added_to_long_term:
            self.short_term_pool_paths.append(new_opponent_name)
            self.short_term_pool_paths.sort(key=lambda p: self.model_generations.get(p, 0), reverse=True)
            if len(self.short_term_pool_paths) > SHORT_TERM_POOL_SIZE:
                # ç§»é™¤æœ€æ—§çš„æ¨¡å‹
                removed_model_name = self.short_term_pool_paths.pop()
                policy_id_to_remove = f"{OPPONENT_POLICY_ID_PREFIX}{removed_model_name.replace('.pt', '')}"
                self._remove_policy_and_files(algorithm, policy_id_to_remove, removed_model_name)
        
        # --- æ¸…ç†ä¸å†ä½¿ç”¨çš„ç­–ç•¥ ---
        current_pool_names = set(self.short_term_pool_paths + self.long_term_pool_paths)
        
        # è·å–ä¸€ä¸ª worker ä¸Šçš„ç­–ç•¥åˆ—è¡¨ä½œä¸ºå‚è€ƒ
        policies_on_workers = set(algorithm.workers.local_worker().policy_map.keys())
        
        for pid in policies_on_workers:
            if pid.startswith(OPPONENT_POLICY_ID_PREFIX):
                model_name = pid.replace(OPPONENT_POLICY_ID_PREFIX, "") + ".pt"
                if model_name not in current_pool_names:
                    self._remove_policy_and_files(algorithm, pid, model_name)

        print("\n--- å¯¹æ‰‹æ± çŠ¶æ€æ›´æ–°å®Œæ¯• ---")
        print(f"çŸ­æœŸæ±  ({len(self.short_term_pool_paths)}/{SHORT_TERM_POOL_SIZE}): {self.short_term_pool_paths}")
        print(f"é•¿æœŸæ±  ({len(self.long_term_pool_paths)}/{LONG_TERM_POOL_SIZE}): {self.long_term_pool_paths}")
        print(f"é•¿æœŸæ± ä»£æ•°å·®å€¼æŒ‡æ•°: {self.long_term_power_of_2} (å½“å‰è¦æ±‚å·®å€¼ >= {2**self.long_term_power_of_2})")

    def _remove_policy_and_files(self, algorithm: Algorithm, policy_id_to_remove: str, model_filename: str):
        """ä»ç®—æ³•ä¸­ç§»é™¤ç­–ç•¥å¹¶åˆ é™¤ç›¸å…³æ–‡ä»¶ã€‚"""
        print(f"âœ‚ï¸  æ¸…ç†è¿‡æ—¶å¯¹æ‰‹ç­–ç•¥: {policy_id_to_remove}")
        try:
            if algorithm.workers.local_worker().has_policy(policy_id_to_remove):
                algorithm.remove_policy(policy_id_to_remove, workers=algorithm.workers)
                print(f"    - æˆåŠŸä»RLlibä¸­ç§»é™¤ç­–ç•¥: {policy_id_to_remove}")
            
            # åˆ é™¤æ¨¡å‹æ–‡ä»¶
            model_path = os.path.join(OPPONENT_POOL_DIR, model_filename)
            if os.path.exists(model_path):
                os.remove(model_path)
                print(f"    - æˆåŠŸåˆ é™¤æ¨¡å‹æ–‡ä»¶: {model_filename}")
            
            # ä»çŠ¶æ€å­—å…¸ä¸­ç§»é™¤
            self.elo_ratings.pop(policy_id_to_remove, None)
            self.model_generations.pop(model_filename, None)
            
        except Exception as e:
            print(f"    - è­¦å‘Š: ç§»é™¤ç­–ç•¥ {policy_id_to_remove} æ—¶å‡ºé”™: {e}")