# src_code/run_league.py

import os
import json
import itertools
from tqdm import tqdm
import numpy as np

# ç¡®ä¿åœ¨è¿è¡Œæ­¤è„šæœ¬æ—¶ï¼Œé¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
import sys
# å°† 'src_code' çš„çˆ¶ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


# å¯¼å…¥æœ¬åœ°æ¨¡å—
from utils.constants import SELF_PLAY_OUTPUT_DIR, OPPONENT_POOL_DIR, MAIN_OPPONENT_PATH
from training.evaluator import evaluate_models

# --- é…ç½® ---
DEFAULT_ELO = 1200
# åœ¨è”èµ›æ›´æ–°ä¸­ï¼ŒKå› å­é€šå¸¸ä¼šè®¾å¾—å°ä¸€äº›ï¼Œå› ä¸ºæ•°æ®é‡æ›´å¤§
ELO_K_FACTOR = 16 

def run_league_tournament():
    """
    æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„è”èµ›ï¼Œè®©æ‰€æœ‰æ¨¡å‹äº’ç›¸æ¯”èµ›ï¼Œå¹¶å…¨å±€æ›´æ–°Eloè¯„åˆ†ã€‚
    """
    print("=" * 70)
    print("           ğŸ† å…¨æ¨¡å‹å¾ªç¯è”èµ›ç³»ç»Ÿ ğŸ†")
    print("=" * 70)

    # --- 1. å‘ç°å¹¶åŠ è½½æ‰€æœ‰å‚èµ›æ¨¡å‹ ---
    print("\n[æ­¥éª¤ 1/5] æ­£åœ¨å‘ç°æ‰€æœ‰å‚èµ›æ¨¡å‹...")
    
    model_paths = []
    model_names = []

    # æ·»åŠ ä¸»å®°è€…æ¨¡å‹
    if os.path.exists(MAIN_OPPONENT_PATH):
        model_paths.append(MAIN_OPPONENT_PATH)
        model_names.append(os.path.basename(MAIN_OPPONENT_PATH))
    
    # æ·»åŠ å¯¹æ‰‹æ± ä¸­çš„æ‰€æœ‰æ¨¡å‹
    if os.path.exists(OPPONENT_POOL_DIR):
        for filename in os.listdir(OPPONENT_POOL_DIR):
            if filename.endswith('.zip'):
                full_path = os.path.join(OPPONENT_POOL_DIR, filename)
                model_paths.append(full_path)
                model_names.append(filename)

    if len(model_paths) < 2:
        print("é”™è¯¯ï¼šå‚èµ›æ¨¡å‹å°‘äº2ä¸ªï¼Œæ— æ³•ä¸¾åŠè”èµ›ã€‚")
        return

    print(f"å‘ç° {len(model_paths)} ä¸ªå‚èµ›æ¨¡å‹: {', '.join(model_names)}")


    # --- 2. åŠ è½½ç°æœ‰Eloè¯„åˆ† ---
    print("\n[æ­¥éª¤ 2/5] æ­£åœ¨åŠ è½½Eloè¯„åˆ†...")
    elo_ratings = {}
    elo_file_path = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
    if os.path.exists(elo_file_path):
        try:
            with open(elo_file_path, 'r') as f:
                elo_ratings = json.load(f)
            print(f"å·²ä» {elo_file_path} åŠ è½½Eloæ•°æ®ã€‚")
        except (json.JSONDecodeError, IOError) as e:
            print(f"è­¦å‘Šï¼šè¯»å–Eloæ–‡ä»¶å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")

    # ä¸ºæ‰€æœ‰æ¨¡å‹ç¡®ä¿Eloè®°å½•å­˜åœ¨
    initial_elos = {}
    for name in model_names:
        initial_elos[name] = elo_ratings.get(name, DEFAULT_ELO)
    
    print("åˆå§‹Eloè¯„åˆ†:")
    for name, elo in initial_elos.items():
        print(f"  - {name}: {elo:.0f}")


    # --- 3. ç”Ÿæˆæ‰€æœ‰å¯¹å±€ç»„åˆ ---
    matchups = list(itertools.combinations(model_paths, 2))
    num_matches = len(matchups)
    print(f"\n[æ­¥éª¤ 3/5] å·²ç”Ÿæˆ {num_matches} åœºå¯¹å±€ã€‚")

    
    # --- 4. æ‰§è¡Œå¾ªç¯èµ› ---
    print("\n[æ­¥éª¤ 4/5] è”èµ›å¼€å§‹ï¼æ­£åœ¨è¿›è¡Œè¯„ä¼°...")
    
    # è®°å½•æ¯ä¸ªæ¨¡å‹åœ¨è”èµ›ä¸­çš„å¾—åˆ†ï¼ˆèƒœ1åˆ†ï¼Œå¹³0.5åˆ†ï¼Œè´Ÿ0åˆ†ï¼‰
    actual_scores = {name: 0.0 for name in model_names}
    
    for model_a_path, model_b_path in tqdm(matchups, desc="è”èµ›è¿›åº¦"):
        model_a_name = os.path.basename(model_a_path)
        model_b_name = os.path.basename(model_b_path)
        
        # evaluate_models è¿”å›æ¨¡å‹Açš„èƒœç‡
        win_rate_a = evaluate_models(model_a_path, model_b_path)
        
        # åœ¨ä¸¤å±€é•œåƒå¯¹å±€ä¸­ï¼ŒAçš„å¾—åˆ†æ˜¯ win_rate_a * 2
        # ï¼ˆä¾‹å¦‚ï¼ŒAèµ¢1è¾“1ï¼Œèƒœç‡0.5ï¼Œå¾—åˆ†1ï¼›Aèµ¢2ï¼Œèƒœç‡1.0ï¼Œå¾—åˆ†2ï¼‰
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ä¸ºç›´æ¥ä½¿ç”¨èƒœç‡ä½œä¸ºå¾—åˆ†æ¯”ä¾‹
        actual_scores[model_a_name] += win_rate_a
        actual_scores[model_b_name] += (1.0 - win_rate_a)


    # --- 5. å…¨å±€æ›´æ–°Eloè¯„åˆ† ---
    print("\n[æ­¥éª¤ 5/5] è”èµ›ç»“æŸï¼Œæ­£åœ¨è®¡ç®—å¹¶æ›´æ–°Eloè¯„åˆ†...")

    # è®¡ç®—æœŸæœ›å¾—åˆ†
    expected_scores = {name: 0.0 for name in model_names}
    for model_a_name in model_names:
        for model_b_name in model_names:
            if model_a_name == model_b_name:
                continue
            
            elo_a = initial_elos[model_a_name]
            elo_b = initial_elos[model_b_name]
            
            # æ¨¡å‹Aå¯¹é˜µæ¨¡å‹Bçš„æœŸæœ›èƒœç‡
            expected_win_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))
            expected_scores[model_a_name] += expected_win_a

    # è®¡ç®—æ–°çš„Elo
    new_elos = {}
    print("\n--- Elo å˜æ›´è¯¦æƒ… ---")
    print(f"{'æ¨¡å‹åç§°':<25} | {'æ—§Elo':>8} | {'æ–°Elo':>8} | {'å˜åŒ–':>8} | {'å®é™…å¾—åˆ†':>10} | {'æœŸæœ›å¾—åˆ†':>10}")
    print("-" * 85)

    sorted_models = sorted(model_names, key=lambda n: initial_elos[n], reverse=True)

    for name in sorted_models:
        old_elo = initial_elos[name]
        # Kå› å­éœ€è¦ä¹˜ä»¥æ¯”èµ›åœºæ•°çš„ä¸€åŠï¼Œå› ä¸ºæ¯ä¸ªæ¨¡å‹éƒ½å’Œ N-1 ä¸ªå¯¹æ‰‹æ¯”èµ›
        # ä½†æˆ‘ä»¬è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„Kå› å­
        score_diff = actual_scores[name] - expected_scores[name]
        new_elo = old_elo + ELO_K_FACTOR * score_diff
        new_elos[name] = new_elo
        
        print(f"{name:<25} | {old_elo:>8.0f} | {new_elo:>8.0f} | {new_elo - old_elo:>+8.1f} | "
              f"{actual_scores[name]:>10.2f} | {expected_scores[name]:>10.2f}")

    # --- ä¿å­˜æœ€ç»ˆç»“æœ ---
    try:
        # åˆå¹¶æ–°æ—§eloï¼Œåªæ›´æ–°å‚èµ›æ¨¡å‹çš„
        elo_ratings.update(new_elos)
        with open(elo_file_path, 'w') as f:
            json.dump(elo_ratings, f, indent=4)
        print(f"\nâœ… æˆåŠŸå°†æ›´æ–°åçš„Eloè¯„åˆ†ä¿å­˜è‡³: {elo_file_path}")
    except IOError as e:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ³•ä¿å­˜Eloè¯„åˆ†æ–‡ä»¶: {e}")

    print("=" * 70)
    print("           ğŸ‰ è”èµ›åœ†æ»¡ç»“æŸ! ğŸ‰")
    print("=" * 70)


if __name__ == '__main__':
    run_league_tournament()