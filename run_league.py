# src_code/run_league.py (å†…å­˜ä¼˜åŒ–ç‰ˆ)

import os
import json
import itertools
import time
from tqdm import tqdm
import numpy as np
import multiprocessing
import sys

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ã€é‡è¦ã€‘ç›´æ¥ä» evaluator å¯¼å…¥æ ¸å¿ƒç»„ä»¶ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå‡½æ•°
from training.evaluator import EvaluationAgent, _play_one_game
from game.environment import GameEnvironment
from utils.constants import (
    SELF_PLAY_OUTPUT_DIR, OPPONENT_POOL_DIR, MAIN_OPPONENT_PATH, EVALUATION_GAMES
)

# --- é…ç½® ---
DEFAULT_ELO = 1200
ELO_K_FACTOR = 16
# ä½¿ç”¨ os.cpu_count() æ¥åŠ¨æ€è®¾ç½®è¿›ç¨‹æ•°ï¼Œæ›´å…·é€‚åº”æ€§
NUM_WORKERS = max(1, os.cpu_count() - 2) if os.cpu_count() else 1


# --- å¹¶è¡Œå·¥ä½œè¿›ç¨‹ä¸“å±å‡½æ•° ---

# å®šä¹‰ä¸€ä¸ªåœ¨å·¥ä½œè¿›ç¨‹ç”Ÿå‘½å‘¨æœŸå†…æŒç»­å­˜åœ¨çš„å…¨å±€å˜é‡ï¼ˆç¼“å­˜ï¼‰
_worker_model_cache = {}

def get_model_from_cache(path: str) -> EvaluationAgent:
    """
    å·¥ä½œè¿›ç¨‹å†…éƒ¨çš„åŠ©æ‰‹å‡½æ•°ï¼Œç”¨äºåŠ è½½å¹¶ç¼“å­˜æ¨¡å‹ã€‚
    æ¯ä¸ªè¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„ _worker_model_cacheã€‚
    """
    if path not in _worker_model_cache:
        # å¦‚æœæ¨¡å‹ä¸åœ¨ç¼“å­˜ä¸­ï¼Œåˆ™åŠ è½½å®ƒå¹¶å­˜å…¥
        _worker_model_cache[path] = EvaluationAgent(path)
    return _worker_model_cache[path]

def evaluate_matchup_worker(matchup: tuple) -> tuple:
    """
    ã€æ–°çš„å·¥ä½œå‡½æ•°ã€‘ç”¨äºå¹¶è¡Œå¤„ç†ã€‚
    å®ƒè´Ÿè´£åŠ è½½æ¨¡å‹ï¼ˆåˆ©ç”¨ç¼“å­˜ï¼‰å¹¶æ‰§è¡Œå®Œæ•´çš„é•œåƒå¯¹å±€è¯„ä¼°ã€‚
    """
    model_a_path, model_b_path = matchup
    
    # 1. ä»ç¼“å­˜ä¸­è·å–æ¨¡å‹
    model_a = get_model_from_cache(model_a_path)
    model_b = get_model_from_cache(model_b_path)
    
    # 2. åœ¨å·¥ä½œè¿›ç¨‹å†…éƒ¨åˆ›å»ºç‹¬ç«‹çš„ç¯å¢ƒ
    eval_env = GameEnvironment()
    
    # 3. æ‰§è¡Œè¯„ä¼°å¾ªç¯ï¼ˆé€»è¾‘ä» evaluate_models ç§»åˆ°æ­¤å¤„ï¼‰
    scores = {'model_a_wins': 0, 'model_b_wins': 0, 'draws': 0}
    num_groups = EVALUATION_GAMES // 2

    for i in range(num_groups):
        game_seed = int(time.time_ns() + i) % (2**32 - 1)
        
        # Aæ‰§çº¢ vs Bæ‰§é»‘
        winner_1 = _play_one_game(eval_env, red_player=model_a, black_player=model_b, seed=game_seed)
        if winner_1 == 1: scores['model_a_wins'] += 1
        elif winner_1 == -1: scores['model_b_wins'] += 1
        else: scores['draws'] += 1

        # Bæ‰§çº¢ vs Aæ‰§é»‘ (é•œåƒ)
        winner_2 = _play_one_game(eval_env, red_player=model_b, black_player=model_a, seed=game_seed)
        if winner_2 == 1: scores['model_b_wins'] += 1
        elif winner_2 == -1: scores['model_a_wins'] += 1
        else: scores['draws'] += 1
    
    eval_env.close()

    # 4. è®¡ç®—å¹¶è¿”å›ç»“æœ
    total_games = scores['model_a_wins'] + scores['model_b_wins'] + scores['draws']
    win_rate_a = scores['model_a_wins'] / total_games if total_games > 0 else 0.5
    
    return model_a_path, model_b_path, win_rate_a

def run_league_tournament():
    """
    ã€å†…å­˜ä¼˜åŒ–ç‰ˆã€‘æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„è”èµ›ï¼Œå¹¶è¡Œå¤„ç†æ‰€æœ‰æ¨¡å‹çš„ç›¸äº’æ¯”èµ›ï¼Œå¹¶å…¨å±€æ›´æ–°Eloè¯„åˆ†ã€‚
    """
    print("=" * 70)
    print("       ğŸ† å…¨æ¨¡å‹å¾ªç¯è”èµ›ç³»ç»Ÿ (å†…å­˜ä¼˜åŒ–å¹¶è¡Œç‰ˆ) ğŸ†")
    print("=" * 70)

    # ... [æ­¥éª¤ 1, 2, 3: å‘ç°æ¨¡å‹ã€åŠ è½½Eloã€ç”Ÿæˆå¯¹å±€ç»„åˆ] (è¿™éƒ¨åˆ†ä»£ç ä¸ä¹‹å‰ç›¸åŒ) ...
    print("\n[æ­¥éª¤ 1/5] æ­£åœ¨å‘ç°æ‰€æœ‰å‚èµ›æ¨¡å‹...")
    model_paths = []
    if os.path.exists(MAIN_OPPONENT_PATH):
        model_paths.append(MAIN_OPPONENT_PATH)
    if os.path.exists(OPPONENT_POOL_DIR):
        for filename in sorted(os.listdir(OPPONENT_POOL_DIR)):
            if filename.endswith('.zip'):
                full_path = os.path.join(OPPONENT_POOL_DIR, filename)
                if full_path not in model_paths:
                    model_paths.append(full_path)
    
    model_names = [os.path.basename(p) for p in model_paths]
    if len(model_paths) < 2:
        print("é”™è¯¯ï¼šå‚èµ›æ¨¡å‹å°‘äº2ä¸ªï¼Œæ— æ³•ä¸¾åŠè”èµ›ã€‚")
        return
    print(f"å‘ç° {len(model_paths)} ä¸ªå‚èµ›æ¨¡å‹ã€‚")
    
    print("\n[æ­¥éª¤ 2/5] æ­£åœ¨åŠ è½½Eloè¯„åˆ†...")
    elo_file_path = os.path.join(SELF_PLAY_OUTPUT_DIR, "elo_ratings.json")
    elo_ratings = {}
    if os.path.exists(elo_file_path):
        with open(elo_file_path, 'r') as f:
            elo_ratings = json.load(f)
    initial_elos = {name: elo_ratings.get(name, DEFAULT_ELO) for name in model_names}
    
    matchups = list(itertools.combinations(model_paths, 2))
    print(f"\n[æ­¥éª¤ 3/5] å·²ç”Ÿæˆ {len(matchups)} åœºå¯¹å±€ã€‚")

    # --- 4. å¹¶è¡Œæ‰§è¡Œå¾ªç¯èµ› ---
    print(f"\n[æ­¥éª¤ 4/5] è”èµ›å¼€å§‹ï¼ä½¿ç”¨ {NUM_WORKERS} ä¸ªè¿›ç¨‹å¹¶è¡Œè¯„ä¼°...")
    actual_scores = {name: 0.0 for name in model_names}
    
    # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œæ‰€æœ‰æ¯”èµ›
    with multiprocessing.Pool(processes=NUM_WORKERS) as pool:
        # ä½¿ç”¨ pool.imap_unordered å¯èƒ½æ›´å¿«ï¼Œå› ä¸ºå®ƒä¼šç«‹å³å¤„ç†å®Œæˆçš„ä»»åŠ¡
        results_iterator = pool.imap_unordered(evaluate_matchup_worker, matchups)
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        results = list(tqdm(results_iterator, total=len(matchups), desc="è”èµ›è¿›åº¦"))

    # å¤„ç†ç»“æœ
    for model_a_path, model_b_path, win_rate_a in results:
        model_a_name = os.path.basename(model_a_path)
        model_b_name = os.path.basename(model_b_path)
        # æ¯åœºæ¯”èµ›ï¼ˆ2å±€é•œåƒï¼‰çš„æ€»åˆ†æ˜¯1åˆ†ï¼ŒæŒ‰èƒœç‡åˆ†é…
        actual_scores[model_a_name] += win_rate_a
        actual_scores[model_b_name] += (1.0 - win_rate_a)

    # ... [æ­¥éª¤ 5: æ›´æ–°Eloè¯„åˆ†] (è¿™éƒ¨åˆ†ä»£ç ä¸ä¹‹å‰ç›¸åŒ) ...
    print("\n[æ­¥éª¤ 5/5] è”èµ›ç»“æŸï¼Œæ­£åœ¨è®¡ç®—å¹¶æ›´æ–°Eloè¯„åˆ†...")
    expected_scores = {name: 0.0 for name in model_names}
    for model_a_name, model_b_name in itertools.permutations(model_names, 2):
        expected_win_a = 1 / (1 + 10 ** ((initial_elos[model_b_name] - initial_elos[model_a_name]) / 400))
        expected_scores[model_a_name] += expected_win_a
    
    new_elos = {}
    print("\n--- Elo å˜æ›´è¯¦æƒ… ---")
    print(f"{'æ¨¡å‹åç§°':<25} | {'æ—§Elo':>8} | {'æ–°Elo':>8} | {'å˜åŒ–':>8} | {'å®é™…å¾—åˆ†':>10} | {'æœŸæœ›å¾—åˆ†':>10}")
    print("-" * 85)
    
    for name in sorted(model_names, key=lambda n: initial_elos.get(n, DEFAULT_ELO), reverse=True):
        old_elo = initial_elos.get(name, DEFAULT_ELO)
        # æ¯ä¸ªæ¨¡å‹ä¼šå’Œå…¶ä»– N-1 ä¸ªæ¨¡å‹æ¯”èµ›
        score_diff = actual_scores[name] - expected_scores[name]
        new_elo = old_elo + ELO_K_FACTOR * score_diff
        new_elos[name] = new_elo
        print(f"{name:<25} | {old_elo:>8.0f} | {new_elo:>8.0f} | {new_elo - old_elo:>+8.1f} | "
              f"{actual_scores[name]:>10.2f} | {expected_scores[name]:>10.2f}")
              
    elo_ratings.update(new_elos)
    with open(elo_file_path, 'w') as f:
        json.dump(elo_ratings, f, indent=4)
    print(f"\nâœ… æˆåŠŸå°†æ›´æ–°åçš„Eloè¯„åˆ†ä¿å­˜è‡³: {elo_file_path}")
    print("=" * 70, "\n           ğŸ‰ è”èµ›åœ†æ»¡ç»“æŸ! ğŸ‰\n", "=" * 70, sep='')


if __name__ == '__main__':
    # åœ¨Windowså’ŒmacOSä¸Šï¼Œ'spawn'æ˜¯æ›´å®‰å…¨çš„å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    if sys.platform.startswith('win') or sys.platform == 'darwin':
        multiprocessing.set_start_method('spawn', force=True)
    run_league_tournament()