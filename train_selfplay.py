# train_selfplay_optimized.py (å·²åº”ç”¨å†…å­˜æ›´æ–°ä¼˜åŒ–)
import os
import shutil
from stable_baselines3.common.env_util import make_vec_env
from sb3_contrib import MaskablePPO
from Game import GameEnvironment
# ã€é‡è¦ã€‘ä»ä¿®æ”¹åçš„æ–‡ä»¶ä¸­å¯¼å…¥ç®¡ç†å™¨
from opponent_model_manager import shared_opponent_manager

def main():
    # --- è®¾ç½®å’Œåˆå§‹åŒ– ---
    CURRICULUM_MODEL_PATH = "cnn_curriculum_models/final_model_cnn.zip"
    SELF_PLAY_MODEL_DIR = "self_play_models_optimized"
    # æˆ‘ä»¬ä¸å†éœ€è¦åœ¨å¾ªç¯ä¸­ä¿å­˜learnerå’Œopponentï¼Œä½†ä¿ç•™è·¯å¾„ç”¨äºæœ€ç»ˆä¿å­˜
    FINAL_MODEL_PATH = os.path.join(SELF_PLAY_MODEL_DIR, "final_selfplay_model_optimized.zip")
    
    TOTAL_TRAINING_LOOPS = 100
    STEPS_PER_LOOP = 50_000 # æ¯æ¬¡æ›´æ–°å¯¹æ‰‹ä¹‹é—´ï¼Œå­¦ä¹ è€…è®­ç»ƒå¤šå°‘æ­¥
    
    print("ğŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–ç‰ˆè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒ...")
    print(f"ğŸ“Š æ€»è®­ç»ƒå¾ªç¯æ•°: {TOTAL_TRAINING_LOOPS}")
    print(f"ğŸ”„ æ¯å¾ªç¯è®­ç»ƒæ­¥æ•°: {STEPS_PER_LOOP:,}")
    print("âš¡ ä½¿ç”¨å…±äº«å¯¹æ‰‹æ¨¡å‹ç®¡ç†å™¨ï¼Œå¹¶é‡‡ç”¨å†…å­˜ç›´æ¥æ›´æ–°ç­–ç•¥ï¼")
    
    # --- åˆå§‹è®¾ç½® (ä»…æ‰§è¡Œä¸€æ¬¡) ---
    os.makedirs(SELF_PLAY_MODEL_DIR, exist_ok=True)
    
    if not os.path.exists(CURRICULUM_MODEL_PATH):
        raise FileNotFoundError(f"è¯¾ç¨‹å­¦ä¹ æœ€ç»ˆæ¨¡å‹ä¸å­˜åœ¨: {CURRICULUM_MODEL_PATH}")
    
    # 1. é¢„åŠ è½½åˆå§‹å¯¹æ‰‹æ¨¡å‹åˆ°å…±äº«ç®¡ç†å™¨
    # è¿™æ˜¯æˆ‘ä»¬å”¯ä¸€ä¸€æ¬¡ä»ç£ç›˜åŠ è½½å¯¹æ‰‹æ¨¡å‹
    print("ğŸ”§ é¢„åŠ è½½åˆå§‹å¯¹æ‰‹æ¨¡å‹åˆ°å…±äº«ç®¡ç†å™¨...")
    shared_opponent_manager.load_model(CURRICULUM_MODEL_PATH)
    model_info = shared_opponent_manager.get_model_info()
    print(f"ğŸ“‹ åˆå§‹å¯¹æ‰‹æ¨¡å‹ä¿¡æ¯: {model_info}")

    # 2. åˆ›å»ºç¯å¢ƒ (è¿™ä¸ªç¯å¢ƒå°†ä¸€ç›´è¢«ä½¿ç”¨ï¼Œä¸å†é‡å»º)
    print("ğŸŒ åˆ›å»ºæŒä¹…åŒ–è‡ªæˆ‘å¯¹å¼ˆç¯å¢ƒ...")
    env = make_vec_env(
        GameEnvironment,
        n_envs=8,
        env_kwargs={
            'curriculum_stage': 4,
            'opponent_policy': CURRICULUM_MODEL_PATH # ä¼ å…¥è·¯å¾„ä»¥è§¦å‘ç®¡ç†å™¨åŠ è½½
        }
    )
    
    # 3. åŠ è½½å­¦ä¹ è€…æ¨¡å‹
    print(f"ğŸ¤– åŠ è½½å­¦ä¹ è€…æ¨¡å‹ä» {CURRICULUM_MODEL_PATH}...")
    model = MaskablePPO.load(
        CURRICULUM_MODEL_PATH,
        env=env,
        learning_rate=3e-4, # å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´å­¦ä¹ ç‡
        tensorboard_log="./self_play_tensorboard_logs_optimized/"
    )
    
    print("ğŸ¯ å¼€å§‹å†…å­˜ä¼˜åŒ–ç‰ˆè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå¾ªç¯...")
    
    # --- ä¸»è®­ç»ƒå¾ªç¯ (ç°åœ¨éå¸¸é«˜æ•ˆ) ---
    for i in range(1, TOTAL_TRAINING_LOOPS + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ è®­ç»ƒå¾ªç¯ {i}/{TOTAL_TRAINING_LOOPS}")
        print(f"{'='*60}")
        
        # (a) è®­ç»ƒå­¦ä¹ è€… - è¿™æ˜¯å¾ªç¯ä¸­å”¯ä¸€è€—æ—¶çš„éƒ¨åˆ†
        print(f"ğŸ‹ï¸  è®­ç»ƒå­¦ä¹ è€… {STEPS_PER_LOOP:,} æ­¥...")
        model.learn(
            total_timesteps=STEPS_PER_LOOP,
            reset_num_timesteps=False,
            progress_bar=True
        )
        
        # (b) ã€æ ¸å¿ƒä¼˜åŒ–ã€‘ç›´æ¥åœ¨å†…å­˜ä¸­æ›´æ–°å¯¹æ‰‹æ¨¡å‹
        # è¿™ä¼šå–ä»£æ‰€æœ‰çš„ save, copy, load å’Œç¯å¢ƒé‡å»ºæ“ä½œ
        shared_opponent_manager.update_model_from_learner(model)
        
        print(f"âœ… å¾ªç¯ {i} å®Œæˆï¼å¯¹æ‰‹å·²åœ¨å†…å­˜ä¸­æ›´æ–°ï¼Œç»§ç»­è®­ç»ƒã€‚")
        
        # (c) å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ (å¯é€‰ï¼Œä½†æ¨è)
        if i % 10 == 0:
            checkpoint_path = os.path.join(SELF_PLAY_MODEL_DIR, f"checkpoint_loop_{i}.zip")
            model.save(checkpoint_path)
            print(f"ğŸ“‹ å·²ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹åˆ° {checkpoint_path}")
    
    # --- æœ€ç»ˆæ”¶å°¾ ---
    model.save(FINAL_MODEL_PATH)
    print(f"\nğŸ‰ å†…å­˜ä¼˜åŒ–ç‰ˆè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“¦ æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {FINAL_MODEL_PATH}")
    
    model_info = shared_opponent_manager.get_model_info()
    print(f"\nğŸ“Š ä¼˜åŒ–æ€»ç»“:")
    print(f"   âœ… æœ€ç»ˆå¯¹æ‰‹æ¨¡å‹çŠ¶æ€: {model_info}")
    print(f"   âš¡ï¸ é€šè¿‡å†…å­˜æ›´æ–°ï¼Œé¿å…äº† {TOTAL_TRAINING_LOOPS} æ¬¡çš„ç£ç›˜I/Oå’Œç¯å¢ƒé‡å»ºï¼")
    print(f"   ğŸ’¾ æ˜¾è‘—èŠ‚çœäº†è®­ç»ƒæ—¶é—´ï¼Œæé«˜äº†ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚")
    
    env.close()

if __name__ == "__main__":
    main()